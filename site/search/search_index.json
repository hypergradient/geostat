{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to GeoStat Model space-time data with Gaussian processes. Geostat makes it easy to write Gaussian Process (GP) models with complex covariance functions. It uses maximum likelihood to fit model parameters. Under the hood it uses Tensorflow to fit models and do inference on GPUs. A good consumer GPU such as an Nvidia RTX 4090 can handle 10k data points. Visit our GitHub repository here . Quickstart Install Geostat using pip: pip install geostat Project layout README.md # The readme file. mkdocs.yml # The configuration file. doc/ docs/ about.md # The about page. index.md # The documentation homepage. src/geostat/ __init__.py custom_op.py kernel.py krige.py mean.py mesh.py metric.py model.py op.py param.py tests/","title":"Home"},{"location":"#welcome-to-geostat","text":"Model space-time data with Gaussian processes. Geostat makes it easy to write Gaussian Process (GP) models with complex covariance functions. It uses maximum likelihood to fit model parameters. Under the hood it uses Tensorflow to fit models and do inference on GPUs. A good consumer GPU such as an Nvidia RTX 4090 can handle 10k data points. Visit our GitHub repository here .","title":"Welcome to GeoStat"},{"location":"#quickstart","text":"Install Geostat using pip: pip install geostat","title":"Quickstart"},{"location":"#project-layout","text":"README.md # The readme file. mkdocs.yml # The configuration file. doc/ docs/ about.md # The about page. index.md # The documentation homepage. src/geostat/ __init__.py custom_op.py kernel.py krige.py mean.py mesh.py metric.py model.py op.py param.py tests/","title":"Project layout"},{"location":"about/","text":"About Some background about the library and it's intended uses. Mention contributors, licences, and changelog. Disclaimer This software is preliminary or provisional and is subject to revision. It is being provided to meet the need for timely best science. The software has not received final approval by the U.S. Geological Survey (USGS). No warranty, expressed or implied, is made by the USGS or the U.S. Government as to the functionality of the software and related material nor shall the fact of release constitute any such warranty. The software is provided on the condition that neither the USGS nor the U.S. Government shall be held liable for any damages resulting from the authorized or unauthorized use of the software.","title":"About"},{"location":"about/#about","text":"Some background about the library and it's intended uses. Mention contributors, licences, and changelog.","title":"About"},{"location":"about/#disclaimer","text":"This software is preliminary or provisional and is subject to revision. It is being provided to meet the need for timely best science. The software has not received final approval by the U.S. Geological Survey (USGS). No warranty, expressed or implied, is made by the USGS or the U.S. Government as to the functionality of the software and related material nor shall the fact of release constitute any such warranty. The software is provided on the condition that neither the USGS nor the U.S. Government shall be held liable for any damages resulting from the authorized or unauthorized use of the software.","title":"Disclaimer"},{"location":"api/","text":"API Reference src.geostat.custom_op function ( f ) A custom op involves two functions f, where it is defined, and g, where it is introduced into the graph. Source code in src/geostat/custom_op.py 19 20 21 22 23 24 25 26 27 28 def function ( f ): \"\"\" A custom op involves two functions: * f, where it is defined, and * g, where it is introduced into the graph. \"\"\" def g ( * args ): # Assume for now that all arguments are Parameters return CustomOp ( f , ** { p . name : p for p in args }) return g src.geostat.kernel Kernel Bases: Op Source code in src/geostat/kernel.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class Kernel ( Op ): def __init__ ( self , fa , autoinputs ): if 'offset' not in autoinputs : autoinputs [ 'offset' ] = 'offset' if 'locs1' not in autoinputs : autoinputs [ 'locs1' ] = 'locs1' if 'locs2' not in autoinputs : autoinputs [ 'locs2' ] = 'locs2' super () . __init__ ( fa , autoinputs ) def __add__ ( self , other ): if other is None : return self else : return Stack ([ self ]) + other def __mul__ ( self , other ): return Product ([ self ]) * other def call ( self , e ): \"\"\" Returns tuple `(mean, covariance)` for locations. Return values may be unbroadcasted. \"\"\" pass def __call__ ( self , e ): \"\"\" Returns tuple `(mean, covariance)` for locations. Return values have correct shapes. \"\"\" C = self . call ( e ) if C is None : C = 0. n1 = tf . shape ( e [ 'locs1' ])[ 0 ] n2 = tf . shape ( e [ 'locs2' ])[ 0 ] C = tf . broadcast_to ( C , [ n1 , n2 ]) return C def report ( self ): string = ', ' . join ( ' %s %4.2f ' % ( v . name , p [ v . name ]) for v in self . vars ()) return '[' + string + ']' __call__ ( e ) Returns tuple (mean, covariance) for locations. Return values have correct shapes. Source code in src/geostat/kernel.py 47 48 49 50 51 52 53 54 55 56 57 def __call__ ( self , e ): \"\"\" Returns tuple `(mean, covariance)` for locations. Return values have correct shapes. \"\"\" C = self . call ( e ) if C is None : C = 0. n1 = tf . shape ( e [ 'locs1' ])[ 0 ] n2 = tf . shape ( e [ 'locs2' ])[ 0 ] C = tf . broadcast_to ( C , [ n1 , n2 ]) return C call ( e ) Returns tuple (mean, covariance) for locations. Return values may be unbroadcasted. Source code in src/geostat/kernel.py 40 41 42 43 44 45 def call ( self , e ): \"\"\" Returns tuple `(mean, covariance)` for locations. Return values may be unbroadcasted. \"\"\" pass Mix Bases: Kernel Source code in src/geostat/kernel.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 class Mix ( Kernel ): def __init__ ( self , inputs , weights = None ): self . inputs = inputs fa = {} ai = dict ( cats1 = 'cats1' , cats2 = 'cats2' ) # Special case if weights is not given. if weights is not None : fa [ 'weights' ] = weights ai [ 'inputs' ] = inputs super () . __init__ ( fa , ai ) def gather_vars ( self , cache = None ): \"\"\"Make a special version of gather_vars because we want to gather variables from `inputs` even when it's not in autoinputs\"\"\" vv = super () . gather_vars ( cache ) for iput in self . inputs : cache [ id ( self )] |= iput . gather_vars ( cache ) return cache [ id ( self )] def vars ( self ): if 'weights' in self . fa : return { k : p for row in self . fa [ 'weights' ] for k , p in get_trend_coefs ( row ) . items ()} else : return {} def call ( self , e ): if 'weights' in e : weights = [] for row in e [ 'weights' ]: if isinstance ( row , ( tuple , list )): row = tf . stack ( row ) weights . append ( row ) weights = tf . stack ( weights ) C = tf . stack ( e [ 'inputs' ], axis =- 1 ) # [locs, locs, numinputs]. Aaug1 = tf . gather ( weights , e [ 'cats1' ]) # [locs, numinputs]. Aaug2 = tf . gather ( weights , e [ 'cats2' ]) # [locs, numinputs]. outer = tf . einsum ( 'ac,bc->abc' , Aaug1 , Aaug2 ) # [locs, locs, numinputs]. C = tf . einsum ( 'abc,abc->ab' , C , outer ) # [locs, locs]. return C else : # When weights is not given, exploit the fact that we don't have # to compute every element in component covariance matrices. N = len ( self . inputs ) catcounts1 = tf . math . bincount ( e [ 'cats1' ], minlength = N , maxlength = N ) catcounts2 = tf . math . bincount ( e [ 'cats2' ], minlength = N , maxlength = N ) catindices1 = tf . math . cumsum ( catcounts1 , exclusive = True ) catindices2 = tf . math . cumsum ( catcounts2 , exclusive = True ) catdiffs = tf . unstack ( catindices2 - catindices1 , num = N ) locsegs1 = tf . split ( e [ 'locs1' ], catcounts1 , num = N ) locsegs2 = tf . split ( e [ 'locs2' ], catcounts2 , num = N ) # TODO: Check that the below is still correct. CC = [] # Observation noise submatrices. for sublocs1 , sublocs2 , catdiff , iput in zip ( locsegs1 , locsegs2 , catdiffs , self . inputs ): cache = dict ( offset = e [ 'offset' ] + catdiff , locs1 = sublocs1 , locs2 = sublocs2 ) cache [ 'per_axis_dist2' ] = PerAxisDist2 () . run ( cache ) cache [ 'euclidean' ] = Euclidean () . run ( cache ) Csub = iput . run ( cache ) CC . append ( Csub ) return block_diag ( CC ) gather_vars ( cache = None ) Make a special version of gather_vars because we want to gather variables from inputs even when it's not in autoinputs Source code in src/geostat/kernel.py 448 449 450 451 452 453 454 455 def gather_vars ( self , cache = None ): \"\"\"Make a special version of gather_vars because we want to gather variables from `inputs` even when it's not in autoinputs\"\"\" vv = super () . gather_vars ( cache ) for iput in self . inputs : cache [ id ( self )] |= iput . gather_vars ( cache ) return cache [ id ( self )] Observation Bases: Op Source code in src/geostat/kernel.py 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 class Observation ( Op ): def __init__ ( self , coefs : List , noise : Kernel ): self . coefs = coefs self . noise = noise super () . __init__ ({}, self . noise ) def vars ( self ): vv = { k : p for c in self . coefs for k , p in upp ( c )} vv |= self . noise . vars () return vv def __call__ ( self , e ): \"\"\" Dummy. \"\"\" return 0. __call__ ( e ) Dummy. Source code in src/geostat/kernel.py 579 580 581 582 583 def __call__ ( self , e ): \"\"\" Dummy. \"\"\" return 0. block_diag ( blocks ) Return a dense block-diagonal matrix. Source code in src/geostat/kernel.py 20 21 22 def block_diag ( blocks ): \"\"\"Return a dense block-diagonal matrix.\"\"\" return LOBlockDiag ([ LOFullMatrix ( b ) for b in blocks ]) . to_dense () quadstack ( x , sills , ranges ) x has arbitrary shape [...], but must be non-negative. sills and ranges both have shape [K]. Source code in src/geostat/kernel.py 293 294 295 296 297 298 299 300 301 302 @tf . recompute_grad def quadstack ( x , sills , ranges ): \"\"\" `x` has arbitrary shape [...], but must be non-negative. `sills` and `ranges` both have shape [K]. \"\"\" ex = ed ( x ) ax = tf . maximum ( 0. , 1. - tf . abs ( ex ) / ranges ) # [..., 1] y = sills * tf . square ( ax ) # [..., K] return tf . reduce_sum ( y , - 1 ) rampstack ( x , sills , ranges ) x has arbitrary shape [...], but must be non-negative. sills and ranges both have shape [K]. Source code in src/geostat/kernel.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 @tf . custom_gradient def rampstack ( x , sills , ranges ): \"\"\" `x` has arbitrary shape [...], but must be non-negative. `sills` and `ranges` both have shape [K]. \"\"\" ax = ed ( tf . abs ( x )) # [..., 1] y = sills * tf . maximum ( 0. , 1. - ax / ranges ) # [..., K] def grad ( upstream ): ax = ed ( tf . abs ( x )) # [..., 1] y = sills * tf . maximum ( 0. , 1. - ax / ranges ) # [..., K] K = tf . shape ( sills )[ 0 ] small = ax < ranges grad_x = upstream * tf . reduce_sum ( tf . where ( small , - tf . sign ( ed ( x )) * ( sills / ranges ), 0. ), - 1 ) # [...] grad_sills = tf . einsum ( 'ak,a->k' , tf . reshape ( y , [ - 1 , K ]), tf . reshape ( upstream , [ - 1 ])) grad_ranges = tf . where ( small , ax * ( sills / tf . square ( ranges )), 0. ) # [..., K} grad_ranges = tf . einsum ( 'ak,a->k' , tf . reshape ( grad_ranges , [ - 1 , K ]), tf . reshape ( upstream , [ - 1 ])) return grad_x , grad_sills , grad_ranges return tf . reduce_sum ( y , - 1 ), grad smooth_convex ( x , sills , ranges ) x has arbitrary shape [...], but must be non-negative. sills and ranges both have shape [K]. Source code in src/geostat/kernel.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 @tf . recompute_grad def smooth_convex ( x , sills , ranges ): \"\"\" `x` has arbitrary shape [...], but must be non-negative. `sills` and `ranges` both have shape [K]. \"\"\" r2 = ranges r1 = tf . pad ( ranges [: - 1 ], [[ 1 , 0 ]]) ex = ed ( x ) ax = tf . abs ( ex ) rx = ax / r2 - 1. c1 = 2. / ( r1 + r2 ) c2 = 1. / ( 1. - tf . square ( r1 / r2 )) # i1 = tf.cast(ax <= r1, tf.float32) # Indicates x <= r1. # i2 = tf.cast(ax <= r2, tf.float32) * (1. - i1) # Indicates r1 < x <= r2. # v = i1 * (1. - c1 * ax) + i2 * c2 * tf.square(rx) v = tf . where ( ax <= r1 , 1. - c1 * ax , c2 * tf . square ( rx )) v = tf . where ( ax <= r2 , v , 0. ) y = tf . einsum ( '...k,k->...' , v , sills ) return y smooth_convex_grad ( x , sills , ranges ) x has arbitrary shape [...], but must be non-negative. sills and ranges both have shape [K]. Source code in src/geostat/kernel.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 @tf . custom_gradient def smooth_convex_grad ( x , sills , ranges ): \"\"\" `x` has arbitrary shape [...], but must be non-negative. `sills` and `ranges` both have shape [K]. \"\"\" r2 = ranges r1 = tf . pad ( ranges [: - 1 ], [[ 1 , 0 ]]) ex = ed ( x ) ax = tf . abs ( ex ) rx = ax / r2 - 1. c1 = 2. / ( r1 + r2 ) c2 = 1. / ( 1. - tf . square ( r1 / r2 )) v = tf . where ( ax <= r1 , 1. - c1 * ax , c2 * tf . square ( rx )) v = tf . where ( ax <= r2 , v , 0. ) y = tf . einsum ( '...k,k->...' , v , sills ) def grad ( upstream ): r2 = ranges r1 = tf . pad ( ranges [: - 1 ], [[ 1 , 0 ]]) ex = ed ( x ) ax = tf . abs ( ex ) rx = ax / r2 - 1. i1 = tf . cast ( ax <= r1 , tf . float32 ) # Indicates x <= r1. i2 = tf . cast ( ax <= r2 , tf . float32 ) * ( 1. - i1 ) # Indicates r1 < x <= r2. c1 = 2. / ( r1 + r2 ) c2 = 1. / ( 1. - tf . square ( r1 / r2 )) c3 = 1. / ( r2 - tf . square ( r1 ) / r2 ) v = i1 * ( 1. - c1 * ax ) + i2 * c2 * tf . square ( rx ) sx = tf . sign ( ex ) K = tf . shape ( sills )[ 0 ] gx = sx * sills * ( i1 * - c1 + i2 * rx * ( 2 * c3 )) grad_x = upstream * tf . reduce_sum ( gx , - 1 ) # [...] grad_sills = tf . einsum ( 'ak,a->k' , tf . reshape ( v , [ - 1 , K ]), tf . reshape ( upstream , [ - 1 ])) u = 2 / tf . square ( r1 + r2 ) * ax * i1 yr1 = u + i2 * tf . square ( rx * c3 ) * 2 * r1 yr2 = u - 2 * i2 * ( rx * c3 + tf . square ( rx * c2 ) / r2 ) yr1 = sills * tf . reshape ( yr1 , [ - 1 , K ]) yr2 = sills * tf . reshape ( yr2 , [ - 1 , K ]) yr = tf . pad ( yr1 [:, 1 :], [[ 0 , 0 ], [ 0 , 1 ]]) + yr2 grad_ranges = tf . einsum ( 'ak,a->k' , yr , tf . reshape ( upstream , [ - 1 ])) return grad_x , grad_sills , grad_ranges return y , grad src.geostat.krige Krige Source code in src/geostat/krige.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 class Krige (): def __init__ ( self , x1 , u1 , bins , variogram_func = None , variogram_params = None , cutoff_dist = 'auto' , featurization = None , projection = None , show_plots = True , verbose = True , ): ''' Parameters: x1 : n-dim array Locations of input data. u1 : 1-d array Values to be kriged. bins : int or None The number of bins to use on the variogram cloud. If None, variogram function is fit to the variogram cloud and is not binned first. variogram_func : str Name of the variogram model to use in the kriging. Should be 'linear', 'gaussian', or 'spherical'. Default is 'gaussian'. cutoff_dist : str or int, optional The maximum lag distance to include in variogram modeling. featurization : function, optional Should be a function that takes x1 (n-dim array of input data) and returns the coordinates, i.e., x, y, x**2, y**2. Example below. Default is None. project : function, opt A function that takes multiple vectors, and returns a tuple of projected vectors. epsg_proj : str The projected coordinate system to use. Ignored if project=False. Default is 'EPSG:3310' (California Albers). show_plots : boolean, optional Whether or not to show variogram plots. Default is True. verbose : boolean, optional Whether or not to print parameters. Default is True. Performs experimental variogram calculation, bins data, and fits variogram model to estimate variogram parameters. Performs ordinary and universal kriging in up to 3 spatial dimensions. Trend model example: def featurization(x1): return x1[:, 0], x1[:, 1], x1[:, 0]**2, x1[:, 1]**2 ''' super () . __init__ ( projection = projection ) # Save original x1 for stats function. self . x1_original = x1 # Check and change the shape of u1 if needed (for pdist). if u1 . ndim == 1 : self . u1 = u1 elif u1 . shape [ 1 ] == 1 : self . u1 = u1 . reshape ( - 1 ) else : raise ValueError ( \"Check dimensions of 'u1'.\" ) # Projection. self . x1 = self . project ( x1 ) # Variogram. if variogram_func == 'gaussian' : self . variogram_func = gaussian elif variogram_func == 'spherical' : self . variogram_func = spherical elif variogram_func == 'linear' : self . variogram_func = linear else : raise ValueError ( \"Variogram function must be 'linear', 'gaussian', or 'spherical'.\" ) if cutoff_dist == 'auto' : self . cutoff_dist = cutoff_dist_func ( self . x1 ) else : self . cutoff_dist = cutoff_dist self . verbose = verbose self . show_plots = show_plots self . bins = bins self . featurization = featurization # Lags and semivariance. dist = pdist ( self . x1 , metric = 'euclidean' ) gamma = 0.5 * pdist ( self . u1 . reshape ( - 1 , 1 ), metric = 'sqeuclidean' ) if self . bins != None : # Use bins. # Variogram cloud calculation. bin_means , bin_edges , binnumber = binned_statistic ( dist , gamma , statistic = 'mean' , bins = self . bins , range = [ dist . min (), self . cutoff_dist ]) bin_width = ( bin_edges [ 1 ] - bin_edges [ 0 ]) bin_centers = bin_edges [ 1 :] - bin_width / 2 # Bin counts calculation. bin_count , bin_count_edges , bin_count_number = binned_statistic ( dist , gamma , statistic = 'count' , bins = self . bins ) bin_count_width = ( bin_count_edges [ 1 ] - bin_count_edges [ 0 ]) bin_count_centers = bin_count_edges [ 1 :] - bin_count_width / 2 if self . show_plots == True : if self . bins == None : # Variogram cloud plot. plt . figure ( dpi = 100 ) plt . scatter ( dist , gamma , ec = 'C0' , fc = 'none' , alpha = 0.3 ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () if self . bins != None : # Variogram cloud plot. plt . figure ( dpi = 100 ) plt . scatter ( dist , gamma , ec = 'C0' , fc = 'none' , alpha = 0.3 ) plt . hlines ( bin_means , bin_edges [: - 1 ], bin_edges [ 1 :], zorder = 1 , color = 'k' ) plt . scatter ( bin_centers , bin_means , ec = 'k' , lw = 0.5 ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () # Bin counts plot. plt . figure ( dpi = 100 ) plt . hlines ( bin_count , bin_count_edges [: - 1 ], bin_count_edges [ 1 :], zorder = 1 , color = 'k' ) plt . scatter ( bin_count_centers , bin_count , ec = 'k' , lw = 0.5 ) plt . ylabel ( 'Bin count' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () ############ Fit the variogram model. if not variogram_params : # Fit to the data. if self . bins == None : # Fit the variogram cloud. if self . variogram_func == gaussian or self . variogram_func == spherical : # Initial guess for the parameters. p0 = [ 0.25 * np . max ( dist ), # range np . max ( gamma ) - np . min ( gamma ), # sill np . min ( gamma ) + 1e-6 ] # nugget # Bounds with constraints. bounds = [( 1e-6 , 1e-6 , 1e-6 ), ( np . inf , np . inf , np . inf )] # Apply the cutoff. dist_cut = np . where ( dist < self . cutoff_dist , dist , np . nan ) gamma_cut = np . where ( dist < self . cutoff_dist , gamma , np . nan ) # Remove nans for curve_fit. dist_cut = dist_cut [ ~ np . isnan ( dist_cut )] gamma_cut = gamma_cut [ ~ np . isnan ( gamma_cut )] popt , pcov = curve_fit ( self . variogram_func , dist_cut , gamma_cut , p0 = p0 , bounds = bounds ) if self . show_plots == True : # Calculate 2d kde to help confirm cutoff. xi = np . linspace ( np . min ( dist ), np . max ( dist ), 60 ) yi = np . linspace ( np . min ( gamma ), np . max ( gamma ), 60 ) xi , yi = np . meshgrid ( xi , yi ) xyi = np . stack ([ xi . reshape ( - 1 ), yi . reshape ( - 1 )], axis = 1 ) . T kde = stats . gaussian_kde ([ dist , gamma ]) z = kde . evaluate ( xyi ) z = z . reshape ( len ( xi ), len ( yi )) plt . figure ( dpi = 100 ) plt . pcolormesh ( xi , yi , z , cmap = plt . cm . Blues , shading = 'auto' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () # With cutoff and variogram model. xnew = np . linspace ( np . min ( dist_cut ), self . cutoff_dist , 100 ) plt . figure ( dpi = 100 ) plt . scatter ( dist_cut , gamma_cut , fc = 'none' , ec = 'C1' , lw = 0.5 , alpha = 0.3 ) plt . plot ( xnew , self . variogram_func ( xnew , * popt ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () vrange = popt [ 0 ] sill = popt [ 1 ] nugget = popt [ 2 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'range: {:.5f} ' . format ( vrange )) print ( 'sill: {:.5f} ' . format ( sill )) print ( 'nugget: {:.5f} ' . format ( nugget )) print ( 'full sill: {:.5f} ' . format ( sill + nugget )) self . parameter_vals = [ vrange , sill , nugget ] elif self . variogram_func == linear : # Initial guess for the parameters. p0 = [( np . max ( dist ) - np . min ( dist )) / ( np . max ( gamma ) - np . min ( gamma )), # slope np . min ( gamma ) + 1e-6 ] # nugget # Bounds with constraints. bounds = [( 1e-6 , 1e-6 ), ( np . inf , np . inf )] # Apply the cutoff. dist_cut = np . where ( dist < self . cutoff_dist , dist , np . nan ) gamma_cut = np . where ( dist < self . cutoff_dist , gamma , np . nan ) # Remove nans for curve_fit. dist_cut = dist_cut [ ~ np . isnan ( dist_cut )] gamma_cut = gamma_cut [ ~ np . isnan ( gamma_cut )] popt , pcov = curve_fit ( self . variogram_func , dist_cut , gamma_cut , p0 = p0 , bounds = bounds ) if self . show_plots == True : # Calculate 2d kde to help confirm cutoff. xi = np . linspace ( np . min ( dist ), np . max ( dist ), 60 ) yi = np . linspace ( np . min ( gamma ), np . max ( gamma ), 60 ) xi , yi = np . meshgrid ( xi , yi ) xyi = np . stack ([ xi . reshape ( - 1 ), yi . reshape ( - 1 )], axis = 1 ) . T kde = stats . gaussian_kde ([ dist , gamma ]) z = kde . evaluate ( xyi ) z = z . reshape ( len ( xi ), len ( yi )) plt . figure ( dpi = 100 ) plt . pcolormesh ( xi , yi , z , cmap = plt . cm . Blues , shading = 'auto' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () # With cutoff and variogram model. xnew = np . linspace ( np . min ( dist_cut ), self . cutoff_dist , 100 ) plt . figure ( dpi = 100 ) plt . scatter ( dist_cut , gamma_cut , fc = 'none' , ec = 'C1' , lw = 0.5 , alpha = 0.3 ) plt . plot ( xnew , self . variogram_func ( xnew , * popt ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () slope = popt [ 0 ] nugget = popt [ 1 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'slope: {:.5f} ' . format ( slope )) print ( 'nugget: {:.5f} ' . format ( nugget )) self . parameter_vals = [ slope , nugget ] elif self . bins != None : # Fit the binned data. if self . variogram_func == gaussian or self . variogram_func == spherical : # Initial guess for the parameters. p0 = [ 0.25 * np . max ( bin_centers ), # range np . max ( bin_means ) - np . min ( bin_means ), # sill np . min ( bin_means )] # nugget # Bounds with constraints. bounds = [( 1e-6 , 1e-6 , 1e-6 ), ( np . inf , np . inf , np . inf )] popt , pcov = curve_fit ( self . variogram_func , bin_centers , bin_means , p0 = p0 , bounds = bounds ) if self . show_plots == True : # Fit variogram plot. plt . figure ( dpi = 100 ) plt . scatter ( bin_centers , bin_means , c = 'C1' , ec = 'k' , lw = 0.5 ) plt . plot ( bin_centers , self . variogram_func ( bin_centers , * popt ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () vrange = popt [ 0 ] sill = popt [ 1 ] nugget = popt [ 2 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'range: {:.5f} ' . format ( vrange )) print ( 'sill: {:.5f} ' . format ( sill )) print ( 'nugget: {:.5f} ' . format ( nugget )) print ( 'full sill: {:.5f} ' . format ( sill + nugget )) self . parameter_vals = [ vrange , sill , nugget ] elif self . variogram_func == linear : # Initial guess for the parameters. p0 = [( np . max ( bin_centers ) - np . min ( bin_centers )) / ( np . max ( bin_means ) - np . min ( bin_means )), # slope np . min ( bin_means )] # nugget # Bounds with constraints. bounds = [( 1e-6 , 1e-6 ), ( np . inf , np . inf )] popt , pcov = curve_fit ( self . variogram_func , bin_centers , bin_means , p0 = p0 , bounds = bounds ) if self . show_plots == True : # Fit variogram plot. plt . figure ( dpi = 100 ) plt . scatter ( bin_centers , bin_means , c = 'C1' , ec = 'k' , lw = 0.5 ) plt . plot ( bin_centers , self . variogram_func ( bin_centers , * popt ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () slope = popt [ 0 ] nugget = popt [ 1 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'slope: {:.5f} ' . format ( slope )) print ( 'nugget: {:.5f} ' . format ( nugget )) self . parameter_vals = [ slope , nugget ] else : # Use the given variogram parameters. self . parameter_vals = variogram_params # Apply the cutoff. dist_cut = np . where ( dist < self . cutoff_dist , dist , np . nan ) gamma_cut = np . where ( dist < self . cutoff_dist , gamma , np . nan ) # Remove nans for curve_fit. dist_cut = dist_cut [ ~ np . isnan ( dist_cut )] gamma_cut = gamma_cut [ ~ np . isnan ( gamma_cut )] if self . show_plots == True : if self . bins == None : # Calculate 2d kde to help confirm cutoff. xi = np . linspace ( np . min ( dist ), np . max ( dist ), 60 ) yi = np . linspace ( np . min ( gamma ), np . max ( gamma ), 60 ) xi , yi = np . meshgrid ( xi , yi ) xyi = np . stack ([ xi . reshape ( - 1 ), yi . reshape ( - 1 )], axis = 1 ) . T kde = stats . gaussian_kde ([ dist , gamma ]) z = kde . evaluate ( xyi ) z = z . reshape ( len ( xi ), len ( yi )) plt . figure ( dpi = 100 ) plt . pcolormesh ( xi , yi , z , cmap = plt . cm . Blues , shading = 'auto' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () # With cutoff and variogram model. xnew = np . linspace ( np . min ( dist_cut ), self . cutoff_dist , 100 ) plt . figure ( dpi = 100 ) plt . scatter ( dist_cut , gamma_cut , fc = 'none' , ec = 'C1' , lw = 0.5 , alpha = 0.3 ) plt . plot ( xnew , self . variogram_func ( xnew , * self . parameter_vals ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () else : # Fit variogram plot. plt . figure ( dpi = 100 ) plt . scatter ( bin_centers , bin_means , fc = 'C1' , ec = 'k' , lw = 0.5 ) plt . plot ( bin_centers , self . variogram_func ( bin_centers , * self . parameter_vals ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () if self . variogram_func == gaussian or self . variogram_func == spherical : vrange = self . parameter_vals [ 0 ] sill = self . parameter_vals [ 1 ] nugget = self . parameter_vals [ 2 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'range: {:.5f} ' . format ( vrange )) print ( 'sill: {:.5f} ' . format ( sill )) print ( 'nugget: {:.5f} ' . format ( nugget )) print ( 'full sill: {:.5f} ' . format ( sill + nugget )) elif self . variogram_func == linear : slope = self . parameter_vals [ 0 ] nugget = self . parameter_vals [ 1 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'slope: {:.5f} ' . format ( slope )) print ( 'nugget: {:.5f} ' . format ( nugget )) #################################################### def predict ( self , x2_pred ): ''' Parameters: x2 : n-dim array Locations to make kriging predictions. Returns: u2_mean : float Kriging mean. u2_var : float Kriging variance. Performs ordinary or universal kriging using the estimated variogram parameters. ''' self . x2 = self . project ( x2_pred ) n1 = len ( self . x1 ) n2 = len ( self . x2 ) # Universal krige. if self . featurization : # Ax = b with a trend. # Build A D1 = cdist ( self . x1 , self . x1 ) drift_data = np . array ( list ( self . featurization ( self . x1 ))) An = n1 + 1 + drift_data . shape [ 0 ] A = np . zeros (( An , An )) A [: n1 , : n1 ] = - self . variogram_func ( D1 , * self . parameter_vals ) np . fill_diagonal ( A , 0. ) A [ n1 , : n1 ] = 1. A [: n1 , n1 ] = 1. A [ n1 , n1 ] = 0. # Add in the trend for A. for i in range ( drift_data . shape [ 0 ]): A [ n1 + i + 1 , : n1 ] = drift_data [ i ] A [: n1 , n1 + i + 1 ] = drift_data [ i ] # Build b. D2 = cdist ( self . x2 , self . x1 ) b = np . zeros (( D2 . shape [ 0 ], D2 . shape [ 1 ] + 1 + drift_data . shape [ 0 ])) b [: n2 , : n1 ] = - self . variogram_func ( D2 , * self . parameter_vals ) b = b . T b [ n1 , :] = 1. # Add the trend for b. drift_pred = np . array ( list ( self . featurization ( self . x2 ))) for i in range ( drift_pred . shape [ 0 ]): b [ n1 + 1 + i , :] = drift_pred [ i ] # Solve. x = np . linalg . solve ( A , b ) u2_mean = np . tensordot ( self . u1 , x [: n1 ], axes = 1 ) u2_var = np . sum ( x . T * - b . T , axis = 1 ) return u2_mean , u2_var # Ordinary krige. else : # Ax = b. # Build A. D1 = cdist ( self . x1 , self . x1 ) A = np . zeros (( n1 + 1 , n1 + 1 )) A [: n1 , : n1 ] = - self . variogram_func ( D1 , * self . parameter_vals ) np . fill_diagonal ( A , 0. ) A [ n1 , :] = 1. A [:, n1 ] = 1. A [ n1 , n1 ] = 0. # Build b. D2 = cdist ( self . x2 , self . x1 ) b = np . zeros (( D2 . shape [ 0 ], D2 . shape [ 1 ] + 1 )) b [: n2 , : n1 ] = - self . variogram_func ( D2 , * self . parameter_vals ) b = b . T b [ n1 , :] = 1. # Solve. x = np . linalg . solve ( A , b ) u2_mean = np . tensordot ( self . u1 , x [: n1 ], axes = 1 ) u2_var = np . sum ( x . T * - b . T , axis = 1 ) return u2_mean , u2_var ######################################################### # Access the projected coords of the input data. def get_projected ( self ): return self . x1 [:, 0 ], self . x1 [:, 1 ] ######################################################### # Provide some stats on the model. def stats ( self ): u2_mean_for_u1 , u2_var_for_u1 = self . predict ( self . x1_original ) self . residuals = self . u1 - u2_mean_for_u1 self . res_mean = np . mean ( self . residuals ) self . res_std = np . std ( self . residuals ) self . res_skew = stats . skew ( self . residuals ) self . res_kurt = stats . kurtosis ( self . residuals ) if self . show_plots == True : plt . figure () plt . hist ( self . u1 , label = 'Input values' ) plt . ylabel ( 'Count' ) plt . xlabel ( 'Input values' ) plt . show () plt . figure () plt . hist ( self . residuals , label = 'Residuals' ) plt . ylabel ( 'Count' ) plt . xlabel ( 'Residuals' ) plt . show () if self . verbose == True : print ( 'Residual mean: {:.3e} ' . format ( self . res_mean )) print ( 'Residual standard deviation: {:.3f} ' . format ( self . res_std )) print ( 'Residual skewness: {:.3f} ' . format ( self . res_skew )) print ( 'Residual kurtosis: {:.3f} ' . format ( self . res_kurt )) def get_residuals ( self ): return self . residuals def get_residual_moments ( self ): return dict ( mean = self . res_mean , std = self . res_std , skew = self . res_skew , kurt = self . res_kurt ) __init__ ( x1 , u1 , bins , variogram_func = None , variogram_params = None , cutoff_dist = 'auto' , featurization = None , projection = None , show_plots = True , verbose = True ) Parameters: Name Type Description Default x1 n-dim array Locations of input data. required u1 1-d array Values to be kriged. required bins int or None The number of bins to use on the variogram cloud. If None, variogram function is fit to the variogram cloud and is not binned first. required variogram_func str Name of the variogram model to use in the kriging. Should be 'linear', 'gaussian', or 'spherical'. Default is 'gaussian'. None cutoff_dist str or int, optional The maximum lag distance to include in variogram modeling. 'auto' featurization function, optional Should be a function that takes x1 (n-dim array of input data) and returns the coordinates, i.e., x, y, x 2, y 2. Example below. Default is None. None project function, opt A function that takes multiple vectors, and returns a tuple of projected vectors. required epsg_proj str The projected coordinate system to use. Ignored if project=False. Default is 'EPSG:3310' (California Albers). required show_plots boolean, optional Whether or not to show variogram plots. Default is True. True verbose boolean, optional Whether or not to print parameters. Default is True. True Performs experimental variogram calculation, bins data, and fits variogram model to estimate variogram parameters. Performs ordinary and universal kriging in up to 3 spatial dimensions. Trend model example: def featurization(x1): return x1[:, 0], x1[:, 1], x1[:, 0] 2, x1[:, 1] 2 Source code in src/geostat/krige.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 def __init__ ( self , x1 , u1 , bins , variogram_func = None , variogram_params = None , cutoff_dist = 'auto' , featurization = None , projection = None , show_plots = True , verbose = True , ): ''' Parameters: x1 : n-dim array Locations of input data. u1 : 1-d array Values to be kriged. bins : int or None The number of bins to use on the variogram cloud. If None, variogram function is fit to the variogram cloud and is not binned first. variogram_func : str Name of the variogram model to use in the kriging. Should be 'linear', 'gaussian', or 'spherical'. Default is 'gaussian'. cutoff_dist : str or int, optional The maximum lag distance to include in variogram modeling. featurization : function, optional Should be a function that takes x1 (n-dim array of input data) and returns the coordinates, i.e., x, y, x**2, y**2. Example below. Default is None. project : function, opt A function that takes multiple vectors, and returns a tuple of projected vectors. epsg_proj : str The projected coordinate system to use. Ignored if project=False. Default is 'EPSG:3310' (California Albers). show_plots : boolean, optional Whether or not to show variogram plots. Default is True. verbose : boolean, optional Whether or not to print parameters. Default is True. Performs experimental variogram calculation, bins data, and fits variogram model to estimate variogram parameters. Performs ordinary and universal kriging in up to 3 spatial dimensions. Trend model example: def featurization(x1): return x1[:, 0], x1[:, 1], x1[:, 0]**2, x1[:, 1]**2 ''' super () . __init__ ( projection = projection ) # Save original x1 for stats function. self . x1_original = x1 # Check and change the shape of u1 if needed (for pdist). if u1 . ndim == 1 : self . u1 = u1 elif u1 . shape [ 1 ] == 1 : self . u1 = u1 . reshape ( - 1 ) else : raise ValueError ( \"Check dimensions of 'u1'.\" ) # Projection. self . x1 = self . project ( x1 ) # Variogram. if variogram_func == 'gaussian' : self . variogram_func = gaussian elif variogram_func == 'spherical' : self . variogram_func = spherical elif variogram_func == 'linear' : self . variogram_func = linear else : raise ValueError ( \"Variogram function must be 'linear', 'gaussian', or 'spherical'.\" ) if cutoff_dist == 'auto' : self . cutoff_dist = cutoff_dist_func ( self . x1 ) else : self . cutoff_dist = cutoff_dist self . verbose = verbose self . show_plots = show_plots self . bins = bins self . featurization = featurization # Lags and semivariance. dist = pdist ( self . x1 , metric = 'euclidean' ) gamma = 0.5 * pdist ( self . u1 . reshape ( - 1 , 1 ), metric = 'sqeuclidean' ) if self . bins != None : # Use bins. # Variogram cloud calculation. bin_means , bin_edges , binnumber = binned_statistic ( dist , gamma , statistic = 'mean' , bins = self . bins , range = [ dist . min (), self . cutoff_dist ]) bin_width = ( bin_edges [ 1 ] - bin_edges [ 0 ]) bin_centers = bin_edges [ 1 :] - bin_width / 2 # Bin counts calculation. bin_count , bin_count_edges , bin_count_number = binned_statistic ( dist , gamma , statistic = 'count' , bins = self . bins ) bin_count_width = ( bin_count_edges [ 1 ] - bin_count_edges [ 0 ]) bin_count_centers = bin_count_edges [ 1 :] - bin_count_width / 2 if self . show_plots == True : if self . bins == None : # Variogram cloud plot. plt . figure ( dpi = 100 ) plt . scatter ( dist , gamma , ec = 'C0' , fc = 'none' , alpha = 0.3 ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () if self . bins != None : # Variogram cloud plot. plt . figure ( dpi = 100 ) plt . scatter ( dist , gamma , ec = 'C0' , fc = 'none' , alpha = 0.3 ) plt . hlines ( bin_means , bin_edges [: - 1 ], bin_edges [ 1 :], zorder = 1 , color = 'k' ) plt . scatter ( bin_centers , bin_means , ec = 'k' , lw = 0.5 ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () # Bin counts plot. plt . figure ( dpi = 100 ) plt . hlines ( bin_count , bin_count_edges [: - 1 ], bin_count_edges [ 1 :], zorder = 1 , color = 'k' ) plt . scatter ( bin_count_centers , bin_count , ec = 'k' , lw = 0.5 ) plt . ylabel ( 'Bin count' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () ############ Fit the variogram model. if not variogram_params : # Fit to the data. if self . bins == None : # Fit the variogram cloud. if self . variogram_func == gaussian or self . variogram_func == spherical : # Initial guess for the parameters. p0 = [ 0.25 * np . max ( dist ), # range np . max ( gamma ) - np . min ( gamma ), # sill np . min ( gamma ) + 1e-6 ] # nugget # Bounds with constraints. bounds = [( 1e-6 , 1e-6 , 1e-6 ), ( np . inf , np . inf , np . inf )] # Apply the cutoff. dist_cut = np . where ( dist < self . cutoff_dist , dist , np . nan ) gamma_cut = np . where ( dist < self . cutoff_dist , gamma , np . nan ) # Remove nans for curve_fit. dist_cut = dist_cut [ ~ np . isnan ( dist_cut )] gamma_cut = gamma_cut [ ~ np . isnan ( gamma_cut )] popt , pcov = curve_fit ( self . variogram_func , dist_cut , gamma_cut , p0 = p0 , bounds = bounds ) if self . show_plots == True : # Calculate 2d kde to help confirm cutoff. xi = np . linspace ( np . min ( dist ), np . max ( dist ), 60 ) yi = np . linspace ( np . min ( gamma ), np . max ( gamma ), 60 ) xi , yi = np . meshgrid ( xi , yi ) xyi = np . stack ([ xi . reshape ( - 1 ), yi . reshape ( - 1 )], axis = 1 ) . T kde = stats . gaussian_kde ([ dist , gamma ]) z = kde . evaluate ( xyi ) z = z . reshape ( len ( xi ), len ( yi )) plt . figure ( dpi = 100 ) plt . pcolormesh ( xi , yi , z , cmap = plt . cm . Blues , shading = 'auto' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () # With cutoff and variogram model. xnew = np . linspace ( np . min ( dist_cut ), self . cutoff_dist , 100 ) plt . figure ( dpi = 100 ) plt . scatter ( dist_cut , gamma_cut , fc = 'none' , ec = 'C1' , lw = 0.5 , alpha = 0.3 ) plt . plot ( xnew , self . variogram_func ( xnew , * popt ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () vrange = popt [ 0 ] sill = popt [ 1 ] nugget = popt [ 2 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'range: {:.5f} ' . format ( vrange )) print ( 'sill: {:.5f} ' . format ( sill )) print ( 'nugget: {:.5f} ' . format ( nugget )) print ( 'full sill: {:.5f} ' . format ( sill + nugget )) self . parameter_vals = [ vrange , sill , nugget ] elif self . variogram_func == linear : # Initial guess for the parameters. p0 = [( np . max ( dist ) - np . min ( dist )) / ( np . max ( gamma ) - np . min ( gamma )), # slope np . min ( gamma ) + 1e-6 ] # nugget # Bounds with constraints. bounds = [( 1e-6 , 1e-6 ), ( np . inf , np . inf )] # Apply the cutoff. dist_cut = np . where ( dist < self . cutoff_dist , dist , np . nan ) gamma_cut = np . where ( dist < self . cutoff_dist , gamma , np . nan ) # Remove nans for curve_fit. dist_cut = dist_cut [ ~ np . isnan ( dist_cut )] gamma_cut = gamma_cut [ ~ np . isnan ( gamma_cut )] popt , pcov = curve_fit ( self . variogram_func , dist_cut , gamma_cut , p0 = p0 , bounds = bounds ) if self . show_plots == True : # Calculate 2d kde to help confirm cutoff. xi = np . linspace ( np . min ( dist ), np . max ( dist ), 60 ) yi = np . linspace ( np . min ( gamma ), np . max ( gamma ), 60 ) xi , yi = np . meshgrid ( xi , yi ) xyi = np . stack ([ xi . reshape ( - 1 ), yi . reshape ( - 1 )], axis = 1 ) . T kde = stats . gaussian_kde ([ dist , gamma ]) z = kde . evaluate ( xyi ) z = z . reshape ( len ( xi ), len ( yi )) plt . figure ( dpi = 100 ) plt . pcolormesh ( xi , yi , z , cmap = plt . cm . Blues , shading = 'auto' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () # With cutoff and variogram model. xnew = np . linspace ( np . min ( dist_cut ), self . cutoff_dist , 100 ) plt . figure ( dpi = 100 ) plt . scatter ( dist_cut , gamma_cut , fc = 'none' , ec = 'C1' , lw = 0.5 , alpha = 0.3 ) plt . plot ( xnew , self . variogram_func ( xnew , * popt ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () slope = popt [ 0 ] nugget = popt [ 1 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'slope: {:.5f} ' . format ( slope )) print ( 'nugget: {:.5f} ' . format ( nugget )) self . parameter_vals = [ slope , nugget ] elif self . bins != None : # Fit the binned data. if self . variogram_func == gaussian or self . variogram_func == spherical : # Initial guess for the parameters. p0 = [ 0.25 * np . max ( bin_centers ), # range np . max ( bin_means ) - np . min ( bin_means ), # sill np . min ( bin_means )] # nugget # Bounds with constraints. bounds = [( 1e-6 , 1e-6 , 1e-6 ), ( np . inf , np . inf , np . inf )] popt , pcov = curve_fit ( self . variogram_func , bin_centers , bin_means , p0 = p0 , bounds = bounds ) if self . show_plots == True : # Fit variogram plot. plt . figure ( dpi = 100 ) plt . scatter ( bin_centers , bin_means , c = 'C1' , ec = 'k' , lw = 0.5 ) plt . plot ( bin_centers , self . variogram_func ( bin_centers , * popt ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () vrange = popt [ 0 ] sill = popt [ 1 ] nugget = popt [ 2 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'range: {:.5f} ' . format ( vrange )) print ( 'sill: {:.5f} ' . format ( sill )) print ( 'nugget: {:.5f} ' . format ( nugget )) print ( 'full sill: {:.5f} ' . format ( sill + nugget )) self . parameter_vals = [ vrange , sill , nugget ] elif self . variogram_func == linear : # Initial guess for the parameters. p0 = [( np . max ( bin_centers ) - np . min ( bin_centers )) / ( np . max ( bin_means ) - np . min ( bin_means )), # slope np . min ( bin_means )] # nugget # Bounds with constraints. bounds = [( 1e-6 , 1e-6 ), ( np . inf , np . inf )] popt , pcov = curve_fit ( self . variogram_func , bin_centers , bin_means , p0 = p0 , bounds = bounds ) if self . show_plots == True : # Fit variogram plot. plt . figure ( dpi = 100 ) plt . scatter ( bin_centers , bin_means , c = 'C1' , ec = 'k' , lw = 0.5 ) plt . plot ( bin_centers , self . variogram_func ( bin_centers , * popt ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () slope = popt [ 0 ] nugget = popt [ 1 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'slope: {:.5f} ' . format ( slope )) print ( 'nugget: {:.5f} ' . format ( nugget )) self . parameter_vals = [ slope , nugget ] else : # Use the given variogram parameters. self . parameter_vals = variogram_params # Apply the cutoff. dist_cut = np . where ( dist < self . cutoff_dist , dist , np . nan ) gamma_cut = np . where ( dist < self . cutoff_dist , gamma , np . nan ) # Remove nans for curve_fit. dist_cut = dist_cut [ ~ np . isnan ( dist_cut )] gamma_cut = gamma_cut [ ~ np . isnan ( gamma_cut )] if self . show_plots == True : if self . bins == None : # Calculate 2d kde to help confirm cutoff. xi = np . linspace ( np . min ( dist ), np . max ( dist ), 60 ) yi = np . linspace ( np . min ( gamma ), np . max ( gamma ), 60 ) xi , yi = np . meshgrid ( xi , yi ) xyi = np . stack ([ xi . reshape ( - 1 ), yi . reshape ( - 1 )], axis = 1 ) . T kde = stats . gaussian_kde ([ dist , gamma ]) z = kde . evaluate ( xyi ) z = z . reshape ( len ( xi ), len ( yi )) plt . figure ( dpi = 100 ) plt . pcolormesh ( xi , yi , z , cmap = plt . cm . Blues , shading = 'auto' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () # With cutoff and variogram model. xnew = np . linspace ( np . min ( dist_cut ), self . cutoff_dist , 100 ) plt . figure ( dpi = 100 ) plt . scatter ( dist_cut , gamma_cut , fc = 'none' , ec = 'C1' , lw = 0.5 , alpha = 0.3 ) plt . plot ( xnew , self . variogram_func ( xnew , * self . parameter_vals ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () else : # Fit variogram plot. plt . figure ( dpi = 100 ) plt . scatter ( bin_centers , bin_means , fc = 'C1' , ec = 'k' , lw = 0.5 ) plt . plot ( bin_centers , self . variogram_func ( bin_centers , * self . parameter_vals ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () if self . variogram_func == gaussian or self . variogram_func == spherical : vrange = self . parameter_vals [ 0 ] sill = self . parameter_vals [ 1 ] nugget = self . parameter_vals [ 2 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'range: {:.5f} ' . format ( vrange )) print ( 'sill: {:.5f} ' . format ( sill )) print ( 'nugget: {:.5f} ' . format ( nugget )) print ( 'full sill: {:.5f} ' . format ( sill + nugget )) elif self . variogram_func == linear : slope = self . parameter_vals [ 0 ] nugget = self . parameter_vals [ 1 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'slope: {:.5f} ' . format ( slope )) print ( 'nugget: {:.5f} ' . format ( nugget )) predict ( x2_pred ) Parameters: Name Type Description Default x2 n-dim array Locations to make kriging predictions. required Returns: Name Type Description u2_mean float Kriging mean. u2_var float Kriging variance. Performs ordinary or universal kriging using the estimated variogram parameters. Source code in src/geostat/krige.py 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 def predict ( self , x2_pred ): ''' Parameters: x2 : n-dim array Locations to make kriging predictions. Returns: u2_mean : float Kriging mean. u2_var : float Kriging variance. Performs ordinary or universal kriging using the estimated variogram parameters. ''' self . x2 = self . project ( x2_pred ) n1 = len ( self . x1 ) n2 = len ( self . x2 ) # Universal krige. if self . featurization : # Ax = b with a trend. # Build A D1 = cdist ( self . x1 , self . x1 ) drift_data = np . array ( list ( self . featurization ( self . x1 ))) An = n1 + 1 + drift_data . shape [ 0 ] A = np . zeros (( An , An )) A [: n1 , : n1 ] = - self . variogram_func ( D1 , * self . parameter_vals ) np . fill_diagonal ( A , 0. ) A [ n1 , : n1 ] = 1. A [: n1 , n1 ] = 1. A [ n1 , n1 ] = 0. # Add in the trend for A. for i in range ( drift_data . shape [ 0 ]): A [ n1 + i + 1 , : n1 ] = drift_data [ i ] A [: n1 , n1 + i + 1 ] = drift_data [ i ] # Build b. D2 = cdist ( self . x2 , self . x1 ) b = np . zeros (( D2 . shape [ 0 ], D2 . shape [ 1 ] + 1 + drift_data . shape [ 0 ])) b [: n2 , : n1 ] = - self . variogram_func ( D2 , * self . parameter_vals ) b = b . T b [ n1 , :] = 1. # Add the trend for b. drift_pred = np . array ( list ( self . featurization ( self . x2 ))) for i in range ( drift_pred . shape [ 0 ]): b [ n1 + 1 + i , :] = drift_pred [ i ] # Solve. x = np . linalg . solve ( A , b ) u2_mean = np . tensordot ( self . u1 , x [: n1 ], axes = 1 ) u2_var = np . sum ( x . T * - b . T , axis = 1 ) return u2_mean , u2_var # Ordinary krige. else : # Ax = b. # Build A. D1 = cdist ( self . x1 , self . x1 ) A = np . zeros (( n1 + 1 , n1 + 1 )) A [: n1 , : n1 ] = - self . variogram_func ( D1 , * self . parameter_vals ) np . fill_diagonal ( A , 0. ) A [ n1 , :] = 1. A [:, n1 ] = 1. A [ n1 , n1 ] = 0. # Build b. D2 = cdist ( self . x2 , self . x1 ) b = np . zeros (( D2 . shape [ 0 ], D2 . shape [ 1 ] + 1 )) b [: n2 , : n1 ] = - self . variogram_func ( D2 , * self . parameter_vals ) b = b . T b [ n1 , :] = 1. # Solve. x = np . linalg . solve ( A , b ) u2_mean = np . tensordot ( self . u1 , x [: n1 ], axes = 1 ) u2_var = np . sum ( x . T * - b . T , axis = 1 ) return u2_mean , u2_var cutoff_dist_func ( x ) Parameters: Name Type Description Default x n-dim array Locations of input data. required Returns: Name Type Description cutoff float The maximum lag distance to use in fitting the variogram. Found using Pythagorean Theorem to roughly find one half the distance across the study area. Source code in src/geostat/krige.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def cutoff_dist_func ( x ): ''' Parameters: x : n-dim array Locations of input data. Returns: cutoff : float The maximum lag distance to use in fitting the variogram. Found using Pythagorean Theorem to roughly find one half the distance across the study area. ''' a2 = np . square ( x [:, 0 ] . max () - x [:, 0 ] . min ()) b2 = np . square ( x [:, 1 ] . max () - x [:, 1 ] . min ()) cutoff = np . sqrt ( a2 + b2 ) / 2 return cutoff src.geostat.mean Mean Bases: Op Source code in src/geostat/mean.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class Mean ( Op ): def __init__ ( self , fa , autoinputs ): if 'locs1' not in autoinputs : autoinputs [ 'locs1' ] = 'locs1' super () . __init__ ( fa , autoinputs ) def __add__ ( self , other ): if isinstance ( other , ZeroTrend ): return self elif isinstance ( self , ZeroTrend ): return other else : return Stack ([ self ]) + other def call ( self , e ): pass def __call__ ( self , e ): \"\"\" Returns tuple `(mean, covariance)` for locations. Return values have correct shapes. \"\"\" M = self . call ( e ) if M is None : M = 0. n1 = tf . shape ( e [ 'locs1' ])[ 0 ] M = tf . broadcast_to ( M , [ n1 ]) return M __call__ ( e ) Returns tuple (mean, covariance) for locations. Return values have correct shapes. Source code in src/geostat/mean.py 39 40 41 42 43 44 45 46 47 48 def __call__ ( self , e ): \"\"\" Returns tuple `(mean, covariance)` for locations. Return values have correct shapes. \"\"\" M = self . call ( e ) if M is None : M = 0. n1 = tf . shape ( e [ 'locs1' ])[ 0 ] M = tf . broadcast_to ( M , [ n1 ]) return M src.geostat.mesh src.geostat.metric src.geostat.model Featurizer Produces featurized locations (F matrix). Source code in src/geostat/model.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 class Featurizer : \"\"\" Produces featurized locations (F matrix). \"\"\" def __init__ ( self , featurization ): self . featurization = featurization def __call__ ( self , locs ): locs = tf . cast ( locs , tf . float32 ) if self . featurization is None : # No features. return tf . ones ([ tf . shape ( locs )[ 0 ], 0 ], dtype = tf . float32 ) feats = self . featurization ( * tf . unstack ( locs , axis = 1 )) if isinstance ( feats , tuple ): # One or many features. if len ( feats ) == 0 : return tf . ones ([ tf . shape ( locs )[ 0 ], 0 ], dtype = tf . float32 ) else : feats = self . featurization ( * tf . unstack ( locs , axis = 1 )) feats = [ tf . broadcast_to ( tf . cast ( f , tf . float32 ), [ tf . shape ( locs )[ 0 ]]) for f in feats ] return tf . stack ( feats , axis = 1 ) else : # One feature. return e ( feats ) Model dataclass Source code in src/geostat/model.py 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 @dataclass class Model (): gp : GP warp : Warp = None parameter_sample_size : Optional [ int ] = None locs : np . ndarray = None vals : np . ndarray = None cats : np . ndarray = None report : Callable = None verbose : bool = True def __post_init__ ( self ): ''' Parameters: x : Pandas DataFrame with columns for locations. u : A Pandas Series containing observations. featurization : function, optional Should be a function that takes x1 (n-dim array of input data) and returns the coordinates, i.e., x, y, x**2, y**2. Example: def featurization(x1): return x1[:, 0], x1[:, 1], x1[:, 0]**2, x1[:, 1]**2. Default is None. latent : List[GP] Name of the covariance function to use in the GP. Should be 'squared-exp' or 'gamma-exp'. Default is 'squared-exp'. verbose : boolean, optional Whether or not to print parameters. Default is True. Performs Gaussian process training and prediction. ''' if self . warp is None : self . warp = NoWarp () # Default reporting function. def default_report ( p , prefix = None ): if prefix : print ( prefix , end = ' ' ) def fmt ( x ): if isinstance ( x , tf . Tensor ): x = x . numpy () if isinstance ( x , ( int , np . int32 , np . int64 )): return ' {:5d} ' . format ( x ) if isinstance ( x , ( float , np . float32 , np . float64 )): return ' {:5.2f} ' . format ( x ) else : with np . printoptions ( precision = 2 , formatter = { 'floatkind' : ' {:5.2f} ' . format }): return str ( x ) print ( '[ %s ]' % ( ' ' . join ( ' %s %s ' % ( k , fmt ( v )) for k , v in p . items ()))) if self . report == None : self . report = default_report if self . locs is not None : self . locs = np . array ( self . locs ) if self . vals is not None : self . vals = np . array ( self . vals ) if self . cats is not None : self . cats = np . array ( self . cats ) # Collect parameters and create TF parameters. for p in self . gather_vars () . values (): p . create_tf_variable () def gather_vars ( self ): return self . gp . gather_vars () | self . warp . gather_vars () def set ( self , ** values ): parameters = self . gather_vars () for name , v in values . items (): if name in parameters : parameters [ name ] . value = v parameters [ name ] . create_tf_variable () else : raise ValueError ( f \" { k } is not a parameter\" ) return self def fit ( self , locs , vals , cats = None , step_size = 0.01 , iters = 100 , reg = None ): # Collect parameters and create TF parameters. parameters = self . gather_vars () # Permute datapoints if cats is given. if cats is not None : cats = np . array ( cats ) perm = np . argsort ( cats ) locs , vals , cats = locs [ perm ], vals [ perm ], cats [ perm ] else : cats = np . zeros ( locs . shape [: 1 ], np . int32 ) perm = None # Data dict. self . data = { 'warplocs' : self . warp ( locs ), 'vals' : tf . constant ( vals , dtype = tf . float32 ), 'cats' : tf . constant ( cats , dtype = tf . int32 )} optimizer = tf . keras . optimizers . Adam ( learning_rate = step_size ) j = 0 # Iteration count. for i in range ( 10 ): t0 = time . time () while j < ( i + 1 ) * iters / 10 : ll , reg_penalty = gp_train_step ( optimizer , self . data , parameters , self . gp , reg ) j += 1 time_elapsed = time . time () - t0 if self . verbose == True : self . report ( dict ( iter = j , ll = ll , time = time_elapsed , reg = reg_penalty ) | { p . name : p . surface () for p in parameters . values ()}) # Save parameter values. for p in parameters . values (): p . update_value () # Restore order if things were permuted. if perm is not None : revperm = np . argsort ( perm ) locs , vals , cats = locs [ revperm ], vals [ revperm ], cats [ revperm ] self . locs = locs self . vals = vals self . cats = cats return self def mcmc ( self , locs , vals , cats = None , chains = 4 , step_size = 0.1 , move_prob = 0.5 , samples = 1000 , burnin = 500 , report_interval = 100 ): assert samples % report_interval == 0 , '`samples` must be a multiple of `report_interval`' assert burnin % report_interval == 0 , '`burnin` must be a multiple of `report_interval`' # Permute datapoints if cats is given. if cats is not None : cats = np . array ( cats ) perm = np . argsort ( cats ) locs , vals , cats = locs [ perm ], vals [ perm ], cats [ perm ] # Data dict. self . data = { 'locs' : tf . constant ( locs , dtype = tf . float32 ), 'vals' : tf . constant ( vals , dtype = tf . float32 ), 'cats' : None if cats is None else tf . constant ( cats , dtype = tf . int32 )} # Initial MCMC state. initial_up = self . parameter_space . get_underlying ( self . parameters ) # Unnormalized log posterior distribution. def g ( up ): sp = self . parameter_space . get_surface ( up ) return gp_log_likelihood ( self . data , sp , self . gp ) def f ( * up_flat ): up = tf . nest . pack_sequence_as ( initial_up , up_flat ) ll = tf . map_fn ( g , up , fn_output_signature = tf . float32 ) # log_prior = -tf.reduce_sum(tf.math.log(1. + tf.square(up_flat)), axis=0) return ll # + log_prior # Run the chain for a burst. @tf . function def run_chain ( current_state , final_results , kernel , iters ): samples , results , final_results = tfp . mcmc . sample_chain ( num_results = iters , current_state = current_state , kernel = kernel , return_final_kernel_results = True , trace_fn = lambda _ , results : results ) return samples , results , final_results def new_state_fn ( scale , dtype ): direction_dist = tfd . Normal ( loc = dtype ( 0 ), scale = dtype ( 1 )) scale_dist = tfd . Exponential ( rate = dtype ( 1 / scale )) pick_dist = tfd . Bernoulli ( probs = move_prob ) def _fn ( state_parts , seed ): next_state_parts = [] part_seeds = tfp . random . split_seed ( seed , n = len ( state_parts ), salt = 'rwmcauchy' ) for sp , ps in zip ( state_parts , part_seeds ): pick = tf . cast ( pick_dist . sample ( sample_shape = sp . shape , seed = ps ), tf . float32 ) direction = direction_dist . sample ( sample_shape = sp . shape , seed = ps ) scale_val = scale_dist . sample ( seed = ps ) next_state_parts . append ( sp + tf . einsum ( 'a...,a->a...' , pick * direction , scale_val )) return next_state_parts return _fn inv_temps = 0.5 ** np . arange ( chains , dtype = np . float32 ) def make_kernel_fn ( target_log_prob_fn ): return tfp . mcmc . RandomWalkMetropolis ( target_log_prob_fn = target_log_prob_fn , new_state_fn = new_state_fn ( scale = step_size / np . sqrt ( inv_temps ), dtype = np . float32 )) kernel = tfp . mcmc . ReplicaExchangeMC ( target_log_prob_fn = f , inverse_temperatures = inv_temps , make_kernel_fn = make_kernel_fn ) # Do bursts. current_state = tf . nest . flatten ( initial_up ) final_results = None acc_states = [] num_bursts = ( samples + burnin ) // report_interval burnin_bursts = burnin // report_interval for i in range ( num_bursts ): is_burnin = i < burnin_bursts if self . verbose and ( i == 0 or i == burnin_bursts ): print ( 'BURNIN \\n ' if is_burnin else ' \\n SAMPLING' ) t0 = time . time () states , results , final_results = run_chain ( current_state , final_results , kernel , report_interval ) if self . verbose == True : if not is_burnin : print () accept_rates = results . post_swap_replica_results . is_accepted . numpy () . mean ( axis = 0 ) print ( '[iter {:4d} ] [time {:.1f} ] [accept rates {} ]' . format ( (( i if is_burnin else i - burnin_bursts ) + 1 ) * report_interval , time . time () - t0 , ' ' . join ([ f ' { x : .2f } ' for x in accept_rates . tolist ()]))) if not is_burnin : acc_states . append ( tf . nest . map_structure ( lambda x : x . numpy (), states )) all_states = [ np . concatenate ( x , 0 ) for x in zip ( * acc_states )] up = tf . nest . pack_sequence_as ( initial_up , all_states ) sp = self . parameter_space . get_surface ( up , numpy = True ) # Reporting if self . verbose == True : for p in [ 5 , 50 , 95 ]: x = tf . nest . map_structure ( lambda x : np . percentile ( x , p , axis = 0 ), sp ) self . report ( x , prefix = f ' { p : 02d } %ile' ) current_state = [ s [ - 1 ] for s in states ] posterior = self . parameter_space . get_surface ( up , numpy = True ) # Restore order if things were permuted. if cats is not None : revperm = np . argsort ( perm ) locs , vals , cats = locs [ revperm ], vals [ revperm ], cats [ revperm ] return replace ( self , parameters = posterior , parameter_sample_size = samples , locs = locs , vals = vals , cats = cats ) def generate ( self , locs , cats = None ): assert self . locs is None and self . vals is None , 'Conditional generation not yet supported' assert self . parameter_sample_size is None , 'Generation from a distribution not yet supported' locs = np . array ( locs ) # Permute datapoints if cats is given. if cats is not None : cats = np . array ( cats ) perm = np . argsort ( cats ) locs , cats = locs [ perm ], cats [ perm ] else : cats = np . zeros ( locs . shape [: 1 ], np . int32 ) perm = None m , S = gp_covariance ( self . gp , self . warp ( locs ) . run ({}), None if cats is None else tf . constant ( cats , dtype = tf . int32 )) vals = MVN ( m , tf . linalg . cholesky ( S )) . sample () . numpy () # Restore order if things were permuted. if perm is not None : revperm = np . argsort ( perm ) locs , vals , cats = locs [ revperm ], vals [ revperm ], cats [ revperm ] self . locs = locs self . vals = vals self . cats = cats return self def predict ( self , locs2 , cats2 = None , * , subsample = None , reduce = None , tracker = None , pair = False ): ''' Performs GP predictions of the mean and variance. Has support for batch predictions for large data sets. ''' assert subsample is None or self . parameter_sample_size is not None , \\ '`subsample` is only valid with sampled parameters' assert reduce is None or self . parameter_sample_size is not None , \\ '`reduce` is only valid with sampled parameters' assert subsample is None or reduce is None , \\ '`subsample` and `reduce` cannot both be given' if tracker is None : tracker = lambda x : x assert self . locs . shape [ - 1 ] == locs2 . shape [ - 1 ], 'Mismatch in location dimensions' if cats2 is not None : assert cats2 . shape == locs2 . shape [: 1 ], 'Mismatched shapes in cats and locs' else : cats2 = np . zeros ( locs2 . shape [: 1 ], np . int32 ) def interpolate_batch ( A11i , locs1 , vals1diff , cats1 , locs2 , cats2 ): \"\"\" Inputs: locs1.shape = [N1, K] vals1diff.shape = [N1] cats1.shape = [N1] locs2.shape = [N2, K] cats2.shape = [N2] Outputs: u2_mean.shape = [N2] u2_var.shape = [N2] \"\"\" N1 = len ( locs1 ) # Number of measurements. # Permute datapoints if cats is given. if cats2 is not None : perm = np . argsort ( cats2 ) locs2 , cats2 = locs2 [ perm ], cats2 [ perm ] locs2 = self . warp ( locs2 ) . run ({}) _ , A12 = gp_covariance2 ( self . gp , tf . constant ( locs1 , dtype = tf . float32 ), tf . constant ( cats1 , dtype = tf . int32 ), tf . constant ( locs2 , dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 ), N1 ) m2 , A22 = gp_covariance ( self . gp , tf . constant ( locs2 , dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 )) # Restore order if things were permuted. if cats2 is not None : revperm = np . argsort ( perm ) m2 = tf . gather ( m2 , revperm ) A12 = tf . gather ( A12 , revperm , axis =- 1 ) A22 = tf . gather ( tf . gather ( A22 , revperm ), revperm , axis =- 1 ) u2_mean = m2 + tf . einsum ( 'ab,a->b' , A12 , tf . einsum ( 'ab,b->a' , A11i , vals1diff )) u2_var = tf . linalg . diag_part ( A22 ) - tf . einsum ( 'ab,ab->b' , A12 , tf . matmul ( A11i , A12 )) return u2_mean , u2_var def interpolate_pair_batch ( A11i , locs1 , vals1diff , cats1 , locs2 , cats2 ): \"\"\" Inputs: locs1.shape = [N1, K] vals1diff.shape = [N1] cats1.shape = [N1] locs2.shape = [N2, 2, K] cats2.shape = [N2] Outputs: u2_mean.shape = [N2, 2] u2_var.shape = [N2, 2, 2] \"\"\" N1 = len ( locs1 ) # Number of measurements. N2 = len ( locs2 ) # Number of prediction pairs. # Permute datapoints if cats is given. perm = np . argsort ( cats2 ) locs2 , cats2 = locs2 [ perm ], cats2 [ perm ] # Warp locs2. locs2_shape = locs2 . shape locs2 = locs2 . reshape ([ - 1 , locs2_shape [ - 1 ]]) # Shape into matrix. locs2 = self . warp ( locs2 ) . run ({}) locs2 = tf . reshape ( locs2 , locs2_shape ) # Revert shape. _ , A12 = gp_covariance2 ( self . gp , tf . constant ( locs1 , dtype = tf . float32 ), tf . constant ( cats1 , dtype = tf . int32 ), tf . constant ( locs2 [:, 0 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 ), N1 ) _ , A13 = gp_covariance2 ( self . gp , tf . constant ( locs1 , dtype = tf . float32 ), tf . constant ( cats1 , dtype = tf . int32 ), tf . constant ( locs2 [:, 1 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 ), N1 ) m2 , A22 = gp_covariance ( self . gp , tf . constant ( locs2 [:, 0 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 )) m3 , A33 = gp_covariance ( self . gp , tf . constant ( locs2 [:, 1 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 )) _ , A23 = gp_covariance2 ( self . gp , tf . constant ( locs2 [:, 0 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 ), tf . constant ( locs2 [:, 1 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 ), N2 ) # Reassemble into more useful shapes. A12 = tf . stack ([ A12 , A13 ], axis =- 1 ) # [N1, N2, 2] m2 = tf . stack ([ m2 , m3 ], axis =- 1 ) # [N2, 2] A22 = tf . linalg . diag_part ( A22 ) A33 = tf . linalg . diag_part ( A33 ) A23 = tf . linalg . diag_part ( A23 ) A22 = tf . stack ([ tf . stack ([ A22 , A23 ], axis =- 1 ), tf . stack ([ A23 , A33 ], axis =- 1 )], axis =- 2 ) # [N2, 2, 2] # Restore order if things were permuted. revperm = np . argsort ( perm ) m2 = tf . gather ( m2 , revperm ) A12 = tf . gather ( A12 , revperm , axis = 1 ) A22 = tf . gather ( A22 , revperm ) u2_mean = m2 + tf . einsum ( 'abc,a->bc' , A12 , tf . einsum ( 'ab,b->a' , A11i , vals1diff )) u2_var = A22 - tf . einsum ( 'abc,abd->bcd' , A12 , tf . einsum ( 'ae,ebd->abd' , A11i , A12 )) return u2_mean , u2_var def interpolate ( locs1 , vals1 , cats1 , locs2 , cats2 , pair = False ): # Interpolate in batches. batch_size = locs1 . shape [ 0 ] // 2 for_gp = [] for start in np . arange ( 0 , len ( locs2 ), batch_size ): stop = start + batch_size subset = locs2 [ start : stop ], cats2 [ start : stop ] for_gp . append ( subset ) # Permute datapoints if cats is given. if cats1 is not None : perm = np . argsort ( cats1 ) locs1 , vals1 , cats1 = locs1 [ perm ], vals1 [ perm ], cats1 [ perm ] locs1 = self . warp ( locs1 ) . run ({}) m1 , A11 = gp_covariance ( self . gp , tf . constant ( locs1 , dtype = tf . float32 ), tf . constant ( cats1 , dtype = tf . int32 )) A11i = tf . linalg . inv ( A11 ) u2_mean_s = [] u2_var_s = [] f = interpolate_pair_batch if pair else interpolate_batch for locs_subset , cats_subset in for_gp : u2_mean , u2_var = f ( A11i , locs1 , vals1 - m1 , cats1 , locs_subset , cats_subset ) u2_mean = u2_mean . numpy () u2_var = u2_var . numpy () u2_mean_s . append ( u2_mean ) u2_var_s . append ( u2_var ) u2_mean = np . concatenate ( u2_mean_s ) u2_var = np . concatenate ( u2_var_s ) return u2_mean , u2_var if self . parameter_sample_size is None : m , v = interpolate ( self . locs , self . vals , self . cats , locs2 , cats2 , pair ) elif reduce == 'median' : raise NotImplementedError p = tf . nest . map_structure ( lambda x : np . quantile ( x , 0.5 , axis = 0 ) . astype ( np . float32 ), self . parameters ) m , v = interpolate ( self . locs , self . vals , self . cats , locs2 , cats2 , p , pair ) elif reduce == 'mean' : raise NotImplementedError p = tf . nest . map_structure ( lambda x : x . mean ( axis = 0 ) . astype ( np . float32 ), self . parameters ) m , v = interpolate ( self . locs , self . vals , self . cats , locs2 , cats2 , p , pair ) else : raise NotImplementedError samples = self . parameter_sample_size if subsample is not None : assert subsample <= samples , '`subsample` may not exceed sample size' else : subsample = samples # Thin by picking roughly equally-spaced samples. a = np . arange ( samples ) * subsample / samples % 1 pick = np . concatenate ([[ True ], a [ 1 :] >= a [: - 1 ]]) parameters = tf . nest . map_structure ( lambda x : x [ pick ], self . parameters ) # Make a prediction for each sample. results = [] for i in tracker ( range ( subsample )): p = tf . nest . map_structure ( lambda x : x [ i ], parameters ) results . append ( interpolate ( self . locs , self . vals , self . cats , locs2 , cats2 , p , pair )) mm , vv = [ np . stack ( x ) for x in zip ( * results )] m = mm . mean ( axis = 0 ) v = ( np . square ( mm ) + vv ) . mean ( axis = 0 ) - np . square ( m ) return m , v __post_init__ () Parameters: Name Type Description Default x Pandas DataFrame with columns for locations. required u A Pandas Series containing observations. required featurization function, optional Should be a function that takes x1 (n-dim array of input data) and returns the coordinates, i.e., x, y, x 2, y 2. Example: def featurization(x1): return x1[:, 0], x1[:, 1], x1[:, 0] 2, x1[:, 1] 2. Default is None. required latent List[GP] Name of the covariance function to use in the GP. Should be 'squared-exp' or 'gamma-exp'. Default is 'squared-exp'. required verbose boolean, optional Whether or not to print parameters. Default is True. required Performs Gaussian process training and prediction. Source code in src/geostat/model.py 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 def __post_init__ ( self ): ''' Parameters: x : Pandas DataFrame with columns for locations. u : A Pandas Series containing observations. featurization : function, optional Should be a function that takes x1 (n-dim array of input data) and returns the coordinates, i.e., x, y, x**2, y**2. Example: def featurization(x1): return x1[:, 0], x1[:, 1], x1[:, 0]**2, x1[:, 1]**2. Default is None. latent : List[GP] Name of the covariance function to use in the GP. Should be 'squared-exp' or 'gamma-exp'. Default is 'squared-exp'. verbose : boolean, optional Whether or not to print parameters. Default is True. Performs Gaussian process training and prediction. ''' if self . warp is None : self . warp = NoWarp () # Default reporting function. def default_report ( p , prefix = None ): if prefix : print ( prefix , end = ' ' ) def fmt ( x ): if isinstance ( x , tf . Tensor ): x = x . numpy () if isinstance ( x , ( int , np . int32 , np . int64 )): return ' {:5d} ' . format ( x ) if isinstance ( x , ( float , np . float32 , np . float64 )): return ' {:5.2f} ' . format ( x ) else : with np . printoptions ( precision = 2 , formatter = { 'floatkind' : ' {:5.2f} ' . format }): return str ( x ) print ( '[ %s ]' % ( ' ' . join ( ' %s %s ' % ( k , fmt ( v )) for k , v in p . items ()))) if self . report == None : self . report = default_report if self . locs is not None : self . locs = np . array ( self . locs ) if self . vals is not None : self . vals = np . array ( self . vals ) if self . cats is not None : self . cats = np . array ( self . cats ) # Collect parameters and create TF parameters. for p in self . gather_vars () . values (): p . create_tf_variable () predict ( locs2 , cats2 = None , * , subsample = None , reduce = None , tracker = None , pair = False ) Performs GP predictions of the mean and variance. Has support for batch predictions for large data sets. Source code in src/geostat/model.py 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 def predict ( self , locs2 , cats2 = None , * , subsample = None , reduce = None , tracker = None , pair = False ): ''' Performs GP predictions of the mean and variance. Has support for batch predictions for large data sets. ''' assert subsample is None or self . parameter_sample_size is not None , \\ '`subsample` is only valid with sampled parameters' assert reduce is None or self . parameter_sample_size is not None , \\ '`reduce` is only valid with sampled parameters' assert subsample is None or reduce is None , \\ '`subsample` and `reduce` cannot both be given' if tracker is None : tracker = lambda x : x assert self . locs . shape [ - 1 ] == locs2 . shape [ - 1 ], 'Mismatch in location dimensions' if cats2 is not None : assert cats2 . shape == locs2 . shape [: 1 ], 'Mismatched shapes in cats and locs' else : cats2 = np . zeros ( locs2 . shape [: 1 ], np . int32 ) def interpolate_batch ( A11i , locs1 , vals1diff , cats1 , locs2 , cats2 ): \"\"\" Inputs: locs1.shape = [N1, K] vals1diff.shape = [N1] cats1.shape = [N1] locs2.shape = [N2, K] cats2.shape = [N2] Outputs: u2_mean.shape = [N2] u2_var.shape = [N2] \"\"\" N1 = len ( locs1 ) # Number of measurements. # Permute datapoints if cats is given. if cats2 is not None : perm = np . argsort ( cats2 ) locs2 , cats2 = locs2 [ perm ], cats2 [ perm ] locs2 = self . warp ( locs2 ) . run ({}) _ , A12 = gp_covariance2 ( self . gp , tf . constant ( locs1 , dtype = tf . float32 ), tf . constant ( cats1 , dtype = tf . int32 ), tf . constant ( locs2 , dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 ), N1 ) m2 , A22 = gp_covariance ( self . gp , tf . constant ( locs2 , dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 )) # Restore order if things were permuted. if cats2 is not None : revperm = np . argsort ( perm ) m2 = tf . gather ( m2 , revperm ) A12 = tf . gather ( A12 , revperm , axis =- 1 ) A22 = tf . gather ( tf . gather ( A22 , revperm ), revperm , axis =- 1 ) u2_mean = m2 + tf . einsum ( 'ab,a->b' , A12 , tf . einsum ( 'ab,b->a' , A11i , vals1diff )) u2_var = tf . linalg . diag_part ( A22 ) - tf . einsum ( 'ab,ab->b' , A12 , tf . matmul ( A11i , A12 )) return u2_mean , u2_var def interpolate_pair_batch ( A11i , locs1 , vals1diff , cats1 , locs2 , cats2 ): \"\"\" Inputs: locs1.shape = [N1, K] vals1diff.shape = [N1] cats1.shape = [N1] locs2.shape = [N2, 2, K] cats2.shape = [N2] Outputs: u2_mean.shape = [N2, 2] u2_var.shape = [N2, 2, 2] \"\"\" N1 = len ( locs1 ) # Number of measurements. N2 = len ( locs2 ) # Number of prediction pairs. # Permute datapoints if cats is given. perm = np . argsort ( cats2 ) locs2 , cats2 = locs2 [ perm ], cats2 [ perm ] # Warp locs2. locs2_shape = locs2 . shape locs2 = locs2 . reshape ([ - 1 , locs2_shape [ - 1 ]]) # Shape into matrix. locs2 = self . warp ( locs2 ) . run ({}) locs2 = tf . reshape ( locs2 , locs2_shape ) # Revert shape. _ , A12 = gp_covariance2 ( self . gp , tf . constant ( locs1 , dtype = tf . float32 ), tf . constant ( cats1 , dtype = tf . int32 ), tf . constant ( locs2 [:, 0 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 ), N1 ) _ , A13 = gp_covariance2 ( self . gp , tf . constant ( locs1 , dtype = tf . float32 ), tf . constant ( cats1 , dtype = tf . int32 ), tf . constant ( locs2 [:, 1 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 ), N1 ) m2 , A22 = gp_covariance ( self . gp , tf . constant ( locs2 [:, 0 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 )) m3 , A33 = gp_covariance ( self . gp , tf . constant ( locs2 [:, 1 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 )) _ , A23 = gp_covariance2 ( self . gp , tf . constant ( locs2 [:, 0 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 ), tf . constant ( locs2 [:, 1 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 ), N2 ) # Reassemble into more useful shapes. A12 = tf . stack ([ A12 , A13 ], axis =- 1 ) # [N1, N2, 2] m2 = tf . stack ([ m2 , m3 ], axis =- 1 ) # [N2, 2] A22 = tf . linalg . diag_part ( A22 ) A33 = tf . linalg . diag_part ( A33 ) A23 = tf . linalg . diag_part ( A23 ) A22 = tf . stack ([ tf . stack ([ A22 , A23 ], axis =- 1 ), tf . stack ([ A23 , A33 ], axis =- 1 )], axis =- 2 ) # [N2, 2, 2] # Restore order if things were permuted. revperm = np . argsort ( perm ) m2 = tf . gather ( m2 , revperm ) A12 = tf . gather ( A12 , revperm , axis = 1 ) A22 = tf . gather ( A22 , revperm ) u2_mean = m2 + tf . einsum ( 'abc,a->bc' , A12 , tf . einsum ( 'ab,b->a' , A11i , vals1diff )) u2_var = A22 - tf . einsum ( 'abc,abd->bcd' , A12 , tf . einsum ( 'ae,ebd->abd' , A11i , A12 )) return u2_mean , u2_var def interpolate ( locs1 , vals1 , cats1 , locs2 , cats2 , pair = False ): # Interpolate in batches. batch_size = locs1 . shape [ 0 ] // 2 for_gp = [] for start in np . arange ( 0 , len ( locs2 ), batch_size ): stop = start + batch_size subset = locs2 [ start : stop ], cats2 [ start : stop ] for_gp . append ( subset ) # Permute datapoints if cats is given. if cats1 is not None : perm = np . argsort ( cats1 ) locs1 , vals1 , cats1 = locs1 [ perm ], vals1 [ perm ], cats1 [ perm ] locs1 = self . warp ( locs1 ) . run ({}) m1 , A11 = gp_covariance ( self . gp , tf . constant ( locs1 , dtype = tf . float32 ), tf . constant ( cats1 , dtype = tf . int32 )) A11i = tf . linalg . inv ( A11 ) u2_mean_s = [] u2_var_s = [] f = interpolate_pair_batch if pair else interpolate_batch for locs_subset , cats_subset in for_gp : u2_mean , u2_var = f ( A11i , locs1 , vals1 - m1 , cats1 , locs_subset , cats_subset ) u2_mean = u2_mean . numpy () u2_var = u2_var . numpy () u2_mean_s . append ( u2_mean ) u2_var_s . append ( u2_var ) u2_mean = np . concatenate ( u2_mean_s ) u2_var = np . concatenate ( u2_var_s ) return u2_mean , u2_var if self . parameter_sample_size is None : m , v = interpolate ( self . locs , self . vals , self . cats , locs2 , cats2 , pair ) elif reduce == 'median' : raise NotImplementedError p = tf . nest . map_structure ( lambda x : np . quantile ( x , 0.5 , axis = 0 ) . astype ( np . float32 ), self . parameters ) m , v = interpolate ( self . locs , self . vals , self . cats , locs2 , cats2 , p , pair ) elif reduce == 'mean' : raise NotImplementedError p = tf . nest . map_structure ( lambda x : x . mean ( axis = 0 ) . astype ( np . float32 ), self . parameters ) m , v = interpolate ( self . locs , self . vals , self . cats , locs2 , cats2 , p , pair ) else : raise NotImplementedError samples = self . parameter_sample_size if subsample is not None : assert subsample <= samples , '`subsample` may not exceed sample size' else : subsample = samples # Thin by picking roughly equally-spaced samples. a = np . arange ( samples ) * subsample / samples % 1 pick = np . concatenate ([[ True ], a [ 1 :] >= a [: - 1 ]]) parameters = tf . nest . map_structure ( lambda x : x [ pick ], self . parameters ) # Make a prediction for each sample. results = [] for i in tracker ( range ( subsample )): p = tf . nest . map_structure ( lambda x : x [ i ], parameters ) results . append ( interpolate ( self . locs , self . vals , self . cats , locs2 , cats2 , p , pair )) mm , vv = [ np . stack ( x ) for x in zip ( * results )] m = mm . mean ( axis = 0 ) v = ( np . square ( mm ) + vv ) . mean ( axis = 0 ) - np . square ( m ) return m , v NormalizingFeaturizer Produces featurized locations (F matrix) and remembers normalization parameters. Adds an intercept feature. Source code in src/geostat/model.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 class NormalizingFeaturizer : \"\"\" Produces featurized locations (F matrix) and remembers normalization parameters. Adds an intercept feature. \"\"\" def __init__ ( self , featurization , locs ): self . unnorm_featurizer = Featurizer ( featurization ) F_unnorm = self . unnorm_featurizer ( locs ) self . unnorm_mean = tf . reduce_mean ( F_unnorm , axis = 0 ) self . unnorm_std = tf . math . reduce_std ( F_unnorm , axis = 0 ) def __call__ ( self , locs ): ones = tf . ones ([ tf . shape ( locs )[ 0 ], 1 ], dtype = tf . float32 ) F_unnorm = self . unnorm_featurizer ( locs ) F_norm = ( F_unnorm - self . unnorm_mean ) / self . unnorm_std return tf . concat ([ ones , F_norm ], axis = 1 ) Warp Source code in src/geostat/model.py 55 56 57 58 59 60 61 62 63 64 65 class Warp : def __call__ ( self , locs , prep ): \"\"\" `locs` is numpy. Returns a WarpLocations. \"\"\" pass def gather_vars ( self ): pass __call__ ( locs , prep ) locs is numpy. Returns a WarpLocations. Source code in src/geostat/model.py 56 57 58 59 60 61 62 def __call__ ( self , locs , prep ): \"\"\" `locs` is numpy. Returns a WarpLocations. \"\"\" pass gp_covariance2 ( gp , locs1 , cats1 , locs2 , cats2 , offset ) offset is i2-i1, where i1 and i2 are the starting indices of locs1 and locs2. It is used to create the diagonal non-zero elements of a Noise covariance function. An non-zero offset results in a covariance matrix with non-zero entries along an off-center diagonal. Source code in src/geostat/model.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 @tf . function def gp_covariance2 ( gp , locs1 , cats1 , locs2 , cats2 , offset ): \"\"\" `offset` is i2-i1, where i1 and i2 are the starting indices of locs1 and locs2. It is used to create the diagonal non-zero elements of a Noise covariance function. An non-zero offset results in a covariance matrix with non-zero entries along an off-center diagonal. \"\"\" # assert np.all(cats1 == np.sort(cats1)), '`cats1` must be in non-descending order' # assert np.all(cats2 == np.sort(cats2)), '`cats2` must be in non-descending order' cache = {} cache [ 'offset' ] = offset cache [ 'locs1' ] = locs1 cache [ 'locs2' ] = locs2 cache [ 'cats1' ] = cats1 cache [ 'cats2' ] = cats2 cache [ 'per_axis_dist2' ] = PerAxisDist2 () . run ( cache ) cache [ 'euclidean' ] = Euclidean () . run ( cache ) M = gp . mean . run ( cache ) C = gp . kernel . run ( cache ) M = tf . cast ( M , tf . float64 ) C = tf . cast ( C , tf . float64 ) return M , C interpolate_1d_tf ( src , tgt , x ) src : (batch, breaks) tgt : (batch, breaks) x : (batch) Source code in src/geostat/model.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 @tf . function def interpolate_1d_tf ( src , tgt , x ): \"\"\" `src`: (batch, breaks) `tgt`: (batch, breaks) `x` : (batch) \"\"\" x_shape = tf . shape ( x ) x = tf . reshape ( x , [ - 1 , 1 ]) # (batch, 1) bucket = tf . searchsorted ( src , x ) bucket = tf . clip_by_value ( bucket - 1 , 0 , tf . shape ( tgt )[ 0 ] - 2 ) src0 = tf . gather ( src , bucket , batch_dims = 1 ) src1 = tf . gather ( src , bucket + 1 , batch_dims = 1 ) tgt0 = tf . gather ( tgt , bucket , batch_dims = 1 ) tgt1 = tf . gather ( tgt , bucket + 1 , batch_dims = 1 ) xout = (( x - src0 ) * tgt1 + ( src1 - x ) * tgt0 ) / ( src1 - src0 ) return tf . reshape ( xout , x_shape ) mvn_log_pdf ( u , m , cov ) Log PDF of a multivariate gaussian. Source code in src/geostat/model.py 309 310 311 312 313 314 def mvn_log_pdf ( u , m , cov ): \"\"\"Log PDF of a multivariate gaussian.\"\"\" u_adj = u - m logdet = tf . linalg . logdet ( 2 * np . pi * cov ) quad = tf . matmul ( e ( u_adj , 0 ), tf . linalg . solve ( cov , e ( u_adj , - 1 )))[ 0 , 0 ] return tf . cast ( - 0.5 * ( logdet + quad ), tf . float32 ) src.geostat.op Op dataclass The autoinputs parameter contains a blob of upstream ops. The leaves in the blob are either the op itself or a string identifier. In the latter case, the string identifier should be present as a key in the cache that gets passed in. This parameter links ops together in a DAG. We walk the DAG for various reasons. run calls the op and puts the output in self.out , after recursively doing this for autoinputs. gather_vars recursively gathers variables from self and autoinputs. Source code in src/geostat/op.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @dataclass class Op : \"\"\" The `autoinputs` parameter contains a blob of upstream ops. The leaves in the blob are either the op itself or a string identifier. In the latter case, the string identifier should be present as a key in the `cache` that gets passed in. This parameter links ops together in a DAG. We walk the DAG for various reasons. - `run` calls the op and puts the output in `self.out`, after recursively doing this for autoinputs. - `gather_vars` recursively gathers variables from self and autoinputs. \"\"\" fa : Dict [ str , object ] # Formal arguments. autoinputs : object # Blob of Ops and strings. def vars ( self ): # Parameters return [] def gather_vars ( self , cache = None ): \"\"\" `cache` maps from Op ids to sets of variable names. Returns a dict of parameters, keyed by name. \"\"\" if cache is None : cache = {} if id ( self ) not in cache : vv = { k : v for op in tf . nest . flatten ( self . autoinputs ) if isinstance ( op , Op ) for k , v in op . gather_vars ( cache ) . items ()} # print(self, '<-', [x.name for x in vv], '|', [x.name for x in set(self.vars())], '\\n') cache [ id ( self )] = vv | self . vars () return cache [ id ( self )] def __call__ ( self , e ): \"\"\" `e` is a dict of params and evaluated inputs from upstream ops. Other values in `e` are supplied by the caller. \"\"\" pass def run ( self , cache ): \"\"\" If op has already been run, return result. Else: - Assemble inputs by recursively calling upstream ops. - Execute op by calling `__call__`. - Store result in cache. \"\"\" def eval ( op ): \"\"\" Evaluate `op`. If `op` is a string, look up its value. Otherwise execute it. \"\"\" if isinstance ( op , str ): return cache [ op ] else : return op . run ( cache ) if id ( self ) not in cache : e = tf . nest . map_structure ( lambda op : eval ( op ), self . autoinputs ) e |= get_parameter_values ( self . fa ) cache [ id ( self )] = self ( e ) # Save the Op so that its ID remains unique. if '__save__' not in cache : cache [ '__save__' ] = [] cache [ '__save__' ] . append ( self ) return cache [ id ( self )] def __tf_tracing_type__ ( self , context ): return SingletonTraceType ( self ) __call__ ( e ) e is a dict of params and evaluated inputs from upstream ops. Other values in e are supplied by the caller. Source code in src/geostat/op.py 50 51 52 53 54 55 def __call__ ( self , e ): \"\"\" `e` is a dict of params and evaluated inputs from upstream ops. Other values in `e` are supplied by the caller. \"\"\" pass gather_vars ( cache = None ) cache maps from Op ids to sets of variable names. Returns a dict of parameters, keyed by name. Source code in src/geostat/op.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def gather_vars ( self , cache = None ): \"\"\" `cache` maps from Op ids to sets of variable names. Returns a dict of parameters, keyed by name. \"\"\" if cache is None : cache = {} if id ( self ) not in cache : vv = { k : v for op in tf . nest . flatten ( self . autoinputs ) if isinstance ( op , Op ) for k , v in op . gather_vars ( cache ) . items ()} # print(self, '<-', [x.name for x in vv], '|', [x.name for x in set(self.vars())], '\\n') cache [ id ( self )] = vv | self . vars () return cache [ id ( self )] run ( cache ) If op has already been run, return result. Else: - Assemble inputs by recursively calling upstream ops. - Execute op by calling __call__ . - Store result in cache. Source code in src/geostat/op.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def run ( self , cache ): \"\"\" If op has already been run, return result. Else: - Assemble inputs by recursively calling upstream ops. - Execute op by calling `__call__`. - Store result in cache. \"\"\" def eval ( op ): \"\"\" Evaluate `op`. If `op` is a string, look up its value. Otherwise execute it. \"\"\" if isinstance ( op , str ): return cache [ op ] else : return op . run ( cache ) if id ( self ) not in cache : e = tf . nest . map_structure ( lambda op : eval ( op ), self . autoinputs ) e |= get_parameter_values ( self . fa ) cache [ id ( self )] = self ( e ) # Save the Op so that its ID remains unique. if '__save__' not in cache : cache [ '__save__' ] = [] cache [ '__save__' ] . append ( self ) return cache [ id ( self )] SingletonTraceType Bases: TraceType A trace type to override TF's default behavior, which is to treat dataclass-based onjects as dicts. Source code in src/geostat/op.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 class SingletonTraceType ( TraceType ): \"\"\" A trace type to override TF's default behavior, which is to treat dataclass-based onjects as dicts. \"\"\" def __init__ ( self , thing ): self . value = thing def is_subtype_of ( self , other ): return self . value is other . value def most_specific_common_supertype ( self , other ): if self . value is other . value : return self . value else : return None def placeholder_value ( self , placeholder_context ): return self . value def __eq__ ( self , other ): return self . value is other . value def __hash__ ( self ): return hash ( id ( self . value )) src.geostat.param Parameter dataclass Source code in src/geostat/param.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 @dataclass class Parameter : name : str value : float lo : float = np . nan hi : float = np . nan underlying : tf . Variable = None def update_bounds ( self , lo : float , hi : float ): if np . isnan ( self . lo ): self . lo = lo else : assert self . lo == lo , f 'Conflicting bounds for parameter { self . name } ' if np . isnan ( self . hi ): self . hi = hi else : assert self . hi == hi , f 'Conflicting bounds for parameter { self . name } ' def bounding ( self ): return \\ ( 'u' if self . lo == float ( '-inf' ) else 'b' ) + \\ ( 'u' if self . hi == float ( 'inf' ) else 'b' ) def create_tf_variable ( self ): \"\"\"Create TF variable for underlying parameter or update it\"\"\" # Create underlying parameter. b = self . bounding () if b == 'bb' : init = logit (( self . value - self . lo ) / ( self . hi - self . lo )) elif b == 'bu' : init = np . log ( self . value - self . lo ) elif b == 'ub' : init = - np . log ( self . hi - self . value ) else : init = self . value if self . underlying is None : self . underlying = tf . Variable ( init , name = self . name , dtype = tf . float32 ) else : self . underlying . assign ( init ) def surface ( self ): \"\"\" Create tensor for surface parameter\"\"\" # Create surface parameter. b = self . bounding () v = self . underlying if b == 'bb' : v = tf . math . sigmoid ( v ) * ( self . hi - self . lo ) + self . lo elif b == 'bu' : v = tf . exp ( v ) + self . lo elif b == 'ub' : v = self . hi - tf . exp ( - v ) else : v = v + tf . constant ( 0. ) return v def update_value ( self ): self . value = self . surface () . numpy () create_tf_variable () Create TF variable for underlying parameter or update it Source code in src/geostat/param.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def create_tf_variable ( self ): \"\"\"Create TF variable for underlying parameter or update it\"\"\" # Create underlying parameter. b = self . bounding () if b == 'bb' : init = logit (( self . value - self . lo ) / ( self . hi - self . lo )) elif b == 'bu' : init = np . log ( self . value - self . lo ) elif b == 'ub' : init = - np . log ( self . hi - self . value ) else : init = self . value if self . underlying is None : self . underlying = tf . Variable ( init , name = self . name , dtype = tf . float32 ) else : self . underlying . assign ( init ) surface () Create tensor for surface parameter Source code in src/geostat/param.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def surface ( self ): \"\"\" Create tensor for surface parameter\"\"\" # Create surface parameter. b = self . bounding () v = self . underlying if b == 'bb' : v = tf . math . sigmoid ( v ) * ( self . hi - self . lo ) + self . lo elif b == 'bu' : v = tf . exp ( v ) + self . lo elif b == 'ub' : v = self . hi - tf . exp ( - v ) else : v = v + tf . constant ( 0. ) return v bpp ( param , lo , hi ) Bounded paper parameter (maybe). Source code in src/geostat/param.py 107 108 109 110 111 112 113 def bpp ( param , lo , hi ): \"\"\"Bounded paper parameter (maybe).\"\"\" if isinstance ( param , Parameter ): param . update_bounds ( lo , hi ) return { param . name : param } else : return {} get_parameter_values ( blob ) For each Parameter encountered in the nested blob, replace it with its surface tensor. Source code in src/geostat/param.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def get_parameter_values ( blob : object ): \"\"\" For each Parameter encountered in the nested blob, replace it with its surface tensor. \"\"\" if isinstance ( blob , dict ): return { k : get_parameter_values ( a ) for k , a in blob . items ()} elif isinstance ( blob , ( list , tuple )): return [ get_parameter_values ( a ) for a in blob ] elif isinstance ( blob , Parameter ): return blob . surface () elif isinstance ( blob , str ): raise ValueError ( f 'Bad parameter { blob } is a string' ) else : return blob ppp ( param ) Positive paper parameter (maybe). Source code in src/geostat/param.py 91 92 93 94 95 96 97 def ppp ( param ): \"\"\"Positive paper parameter (maybe).\"\"\" if isinstance ( param , Parameter ): param . update_bounds ( 0. , np . inf ) return { param . name : param } else : return {} upp ( param ) Unbounded paper parameter (maybe). Source code in src/geostat/param.py 99 100 101 102 103 104 105 def upp ( param ): \"\"\"Unbounded paper parameter (maybe).\"\"\" if isinstance ( param , Parameter ): param . update_bounds ( - np . inf , np . inf ) return { param . name : param } else : return {}","title":"API Reference"},{"location":"api/#api-reference","text":"","title":"API Reference"},{"location":"api/#src.geostat.custom_op","text":"","title":"custom_op"},{"location":"api/#src.geostat.custom_op.function","text":"A custom op involves two functions f, where it is defined, and g, where it is introduced into the graph. Source code in src/geostat/custom_op.py 19 20 21 22 23 24 25 26 27 28 def function ( f ): \"\"\" A custom op involves two functions: * f, where it is defined, and * g, where it is introduced into the graph. \"\"\" def g ( * args ): # Assume for now that all arguments are Parameters return CustomOp ( f , ** { p . name : p for p in args }) return g","title":"function"},{"location":"api/#src.geostat.kernel","text":"","title":"kernel"},{"location":"api/#src.geostat.kernel.Kernel","text":"Bases: Op Source code in src/geostat/kernel.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class Kernel ( Op ): def __init__ ( self , fa , autoinputs ): if 'offset' not in autoinputs : autoinputs [ 'offset' ] = 'offset' if 'locs1' not in autoinputs : autoinputs [ 'locs1' ] = 'locs1' if 'locs2' not in autoinputs : autoinputs [ 'locs2' ] = 'locs2' super () . __init__ ( fa , autoinputs ) def __add__ ( self , other ): if other is None : return self else : return Stack ([ self ]) + other def __mul__ ( self , other ): return Product ([ self ]) * other def call ( self , e ): \"\"\" Returns tuple `(mean, covariance)` for locations. Return values may be unbroadcasted. \"\"\" pass def __call__ ( self , e ): \"\"\" Returns tuple `(mean, covariance)` for locations. Return values have correct shapes. \"\"\" C = self . call ( e ) if C is None : C = 0. n1 = tf . shape ( e [ 'locs1' ])[ 0 ] n2 = tf . shape ( e [ 'locs2' ])[ 0 ] C = tf . broadcast_to ( C , [ n1 , n2 ]) return C def report ( self ): string = ', ' . join ( ' %s %4.2f ' % ( v . name , p [ v . name ]) for v in self . vars ()) return '[' + string + ']'","title":"Kernel"},{"location":"api/#src.geostat.kernel.Kernel.__call__","text":"Returns tuple (mean, covariance) for locations. Return values have correct shapes. Source code in src/geostat/kernel.py 47 48 49 50 51 52 53 54 55 56 57 def __call__ ( self , e ): \"\"\" Returns tuple `(mean, covariance)` for locations. Return values have correct shapes. \"\"\" C = self . call ( e ) if C is None : C = 0. n1 = tf . shape ( e [ 'locs1' ])[ 0 ] n2 = tf . shape ( e [ 'locs2' ])[ 0 ] C = tf . broadcast_to ( C , [ n1 , n2 ]) return C","title":"__call__"},{"location":"api/#src.geostat.kernel.Kernel.call","text":"Returns tuple (mean, covariance) for locations. Return values may be unbroadcasted. Source code in src/geostat/kernel.py 40 41 42 43 44 45 def call ( self , e ): \"\"\" Returns tuple `(mean, covariance)` for locations. Return values may be unbroadcasted. \"\"\" pass","title":"call"},{"location":"api/#src.geostat.kernel.Mix","text":"Bases: Kernel Source code in src/geostat/kernel.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 class Mix ( Kernel ): def __init__ ( self , inputs , weights = None ): self . inputs = inputs fa = {} ai = dict ( cats1 = 'cats1' , cats2 = 'cats2' ) # Special case if weights is not given. if weights is not None : fa [ 'weights' ] = weights ai [ 'inputs' ] = inputs super () . __init__ ( fa , ai ) def gather_vars ( self , cache = None ): \"\"\"Make a special version of gather_vars because we want to gather variables from `inputs` even when it's not in autoinputs\"\"\" vv = super () . gather_vars ( cache ) for iput in self . inputs : cache [ id ( self )] |= iput . gather_vars ( cache ) return cache [ id ( self )] def vars ( self ): if 'weights' in self . fa : return { k : p for row in self . fa [ 'weights' ] for k , p in get_trend_coefs ( row ) . items ()} else : return {} def call ( self , e ): if 'weights' in e : weights = [] for row in e [ 'weights' ]: if isinstance ( row , ( tuple , list )): row = tf . stack ( row ) weights . append ( row ) weights = tf . stack ( weights ) C = tf . stack ( e [ 'inputs' ], axis =- 1 ) # [locs, locs, numinputs]. Aaug1 = tf . gather ( weights , e [ 'cats1' ]) # [locs, numinputs]. Aaug2 = tf . gather ( weights , e [ 'cats2' ]) # [locs, numinputs]. outer = tf . einsum ( 'ac,bc->abc' , Aaug1 , Aaug2 ) # [locs, locs, numinputs]. C = tf . einsum ( 'abc,abc->ab' , C , outer ) # [locs, locs]. return C else : # When weights is not given, exploit the fact that we don't have # to compute every element in component covariance matrices. N = len ( self . inputs ) catcounts1 = tf . math . bincount ( e [ 'cats1' ], minlength = N , maxlength = N ) catcounts2 = tf . math . bincount ( e [ 'cats2' ], minlength = N , maxlength = N ) catindices1 = tf . math . cumsum ( catcounts1 , exclusive = True ) catindices2 = tf . math . cumsum ( catcounts2 , exclusive = True ) catdiffs = tf . unstack ( catindices2 - catindices1 , num = N ) locsegs1 = tf . split ( e [ 'locs1' ], catcounts1 , num = N ) locsegs2 = tf . split ( e [ 'locs2' ], catcounts2 , num = N ) # TODO: Check that the below is still correct. CC = [] # Observation noise submatrices. for sublocs1 , sublocs2 , catdiff , iput in zip ( locsegs1 , locsegs2 , catdiffs , self . inputs ): cache = dict ( offset = e [ 'offset' ] + catdiff , locs1 = sublocs1 , locs2 = sublocs2 ) cache [ 'per_axis_dist2' ] = PerAxisDist2 () . run ( cache ) cache [ 'euclidean' ] = Euclidean () . run ( cache ) Csub = iput . run ( cache ) CC . append ( Csub ) return block_diag ( CC )","title":"Mix"},{"location":"api/#src.geostat.kernel.Mix.gather_vars","text":"Make a special version of gather_vars because we want to gather variables from inputs even when it's not in autoinputs Source code in src/geostat/kernel.py 448 449 450 451 452 453 454 455 def gather_vars ( self , cache = None ): \"\"\"Make a special version of gather_vars because we want to gather variables from `inputs` even when it's not in autoinputs\"\"\" vv = super () . gather_vars ( cache ) for iput in self . inputs : cache [ id ( self )] |= iput . gather_vars ( cache ) return cache [ id ( self )]","title":"gather_vars"},{"location":"api/#src.geostat.kernel.Observation","text":"Bases: Op Source code in src/geostat/kernel.py 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 class Observation ( Op ): def __init__ ( self , coefs : List , noise : Kernel ): self . coefs = coefs self . noise = noise super () . __init__ ({}, self . noise ) def vars ( self ): vv = { k : p for c in self . coefs for k , p in upp ( c )} vv |= self . noise . vars () return vv def __call__ ( self , e ): \"\"\" Dummy. \"\"\" return 0.","title":"Observation"},{"location":"api/#src.geostat.kernel.Observation.__call__","text":"Dummy. Source code in src/geostat/kernel.py 579 580 581 582 583 def __call__ ( self , e ): \"\"\" Dummy. \"\"\" return 0.","title":"__call__"},{"location":"api/#src.geostat.kernel.block_diag","text":"Return a dense block-diagonal matrix. Source code in src/geostat/kernel.py 20 21 22 def block_diag ( blocks ): \"\"\"Return a dense block-diagonal matrix.\"\"\" return LOBlockDiag ([ LOFullMatrix ( b ) for b in blocks ]) . to_dense ()","title":"block_diag"},{"location":"api/#src.geostat.kernel.quadstack","text":"x has arbitrary shape [...], but must be non-negative. sills and ranges both have shape [K]. Source code in src/geostat/kernel.py 293 294 295 296 297 298 299 300 301 302 @tf . recompute_grad def quadstack ( x , sills , ranges ): \"\"\" `x` has arbitrary shape [...], but must be non-negative. `sills` and `ranges` both have shape [K]. \"\"\" ex = ed ( x ) ax = tf . maximum ( 0. , 1. - tf . abs ( ex ) / ranges ) # [..., 1] y = sills * tf . square ( ax ) # [..., K] return tf . reduce_sum ( y , - 1 )","title":"quadstack"},{"location":"api/#src.geostat.kernel.rampstack","text":"x has arbitrary shape [...], but must be non-negative. sills and ranges both have shape [K]. Source code in src/geostat/kernel.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 @tf . custom_gradient def rampstack ( x , sills , ranges ): \"\"\" `x` has arbitrary shape [...], but must be non-negative. `sills` and `ranges` both have shape [K]. \"\"\" ax = ed ( tf . abs ( x )) # [..., 1] y = sills * tf . maximum ( 0. , 1. - ax / ranges ) # [..., K] def grad ( upstream ): ax = ed ( tf . abs ( x )) # [..., 1] y = sills * tf . maximum ( 0. , 1. - ax / ranges ) # [..., K] K = tf . shape ( sills )[ 0 ] small = ax < ranges grad_x = upstream * tf . reduce_sum ( tf . where ( small , - tf . sign ( ed ( x )) * ( sills / ranges ), 0. ), - 1 ) # [...] grad_sills = tf . einsum ( 'ak,a->k' , tf . reshape ( y , [ - 1 , K ]), tf . reshape ( upstream , [ - 1 ])) grad_ranges = tf . where ( small , ax * ( sills / tf . square ( ranges )), 0. ) # [..., K} grad_ranges = tf . einsum ( 'ak,a->k' , tf . reshape ( grad_ranges , [ - 1 , K ]), tf . reshape ( upstream , [ - 1 ])) return grad_x , grad_sills , grad_ranges return tf . reduce_sum ( y , - 1 ), grad","title":"rampstack"},{"location":"api/#src.geostat.kernel.smooth_convex","text":"x has arbitrary shape [...], but must be non-negative. sills and ranges both have shape [K]. Source code in src/geostat/kernel.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 @tf . recompute_grad def smooth_convex ( x , sills , ranges ): \"\"\" `x` has arbitrary shape [...], but must be non-negative. `sills` and `ranges` both have shape [K]. \"\"\" r2 = ranges r1 = tf . pad ( ranges [: - 1 ], [[ 1 , 0 ]]) ex = ed ( x ) ax = tf . abs ( ex ) rx = ax / r2 - 1. c1 = 2. / ( r1 + r2 ) c2 = 1. / ( 1. - tf . square ( r1 / r2 )) # i1 = tf.cast(ax <= r1, tf.float32) # Indicates x <= r1. # i2 = tf.cast(ax <= r2, tf.float32) * (1. - i1) # Indicates r1 < x <= r2. # v = i1 * (1. - c1 * ax) + i2 * c2 * tf.square(rx) v = tf . where ( ax <= r1 , 1. - c1 * ax , c2 * tf . square ( rx )) v = tf . where ( ax <= r2 , v , 0. ) y = tf . einsum ( '...k,k->...' , v , sills ) return y","title":"smooth_convex"},{"location":"api/#src.geostat.kernel.smooth_convex_grad","text":"x has arbitrary shape [...], but must be non-negative. sills and ranges both have shape [K]. Source code in src/geostat/kernel.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 @tf . custom_gradient def smooth_convex_grad ( x , sills , ranges ): \"\"\" `x` has arbitrary shape [...], but must be non-negative. `sills` and `ranges` both have shape [K]. \"\"\" r2 = ranges r1 = tf . pad ( ranges [: - 1 ], [[ 1 , 0 ]]) ex = ed ( x ) ax = tf . abs ( ex ) rx = ax / r2 - 1. c1 = 2. / ( r1 + r2 ) c2 = 1. / ( 1. - tf . square ( r1 / r2 )) v = tf . where ( ax <= r1 , 1. - c1 * ax , c2 * tf . square ( rx )) v = tf . where ( ax <= r2 , v , 0. ) y = tf . einsum ( '...k,k->...' , v , sills ) def grad ( upstream ): r2 = ranges r1 = tf . pad ( ranges [: - 1 ], [[ 1 , 0 ]]) ex = ed ( x ) ax = tf . abs ( ex ) rx = ax / r2 - 1. i1 = tf . cast ( ax <= r1 , tf . float32 ) # Indicates x <= r1. i2 = tf . cast ( ax <= r2 , tf . float32 ) * ( 1. - i1 ) # Indicates r1 < x <= r2. c1 = 2. / ( r1 + r2 ) c2 = 1. / ( 1. - tf . square ( r1 / r2 )) c3 = 1. / ( r2 - tf . square ( r1 ) / r2 ) v = i1 * ( 1. - c1 * ax ) + i2 * c2 * tf . square ( rx ) sx = tf . sign ( ex ) K = tf . shape ( sills )[ 0 ] gx = sx * sills * ( i1 * - c1 + i2 * rx * ( 2 * c3 )) grad_x = upstream * tf . reduce_sum ( gx , - 1 ) # [...] grad_sills = tf . einsum ( 'ak,a->k' , tf . reshape ( v , [ - 1 , K ]), tf . reshape ( upstream , [ - 1 ])) u = 2 / tf . square ( r1 + r2 ) * ax * i1 yr1 = u + i2 * tf . square ( rx * c3 ) * 2 * r1 yr2 = u - 2 * i2 * ( rx * c3 + tf . square ( rx * c2 ) / r2 ) yr1 = sills * tf . reshape ( yr1 , [ - 1 , K ]) yr2 = sills * tf . reshape ( yr2 , [ - 1 , K ]) yr = tf . pad ( yr1 [:, 1 :], [[ 0 , 0 ], [ 0 , 1 ]]) + yr2 grad_ranges = tf . einsum ( 'ak,a->k' , yr , tf . reshape ( upstream , [ - 1 ])) return grad_x , grad_sills , grad_ranges return y , grad","title":"smooth_convex_grad"},{"location":"api/#src.geostat.krige","text":"","title":"krige"},{"location":"api/#src.geostat.krige.Krige","text":"Source code in src/geostat/krige.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 class Krige (): def __init__ ( self , x1 , u1 , bins , variogram_func = None , variogram_params = None , cutoff_dist = 'auto' , featurization = None , projection = None , show_plots = True , verbose = True , ): ''' Parameters: x1 : n-dim array Locations of input data. u1 : 1-d array Values to be kriged. bins : int or None The number of bins to use on the variogram cloud. If None, variogram function is fit to the variogram cloud and is not binned first. variogram_func : str Name of the variogram model to use in the kriging. Should be 'linear', 'gaussian', or 'spherical'. Default is 'gaussian'. cutoff_dist : str or int, optional The maximum lag distance to include in variogram modeling. featurization : function, optional Should be a function that takes x1 (n-dim array of input data) and returns the coordinates, i.e., x, y, x**2, y**2. Example below. Default is None. project : function, opt A function that takes multiple vectors, and returns a tuple of projected vectors. epsg_proj : str The projected coordinate system to use. Ignored if project=False. Default is 'EPSG:3310' (California Albers). show_plots : boolean, optional Whether or not to show variogram plots. Default is True. verbose : boolean, optional Whether or not to print parameters. Default is True. Performs experimental variogram calculation, bins data, and fits variogram model to estimate variogram parameters. Performs ordinary and universal kriging in up to 3 spatial dimensions. Trend model example: def featurization(x1): return x1[:, 0], x1[:, 1], x1[:, 0]**2, x1[:, 1]**2 ''' super () . __init__ ( projection = projection ) # Save original x1 for stats function. self . x1_original = x1 # Check and change the shape of u1 if needed (for pdist). if u1 . ndim == 1 : self . u1 = u1 elif u1 . shape [ 1 ] == 1 : self . u1 = u1 . reshape ( - 1 ) else : raise ValueError ( \"Check dimensions of 'u1'.\" ) # Projection. self . x1 = self . project ( x1 ) # Variogram. if variogram_func == 'gaussian' : self . variogram_func = gaussian elif variogram_func == 'spherical' : self . variogram_func = spherical elif variogram_func == 'linear' : self . variogram_func = linear else : raise ValueError ( \"Variogram function must be 'linear', 'gaussian', or 'spherical'.\" ) if cutoff_dist == 'auto' : self . cutoff_dist = cutoff_dist_func ( self . x1 ) else : self . cutoff_dist = cutoff_dist self . verbose = verbose self . show_plots = show_plots self . bins = bins self . featurization = featurization # Lags and semivariance. dist = pdist ( self . x1 , metric = 'euclidean' ) gamma = 0.5 * pdist ( self . u1 . reshape ( - 1 , 1 ), metric = 'sqeuclidean' ) if self . bins != None : # Use bins. # Variogram cloud calculation. bin_means , bin_edges , binnumber = binned_statistic ( dist , gamma , statistic = 'mean' , bins = self . bins , range = [ dist . min (), self . cutoff_dist ]) bin_width = ( bin_edges [ 1 ] - bin_edges [ 0 ]) bin_centers = bin_edges [ 1 :] - bin_width / 2 # Bin counts calculation. bin_count , bin_count_edges , bin_count_number = binned_statistic ( dist , gamma , statistic = 'count' , bins = self . bins ) bin_count_width = ( bin_count_edges [ 1 ] - bin_count_edges [ 0 ]) bin_count_centers = bin_count_edges [ 1 :] - bin_count_width / 2 if self . show_plots == True : if self . bins == None : # Variogram cloud plot. plt . figure ( dpi = 100 ) plt . scatter ( dist , gamma , ec = 'C0' , fc = 'none' , alpha = 0.3 ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () if self . bins != None : # Variogram cloud plot. plt . figure ( dpi = 100 ) plt . scatter ( dist , gamma , ec = 'C0' , fc = 'none' , alpha = 0.3 ) plt . hlines ( bin_means , bin_edges [: - 1 ], bin_edges [ 1 :], zorder = 1 , color = 'k' ) plt . scatter ( bin_centers , bin_means , ec = 'k' , lw = 0.5 ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () # Bin counts plot. plt . figure ( dpi = 100 ) plt . hlines ( bin_count , bin_count_edges [: - 1 ], bin_count_edges [ 1 :], zorder = 1 , color = 'k' ) plt . scatter ( bin_count_centers , bin_count , ec = 'k' , lw = 0.5 ) plt . ylabel ( 'Bin count' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () ############ Fit the variogram model. if not variogram_params : # Fit to the data. if self . bins == None : # Fit the variogram cloud. if self . variogram_func == gaussian or self . variogram_func == spherical : # Initial guess for the parameters. p0 = [ 0.25 * np . max ( dist ), # range np . max ( gamma ) - np . min ( gamma ), # sill np . min ( gamma ) + 1e-6 ] # nugget # Bounds with constraints. bounds = [( 1e-6 , 1e-6 , 1e-6 ), ( np . inf , np . inf , np . inf )] # Apply the cutoff. dist_cut = np . where ( dist < self . cutoff_dist , dist , np . nan ) gamma_cut = np . where ( dist < self . cutoff_dist , gamma , np . nan ) # Remove nans for curve_fit. dist_cut = dist_cut [ ~ np . isnan ( dist_cut )] gamma_cut = gamma_cut [ ~ np . isnan ( gamma_cut )] popt , pcov = curve_fit ( self . variogram_func , dist_cut , gamma_cut , p0 = p0 , bounds = bounds ) if self . show_plots == True : # Calculate 2d kde to help confirm cutoff. xi = np . linspace ( np . min ( dist ), np . max ( dist ), 60 ) yi = np . linspace ( np . min ( gamma ), np . max ( gamma ), 60 ) xi , yi = np . meshgrid ( xi , yi ) xyi = np . stack ([ xi . reshape ( - 1 ), yi . reshape ( - 1 )], axis = 1 ) . T kde = stats . gaussian_kde ([ dist , gamma ]) z = kde . evaluate ( xyi ) z = z . reshape ( len ( xi ), len ( yi )) plt . figure ( dpi = 100 ) plt . pcolormesh ( xi , yi , z , cmap = plt . cm . Blues , shading = 'auto' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () # With cutoff and variogram model. xnew = np . linspace ( np . min ( dist_cut ), self . cutoff_dist , 100 ) plt . figure ( dpi = 100 ) plt . scatter ( dist_cut , gamma_cut , fc = 'none' , ec = 'C1' , lw = 0.5 , alpha = 0.3 ) plt . plot ( xnew , self . variogram_func ( xnew , * popt ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () vrange = popt [ 0 ] sill = popt [ 1 ] nugget = popt [ 2 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'range: {:.5f} ' . format ( vrange )) print ( 'sill: {:.5f} ' . format ( sill )) print ( 'nugget: {:.5f} ' . format ( nugget )) print ( 'full sill: {:.5f} ' . format ( sill + nugget )) self . parameter_vals = [ vrange , sill , nugget ] elif self . variogram_func == linear : # Initial guess for the parameters. p0 = [( np . max ( dist ) - np . min ( dist )) / ( np . max ( gamma ) - np . min ( gamma )), # slope np . min ( gamma ) + 1e-6 ] # nugget # Bounds with constraints. bounds = [( 1e-6 , 1e-6 ), ( np . inf , np . inf )] # Apply the cutoff. dist_cut = np . where ( dist < self . cutoff_dist , dist , np . nan ) gamma_cut = np . where ( dist < self . cutoff_dist , gamma , np . nan ) # Remove nans for curve_fit. dist_cut = dist_cut [ ~ np . isnan ( dist_cut )] gamma_cut = gamma_cut [ ~ np . isnan ( gamma_cut )] popt , pcov = curve_fit ( self . variogram_func , dist_cut , gamma_cut , p0 = p0 , bounds = bounds ) if self . show_plots == True : # Calculate 2d kde to help confirm cutoff. xi = np . linspace ( np . min ( dist ), np . max ( dist ), 60 ) yi = np . linspace ( np . min ( gamma ), np . max ( gamma ), 60 ) xi , yi = np . meshgrid ( xi , yi ) xyi = np . stack ([ xi . reshape ( - 1 ), yi . reshape ( - 1 )], axis = 1 ) . T kde = stats . gaussian_kde ([ dist , gamma ]) z = kde . evaluate ( xyi ) z = z . reshape ( len ( xi ), len ( yi )) plt . figure ( dpi = 100 ) plt . pcolormesh ( xi , yi , z , cmap = plt . cm . Blues , shading = 'auto' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () # With cutoff and variogram model. xnew = np . linspace ( np . min ( dist_cut ), self . cutoff_dist , 100 ) plt . figure ( dpi = 100 ) plt . scatter ( dist_cut , gamma_cut , fc = 'none' , ec = 'C1' , lw = 0.5 , alpha = 0.3 ) plt . plot ( xnew , self . variogram_func ( xnew , * popt ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () slope = popt [ 0 ] nugget = popt [ 1 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'slope: {:.5f} ' . format ( slope )) print ( 'nugget: {:.5f} ' . format ( nugget )) self . parameter_vals = [ slope , nugget ] elif self . bins != None : # Fit the binned data. if self . variogram_func == gaussian or self . variogram_func == spherical : # Initial guess for the parameters. p0 = [ 0.25 * np . max ( bin_centers ), # range np . max ( bin_means ) - np . min ( bin_means ), # sill np . min ( bin_means )] # nugget # Bounds with constraints. bounds = [( 1e-6 , 1e-6 , 1e-6 ), ( np . inf , np . inf , np . inf )] popt , pcov = curve_fit ( self . variogram_func , bin_centers , bin_means , p0 = p0 , bounds = bounds ) if self . show_plots == True : # Fit variogram plot. plt . figure ( dpi = 100 ) plt . scatter ( bin_centers , bin_means , c = 'C1' , ec = 'k' , lw = 0.5 ) plt . plot ( bin_centers , self . variogram_func ( bin_centers , * popt ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () vrange = popt [ 0 ] sill = popt [ 1 ] nugget = popt [ 2 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'range: {:.5f} ' . format ( vrange )) print ( 'sill: {:.5f} ' . format ( sill )) print ( 'nugget: {:.5f} ' . format ( nugget )) print ( 'full sill: {:.5f} ' . format ( sill + nugget )) self . parameter_vals = [ vrange , sill , nugget ] elif self . variogram_func == linear : # Initial guess for the parameters. p0 = [( np . max ( bin_centers ) - np . min ( bin_centers )) / ( np . max ( bin_means ) - np . min ( bin_means )), # slope np . min ( bin_means )] # nugget # Bounds with constraints. bounds = [( 1e-6 , 1e-6 ), ( np . inf , np . inf )] popt , pcov = curve_fit ( self . variogram_func , bin_centers , bin_means , p0 = p0 , bounds = bounds ) if self . show_plots == True : # Fit variogram plot. plt . figure ( dpi = 100 ) plt . scatter ( bin_centers , bin_means , c = 'C1' , ec = 'k' , lw = 0.5 ) plt . plot ( bin_centers , self . variogram_func ( bin_centers , * popt ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () slope = popt [ 0 ] nugget = popt [ 1 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'slope: {:.5f} ' . format ( slope )) print ( 'nugget: {:.5f} ' . format ( nugget )) self . parameter_vals = [ slope , nugget ] else : # Use the given variogram parameters. self . parameter_vals = variogram_params # Apply the cutoff. dist_cut = np . where ( dist < self . cutoff_dist , dist , np . nan ) gamma_cut = np . where ( dist < self . cutoff_dist , gamma , np . nan ) # Remove nans for curve_fit. dist_cut = dist_cut [ ~ np . isnan ( dist_cut )] gamma_cut = gamma_cut [ ~ np . isnan ( gamma_cut )] if self . show_plots == True : if self . bins == None : # Calculate 2d kde to help confirm cutoff. xi = np . linspace ( np . min ( dist ), np . max ( dist ), 60 ) yi = np . linspace ( np . min ( gamma ), np . max ( gamma ), 60 ) xi , yi = np . meshgrid ( xi , yi ) xyi = np . stack ([ xi . reshape ( - 1 ), yi . reshape ( - 1 )], axis = 1 ) . T kde = stats . gaussian_kde ([ dist , gamma ]) z = kde . evaluate ( xyi ) z = z . reshape ( len ( xi ), len ( yi )) plt . figure ( dpi = 100 ) plt . pcolormesh ( xi , yi , z , cmap = plt . cm . Blues , shading = 'auto' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () # With cutoff and variogram model. xnew = np . linspace ( np . min ( dist_cut ), self . cutoff_dist , 100 ) plt . figure ( dpi = 100 ) plt . scatter ( dist_cut , gamma_cut , fc = 'none' , ec = 'C1' , lw = 0.5 , alpha = 0.3 ) plt . plot ( xnew , self . variogram_func ( xnew , * self . parameter_vals ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () else : # Fit variogram plot. plt . figure ( dpi = 100 ) plt . scatter ( bin_centers , bin_means , fc = 'C1' , ec = 'k' , lw = 0.5 ) plt . plot ( bin_centers , self . variogram_func ( bin_centers , * self . parameter_vals ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () if self . variogram_func == gaussian or self . variogram_func == spherical : vrange = self . parameter_vals [ 0 ] sill = self . parameter_vals [ 1 ] nugget = self . parameter_vals [ 2 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'range: {:.5f} ' . format ( vrange )) print ( 'sill: {:.5f} ' . format ( sill )) print ( 'nugget: {:.5f} ' . format ( nugget )) print ( 'full sill: {:.5f} ' . format ( sill + nugget )) elif self . variogram_func == linear : slope = self . parameter_vals [ 0 ] nugget = self . parameter_vals [ 1 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'slope: {:.5f} ' . format ( slope )) print ( 'nugget: {:.5f} ' . format ( nugget )) #################################################### def predict ( self , x2_pred ): ''' Parameters: x2 : n-dim array Locations to make kriging predictions. Returns: u2_mean : float Kriging mean. u2_var : float Kriging variance. Performs ordinary or universal kriging using the estimated variogram parameters. ''' self . x2 = self . project ( x2_pred ) n1 = len ( self . x1 ) n2 = len ( self . x2 ) # Universal krige. if self . featurization : # Ax = b with a trend. # Build A D1 = cdist ( self . x1 , self . x1 ) drift_data = np . array ( list ( self . featurization ( self . x1 ))) An = n1 + 1 + drift_data . shape [ 0 ] A = np . zeros (( An , An )) A [: n1 , : n1 ] = - self . variogram_func ( D1 , * self . parameter_vals ) np . fill_diagonal ( A , 0. ) A [ n1 , : n1 ] = 1. A [: n1 , n1 ] = 1. A [ n1 , n1 ] = 0. # Add in the trend for A. for i in range ( drift_data . shape [ 0 ]): A [ n1 + i + 1 , : n1 ] = drift_data [ i ] A [: n1 , n1 + i + 1 ] = drift_data [ i ] # Build b. D2 = cdist ( self . x2 , self . x1 ) b = np . zeros (( D2 . shape [ 0 ], D2 . shape [ 1 ] + 1 + drift_data . shape [ 0 ])) b [: n2 , : n1 ] = - self . variogram_func ( D2 , * self . parameter_vals ) b = b . T b [ n1 , :] = 1. # Add the trend for b. drift_pred = np . array ( list ( self . featurization ( self . x2 ))) for i in range ( drift_pred . shape [ 0 ]): b [ n1 + 1 + i , :] = drift_pred [ i ] # Solve. x = np . linalg . solve ( A , b ) u2_mean = np . tensordot ( self . u1 , x [: n1 ], axes = 1 ) u2_var = np . sum ( x . T * - b . T , axis = 1 ) return u2_mean , u2_var # Ordinary krige. else : # Ax = b. # Build A. D1 = cdist ( self . x1 , self . x1 ) A = np . zeros (( n1 + 1 , n1 + 1 )) A [: n1 , : n1 ] = - self . variogram_func ( D1 , * self . parameter_vals ) np . fill_diagonal ( A , 0. ) A [ n1 , :] = 1. A [:, n1 ] = 1. A [ n1 , n1 ] = 0. # Build b. D2 = cdist ( self . x2 , self . x1 ) b = np . zeros (( D2 . shape [ 0 ], D2 . shape [ 1 ] + 1 )) b [: n2 , : n1 ] = - self . variogram_func ( D2 , * self . parameter_vals ) b = b . T b [ n1 , :] = 1. # Solve. x = np . linalg . solve ( A , b ) u2_mean = np . tensordot ( self . u1 , x [: n1 ], axes = 1 ) u2_var = np . sum ( x . T * - b . T , axis = 1 ) return u2_mean , u2_var ######################################################### # Access the projected coords of the input data. def get_projected ( self ): return self . x1 [:, 0 ], self . x1 [:, 1 ] ######################################################### # Provide some stats on the model. def stats ( self ): u2_mean_for_u1 , u2_var_for_u1 = self . predict ( self . x1_original ) self . residuals = self . u1 - u2_mean_for_u1 self . res_mean = np . mean ( self . residuals ) self . res_std = np . std ( self . residuals ) self . res_skew = stats . skew ( self . residuals ) self . res_kurt = stats . kurtosis ( self . residuals ) if self . show_plots == True : plt . figure () plt . hist ( self . u1 , label = 'Input values' ) plt . ylabel ( 'Count' ) plt . xlabel ( 'Input values' ) plt . show () plt . figure () plt . hist ( self . residuals , label = 'Residuals' ) plt . ylabel ( 'Count' ) plt . xlabel ( 'Residuals' ) plt . show () if self . verbose == True : print ( 'Residual mean: {:.3e} ' . format ( self . res_mean )) print ( 'Residual standard deviation: {:.3f} ' . format ( self . res_std )) print ( 'Residual skewness: {:.3f} ' . format ( self . res_skew )) print ( 'Residual kurtosis: {:.3f} ' . format ( self . res_kurt )) def get_residuals ( self ): return self . residuals def get_residual_moments ( self ): return dict ( mean = self . res_mean , std = self . res_std , skew = self . res_skew , kurt = self . res_kurt )","title":"Krige"},{"location":"api/#src.geostat.krige.Krige.__init__","text":"Parameters: Name Type Description Default x1 n-dim array Locations of input data. required u1 1-d array Values to be kriged. required bins int or None The number of bins to use on the variogram cloud. If None, variogram function is fit to the variogram cloud and is not binned first. required variogram_func str Name of the variogram model to use in the kriging. Should be 'linear', 'gaussian', or 'spherical'. Default is 'gaussian'. None cutoff_dist str or int, optional The maximum lag distance to include in variogram modeling. 'auto' featurization function, optional Should be a function that takes x1 (n-dim array of input data) and returns the coordinates, i.e., x, y, x 2, y 2. Example below. Default is None. None project function, opt A function that takes multiple vectors, and returns a tuple of projected vectors. required epsg_proj str The projected coordinate system to use. Ignored if project=False. Default is 'EPSG:3310' (California Albers). required show_plots boolean, optional Whether or not to show variogram plots. Default is True. True verbose boolean, optional Whether or not to print parameters. Default is True. True Performs experimental variogram calculation, bins data, and fits variogram model to estimate variogram parameters. Performs ordinary and universal kriging in up to 3 spatial dimensions. Trend model example: def featurization(x1): return x1[:, 0], x1[:, 1], x1[:, 0] 2, x1[:, 1] 2 Source code in src/geostat/krige.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 def __init__ ( self , x1 , u1 , bins , variogram_func = None , variogram_params = None , cutoff_dist = 'auto' , featurization = None , projection = None , show_plots = True , verbose = True , ): ''' Parameters: x1 : n-dim array Locations of input data. u1 : 1-d array Values to be kriged. bins : int or None The number of bins to use on the variogram cloud. If None, variogram function is fit to the variogram cloud and is not binned first. variogram_func : str Name of the variogram model to use in the kriging. Should be 'linear', 'gaussian', or 'spherical'. Default is 'gaussian'. cutoff_dist : str or int, optional The maximum lag distance to include in variogram modeling. featurization : function, optional Should be a function that takes x1 (n-dim array of input data) and returns the coordinates, i.e., x, y, x**2, y**2. Example below. Default is None. project : function, opt A function that takes multiple vectors, and returns a tuple of projected vectors. epsg_proj : str The projected coordinate system to use. Ignored if project=False. Default is 'EPSG:3310' (California Albers). show_plots : boolean, optional Whether or not to show variogram plots. Default is True. verbose : boolean, optional Whether or not to print parameters. Default is True. Performs experimental variogram calculation, bins data, and fits variogram model to estimate variogram parameters. Performs ordinary and universal kriging in up to 3 spatial dimensions. Trend model example: def featurization(x1): return x1[:, 0], x1[:, 1], x1[:, 0]**2, x1[:, 1]**2 ''' super () . __init__ ( projection = projection ) # Save original x1 for stats function. self . x1_original = x1 # Check and change the shape of u1 if needed (for pdist). if u1 . ndim == 1 : self . u1 = u1 elif u1 . shape [ 1 ] == 1 : self . u1 = u1 . reshape ( - 1 ) else : raise ValueError ( \"Check dimensions of 'u1'.\" ) # Projection. self . x1 = self . project ( x1 ) # Variogram. if variogram_func == 'gaussian' : self . variogram_func = gaussian elif variogram_func == 'spherical' : self . variogram_func = spherical elif variogram_func == 'linear' : self . variogram_func = linear else : raise ValueError ( \"Variogram function must be 'linear', 'gaussian', or 'spherical'.\" ) if cutoff_dist == 'auto' : self . cutoff_dist = cutoff_dist_func ( self . x1 ) else : self . cutoff_dist = cutoff_dist self . verbose = verbose self . show_plots = show_plots self . bins = bins self . featurization = featurization # Lags and semivariance. dist = pdist ( self . x1 , metric = 'euclidean' ) gamma = 0.5 * pdist ( self . u1 . reshape ( - 1 , 1 ), metric = 'sqeuclidean' ) if self . bins != None : # Use bins. # Variogram cloud calculation. bin_means , bin_edges , binnumber = binned_statistic ( dist , gamma , statistic = 'mean' , bins = self . bins , range = [ dist . min (), self . cutoff_dist ]) bin_width = ( bin_edges [ 1 ] - bin_edges [ 0 ]) bin_centers = bin_edges [ 1 :] - bin_width / 2 # Bin counts calculation. bin_count , bin_count_edges , bin_count_number = binned_statistic ( dist , gamma , statistic = 'count' , bins = self . bins ) bin_count_width = ( bin_count_edges [ 1 ] - bin_count_edges [ 0 ]) bin_count_centers = bin_count_edges [ 1 :] - bin_count_width / 2 if self . show_plots == True : if self . bins == None : # Variogram cloud plot. plt . figure ( dpi = 100 ) plt . scatter ( dist , gamma , ec = 'C0' , fc = 'none' , alpha = 0.3 ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () if self . bins != None : # Variogram cloud plot. plt . figure ( dpi = 100 ) plt . scatter ( dist , gamma , ec = 'C0' , fc = 'none' , alpha = 0.3 ) plt . hlines ( bin_means , bin_edges [: - 1 ], bin_edges [ 1 :], zorder = 1 , color = 'k' ) plt . scatter ( bin_centers , bin_means , ec = 'k' , lw = 0.5 ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () # Bin counts plot. plt . figure ( dpi = 100 ) plt . hlines ( bin_count , bin_count_edges [: - 1 ], bin_count_edges [ 1 :], zorder = 1 , color = 'k' ) plt . scatter ( bin_count_centers , bin_count , ec = 'k' , lw = 0.5 ) plt . ylabel ( 'Bin count' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () ############ Fit the variogram model. if not variogram_params : # Fit to the data. if self . bins == None : # Fit the variogram cloud. if self . variogram_func == gaussian or self . variogram_func == spherical : # Initial guess for the parameters. p0 = [ 0.25 * np . max ( dist ), # range np . max ( gamma ) - np . min ( gamma ), # sill np . min ( gamma ) + 1e-6 ] # nugget # Bounds with constraints. bounds = [( 1e-6 , 1e-6 , 1e-6 ), ( np . inf , np . inf , np . inf )] # Apply the cutoff. dist_cut = np . where ( dist < self . cutoff_dist , dist , np . nan ) gamma_cut = np . where ( dist < self . cutoff_dist , gamma , np . nan ) # Remove nans for curve_fit. dist_cut = dist_cut [ ~ np . isnan ( dist_cut )] gamma_cut = gamma_cut [ ~ np . isnan ( gamma_cut )] popt , pcov = curve_fit ( self . variogram_func , dist_cut , gamma_cut , p0 = p0 , bounds = bounds ) if self . show_plots == True : # Calculate 2d kde to help confirm cutoff. xi = np . linspace ( np . min ( dist ), np . max ( dist ), 60 ) yi = np . linspace ( np . min ( gamma ), np . max ( gamma ), 60 ) xi , yi = np . meshgrid ( xi , yi ) xyi = np . stack ([ xi . reshape ( - 1 ), yi . reshape ( - 1 )], axis = 1 ) . T kde = stats . gaussian_kde ([ dist , gamma ]) z = kde . evaluate ( xyi ) z = z . reshape ( len ( xi ), len ( yi )) plt . figure ( dpi = 100 ) plt . pcolormesh ( xi , yi , z , cmap = plt . cm . Blues , shading = 'auto' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () # With cutoff and variogram model. xnew = np . linspace ( np . min ( dist_cut ), self . cutoff_dist , 100 ) plt . figure ( dpi = 100 ) plt . scatter ( dist_cut , gamma_cut , fc = 'none' , ec = 'C1' , lw = 0.5 , alpha = 0.3 ) plt . plot ( xnew , self . variogram_func ( xnew , * popt ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () vrange = popt [ 0 ] sill = popt [ 1 ] nugget = popt [ 2 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'range: {:.5f} ' . format ( vrange )) print ( 'sill: {:.5f} ' . format ( sill )) print ( 'nugget: {:.5f} ' . format ( nugget )) print ( 'full sill: {:.5f} ' . format ( sill + nugget )) self . parameter_vals = [ vrange , sill , nugget ] elif self . variogram_func == linear : # Initial guess for the parameters. p0 = [( np . max ( dist ) - np . min ( dist )) / ( np . max ( gamma ) - np . min ( gamma )), # slope np . min ( gamma ) + 1e-6 ] # nugget # Bounds with constraints. bounds = [( 1e-6 , 1e-6 ), ( np . inf , np . inf )] # Apply the cutoff. dist_cut = np . where ( dist < self . cutoff_dist , dist , np . nan ) gamma_cut = np . where ( dist < self . cutoff_dist , gamma , np . nan ) # Remove nans for curve_fit. dist_cut = dist_cut [ ~ np . isnan ( dist_cut )] gamma_cut = gamma_cut [ ~ np . isnan ( gamma_cut )] popt , pcov = curve_fit ( self . variogram_func , dist_cut , gamma_cut , p0 = p0 , bounds = bounds ) if self . show_plots == True : # Calculate 2d kde to help confirm cutoff. xi = np . linspace ( np . min ( dist ), np . max ( dist ), 60 ) yi = np . linspace ( np . min ( gamma ), np . max ( gamma ), 60 ) xi , yi = np . meshgrid ( xi , yi ) xyi = np . stack ([ xi . reshape ( - 1 ), yi . reshape ( - 1 )], axis = 1 ) . T kde = stats . gaussian_kde ([ dist , gamma ]) z = kde . evaluate ( xyi ) z = z . reshape ( len ( xi ), len ( yi )) plt . figure ( dpi = 100 ) plt . pcolormesh ( xi , yi , z , cmap = plt . cm . Blues , shading = 'auto' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () # With cutoff and variogram model. xnew = np . linspace ( np . min ( dist_cut ), self . cutoff_dist , 100 ) plt . figure ( dpi = 100 ) plt . scatter ( dist_cut , gamma_cut , fc = 'none' , ec = 'C1' , lw = 0.5 , alpha = 0.3 ) plt . plot ( xnew , self . variogram_func ( xnew , * popt ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () slope = popt [ 0 ] nugget = popt [ 1 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'slope: {:.5f} ' . format ( slope )) print ( 'nugget: {:.5f} ' . format ( nugget )) self . parameter_vals = [ slope , nugget ] elif self . bins != None : # Fit the binned data. if self . variogram_func == gaussian or self . variogram_func == spherical : # Initial guess for the parameters. p0 = [ 0.25 * np . max ( bin_centers ), # range np . max ( bin_means ) - np . min ( bin_means ), # sill np . min ( bin_means )] # nugget # Bounds with constraints. bounds = [( 1e-6 , 1e-6 , 1e-6 ), ( np . inf , np . inf , np . inf )] popt , pcov = curve_fit ( self . variogram_func , bin_centers , bin_means , p0 = p0 , bounds = bounds ) if self . show_plots == True : # Fit variogram plot. plt . figure ( dpi = 100 ) plt . scatter ( bin_centers , bin_means , c = 'C1' , ec = 'k' , lw = 0.5 ) plt . plot ( bin_centers , self . variogram_func ( bin_centers , * popt ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () vrange = popt [ 0 ] sill = popt [ 1 ] nugget = popt [ 2 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'range: {:.5f} ' . format ( vrange )) print ( 'sill: {:.5f} ' . format ( sill )) print ( 'nugget: {:.5f} ' . format ( nugget )) print ( 'full sill: {:.5f} ' . format ( sill + nugget )) self . parameter_vals = [ vrange , sill , nugget ] elif self . variogram_func == linear : # Initial guess for the parameters. p0 = [( np . max ( bin_centers ) - np . min ( bin_centers )) / ( np . max ( bin_means ) - np . min ( bin_means )), # slope np . min ( bin_means )] # nugget # Bounds with constraints. bounds = [( 1e-6 , 1e-6 ), ( np . inf , np . inf )] popt , pcov = curve_fit ( self . variogram_func , bin_centers , bin_means , p0 = p0 , bounds = bounds ) if self . show_plots == True : # Fit variogram plot. plt . figure ( dpi = 100 ) plt . scatter ( bin_centers , bin_means , c = 'C1' , ec = 'k' , lw = 0.5 ) plt . plot ( bin_centers , self . variogram_func ( bin_centers , * popt ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () slope = popt [ 0 ] nugget = popt [ 1 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'slope: {:.5f} ' . format ( slope )) print ( 'nugget: {:.5f} ' . format ( nugget )) self . parameter_vals = [ slope , nugget ] else : # Use the given variogram parameters. self . parameter_vals = variogram_params # Apply the cutoff. dist_cut = np . where ( dist < self . cutoff_dist , dist , np . nan ) gamma_cut = np . where ( dist < self . cutoff_dist , gamma , np . nan ) # Remove nans for curve_fit. dist_cut = dist_cut [ ~ np . isnan ( dist_cut )] gamma_cut = gamma_cut [ ~ np . isnan ( gamma_cut )] if self . show_plots == True : if self . bins == None : # Calculate 2d kde to help confirm cutoff. xi = np . linspace ( np . min ( dist ), np . max ( dist ), 60 ) yi = np . linspace ( np . min ( gamma ), np . max ( gamma ), 60 ) xi , yi = np . meshgrid ( xi , yi ) xyi = np . stack ([ xi . reshape ( - 1 ), yi . reshape ( - 1 )], axis = 1 ) . T kde = stats . gaussian_kde ([ dist , gamma ]) z = kde . evaluate ( xyi ) z = z . reshape ( len ( xi ), len ( yi )) plt . figure ( dpi = 100 ) plt . pcolormesh ( xi , yi , z , cmap = plt . cm . Blues , shading = 'auto' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () # With cutoff and variogram model. xnew = np . linspace ( np . min ( dist_cut ), self . cutoff_dist , 100 ) plt . figure ( dpi = 100 ) plt . scatter ( dist_cut , gamma_cut , fc = 'none' , ec = 'C1' , lw = 0.5 , alpha = 0.3 ) plt . plot ( xnew , self . variogram_func ( xnew , * self . parameter_vals ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () else : # Fit variogram plot. plt . figure ( dpi = 100 ) plt . scatter ( bin_centers , bin_means , fc = 'C1' , ec = 'k' , lw = 0.5 ) plt . plot ( bin_centers , self . variogram_func ( bin_centers , * self . parameter_vals ), color = 'k' ) plt . ylabel ( '$\\gamma(h)$' ) plt . xlabel ( '$h$' ) plt . grid ( alpha = 0.4 ) plt . show () if self . variogram_func == gaussian or self . variogram_func == spherical : vrange = self . parameter_vals [ 0 ] sill = self . parameter_vals [ 1 ] nugget = self . parameter_vals [ 2 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'range: {:.5f} ' . format ( vrange )) print ( 'sill: {:.5f} ' . format ( sill )) print ( 'nugget: {:.5f} ' . format ( nugget )) print ( 'full sill: {:.5f} ' . format ( sill + nugget )) elif self . variogram_func == linear : slope = self . parameter_vals [ 0 ] nugget = self . parameter_vals [ 1 ] if self . verbose == True : print ( 'variogram model: {} ' . format ( variogram_func )) print ( 'cutoff: {:.2f} ' . format ( self . cutoff_dist )) print ( 'slope: {:.5f} ' . format ( slope )) print ( 'nugget: {:.5f} ' . format ( nugget ))","title":"__init__"},{"location":"api/#src.geostat.krige.Krige.predict","text":"Parameters: Name Type Description Default x2 n-dim array Locations to make kriging predictions. required Returns: Name Type Description u2_mean float Kriging mean. u2_var float Kriging variance. Performs ordinary or universal kriging using the estimated variogram parameters. Source code in src/geostat/krige.py 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 def predict ( self , x2_pred ): ''' Parameters: x2 : n-dim array Locations to make kriging predictions. Returns: u2_mean : float Kriging mean. u2_var : float Kriging variance. Performs ordinary or universal kriging using the estimated variogram parameters. ''' self . x2 = self . project ( x2_pred ) n1 = len ( self . x1 ) n2 = len ( self . x2 ) # Universal krige. if self . featurization : # Ax = b with a trend. # Build A D1 = cdist ( self . x1 , self . x1 ) drift_data = np . array ( list ( self . featurization ( self . x1 ))) An = n1 + 1 + drift_data . shape [ 0 ] A = np . zeros (( An , An )) A [: n1 , : n1 ] = - self . variogram_func ( D1 , * self . parameter_vals ) np . fill_diagonal ( A , 0. ) A [ n1 , : n1 ] = 1. A [: n1 , n1 ] = 1. A [ n1 , n1 ] = 0. # Add in the trend for A. for i in range ( drift_data . shape [ 0 ]): A [ n1 + i + 1 , : n1 ] = drift_data [ i ] A [: n1 , n1 + i + 1 ] = drift_data [ i ] # Build b. D2 = cdist ( self . x2 , self . x1 ) b = np . zeros (( D2 . shape [ 0 ], D2 . shape [ 1 ] + 1 + drift_data . shape [ 0 ])) b [: n2 , : n1 ] = - self . variogram_func ( D2 , * self . parameter_vals ) b = b . T b [ n1 , :] = 1. # Add the trend for b. drift_pred = np . array ( list ( self . featurization ( self . x2 ))) for i in range ( drift_pred . shape [ 0 ]): b [ n1 + 1 + i , :] = drift_pred [ i ] # Solve. x = np . linalg . solve ( A , b ) u2_mean = np . tensordot ( self . u1 , x [: n1 ], axes = 1 ) u2_var = np . sum ( x . T * - b . T , axis = 1 ) return u2_mean , u2_var # Ordinary krige. else : # Ax = b. # Build A. D1 = cdist ( self . x1 , self . x1 ) A = np . zeros (( n1 + 1 , n1 + 1 )) A [: n1 , : n1 ] = - self . variogram_func ( D1 , * self . parameter_vals ) np . fill_diagonal ( A , 0. ) A [ n1 , :] = 1. A [:, n1 ] = 1. A [ n1 , n1 ] = 0. # Build b. D2 = cdist ( self . x2 , self . x1 ) b = np . zeros (( D2 . shape [ 0 ], D2 . shape [ 1 ] + 1 )) b [: n2 , : n1 ] = - self . variogram_func ( D2 , * self . parameter_vals ) b = b . T b [ n1 , :] = 1. # Solve. x = np . linalg . solve ( A , b ) u2_mean = np . tensordot ( self . u1 , x [: n1 ], axes = 1 ) u2_var = np . sum ( x . T * - b . T , axis = 1 ) return u2_mean , u2_var","title":"predict"},{"location":"api/#src.geostat.krige.cutoff_dist_func","text":"Parameters: Name Type Description Default x n-dim array Locations of input data. required Returns: Name Type Description cutoff float The maximum lag distance to use in fitting the variogram. Found using Pythagorean Theorem to roughly find one half the distance across the study area. Source code in src/geostat/krige.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def cutoff_dist_func ( x ): ''' Parameters: x : n-dim array Locations of input data. Returns: cutoff : float The maximum lag distance to use in fitting the variogram. Found using Pythagorean Theorem to roughly find one half the distance across the study area. ''' a2 = np . square ( x [:, 0 ] . max () - x [:, 0 ] . min ()) b2 = np . square ( x [:, 1 ] . max () - x [:, 1 ] . min ()) cutoff = np . sqrt ( a2 + b2 ) / 2 return cutoff","title":"cutoff_dist_func"},{"location":"api/#src.geostat.mean","text":"","title":"mean"},{"location":"api/#src.geostat.mean.Mean","text":"Bases: Op Source code in src/geostat/mean.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class Mean ( Op ): def __init__ ( self , fa , autoinputs ): if 'locs1' not in autoinputs : autoinputs [ 'locs1' ] = 'locs1' super () . __init__ ( fa , autoinputs ) def __add__ ( self , other ): if isinstance ( other , ZeroTrend ): return self elif isinstance ( self , ZeroTrend ): return other else : return Stack ([ self ]) + other def call ( self , e ): pass def __call__ ( self , e ): \"\"\" Returns tuple `(mean, covariance)` for locations. Return values have correct shapes. \"\"\" M = self . call ( e ) if M is None : M = 0. n1 = tf . shape ( e [ 'locs1' ])[ 0 ] M = tf . broadcast_to ( M , [ n1 ]) return M","title":"Mean"},{"location":"api/#src.geostat.mean.Mean.__call__","text":"Returns tuple (mean, covariance) for locations. Return values have correct shapes. Source code in src/geostat/mean.py 39 40 41 42 43 44 45 46 47 48 def __call__ ( self , e ): \"\"\" Returns tuple `(mean, covariance)` for locations. Return values have correct shapes. \"\"\" M = self . call ( e ) if M is None : M = 0. n1 = tf . shape ( e [ 'locs1' ])[ 0 ] M = tf . broadcast_to ( M , [ n1 ]) return M","title":"__call__"},{"location":"api/#src.geostat.mesh","text":"","title":"mesh"},{"location":"api/#src.geostat.metric","text":"","title":"metric"},{"location":"api/#src.geostat.model","text":"","title":"model"},{"location":"api/#src.geostat.model.Featurizer","text":"Produces featurized locations (F matrix). Source code in src/geostat/model.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 class Featurizer : \"\"\" Produces featurized locations (F matrix). \"\"\" def __init__ ( self , featurization ): self . featurization = featurization def __call__ ( self , locs ): locs = tf . cast ( locs , tf . float32 ) if self . featurization is None : # No features. return tf . ones ([ tf . shape ( locs )[ 0 ], 0 ], dtype = tf . float32 ) feats = self . featurization ( * tf . unstack ( locs , axis = 1 )) if isinstance ( feats , tuple ): # One or many features. if len ( feats ) == 0 : return tf . ones ([ tf . shape ( locs )[ 0 ], 0 ], dtype = tf . float32 ) else : feats = self . featurization ( * tf . unstack ( locs , axis = 1 )) feats = [ tf . broadcast_to ( tf . cast ( f , tf . float32 ), [ tf . shape ( locs )[ 0 ]]) for f in feats ] return tf . stack ( feats , axis = 1 ) else : # One feature. return e ( feats )","title":"Featurizer"},{"location":"api/#src.geostat.model.Model","text":"Source code in src/geostat/model.py 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 @dataclass class Model (): gp : GP warp : Warp = None parameter_sample_size : Optional [ int ] = None locs : np . ndarray = None vals : np . ndarray = None cats : np . ndarray = None report : Callable = None verbose : bool = True def __post_init__ ( self ): ''' Parameters: x : Pandas DataFrame with columns for locations. u : A Pandas Series containing observations. featurization : function, optional Should be a function that takes x1 (n-dim array of input data) and returns the coordinates, i.e., x, y, x**2, y**2. Example: def featurization(x1): return x1[:, 0], x1[:, 1], x1[:, 0]**2, x1[:, 1]**2. Default is None. latent : List[GP] Name of the covariance function to use in the GP. Should be 'squared-exp' or 'gamma-exp'. Default is 'squared-exp'. verbose : boolean, optional Whether or not to print parameters. Default is True. Performs Gaussian process training and prediction. ''' if self . warp is None : self . warp = NoWarp () # Default reporting function. def default_report ( p , prefix = None ): if prefix : print ( prefix , end = ' ' ) def fmt ( x ): if isinstance ( x , tf . Tensor ): x = x . numpy () if isinstance ( x , ( int , np . int32 , np . int64 )): return ' {:5d} ' . format ( x ) if isinstance ( x , ( float , np . float32 , np . float64 )): return ' {:5.2f} ' . format ( x ) else : with np . printoptions ( precision = 2 , formatter = { 'floatkind' : ' {:5.2f} ' . format }): return str ( x ) print ( '[ %s ]' % ( ' ' . join ( ' %s %s ' % ( k , fmt ( v )) for k , v in p . items ()))) if self . report == None : self . report = default_report if self . locs is not None : self . locs = np . array ( self . locs ) if self . vals is not None : self . vals = np . array ( self . vals ) if self . cats is not None : self . cats = np . array ( self . cats ) # Collect parameters and create TF parameters. for p in self . gather_vars () . values (): p . create_tf_variable () def gather_vars ( self ): return self . gp . gather_vars () | self . warp . gather_vars () def set ( self , ** values ): parameters = self . gather_vars () for name , v in values . items (): if name in parameters : parameters [ name ] . value = v parameters [ name ] . create_tf_variable () else : raise ValueError ( f \" { k } is not a parameter\" ) return self def fit ( self , locs , vals , cats = None , step_size = 0.01 , iters = 100 , reg = None ): # Collect parameters and create TF parameters. parameters = self . gather_vars () # Permute datapoints if cats is given. if cats is not None : cats = np . array ( cats ) perm = np . argsort ( cats ) locs , vals , cats = locs [ perm ], vals [ perm ], cats [ perm ] else : cats = np . zeros ( locs . shape [: 1 ], np . int32 ) perm = None # Data dict. self . data = { 'warplocs' : self . warp ( locs ), 'vals' : tf . constant ( vals , dtype = tf . float32 ), 'cats' : tf . constant ( cats , dtype = tf . int32 )} optimizer = tf . keras . optimizers . Adam ( learning_rate = step_size ) j = 0 # Iteration count. for i in range ( 10 ): t0 = time . time () while j < ( i + 1 ) * iters / 10 : ll , reg_penalty = gp_train_step ( optimizer , self . data , parameters , self . gp , reg ) j += 1 time_elapsed = time . time () - t0 if self . verbose == True : self . report ( dict ( iter = j , ll = ll , time = time_elapsed , reg = reg_penalty ) | { p . name : p . surface () for p in parameters . values ()}) # Save parameter values. for p in parameters . values (): p . update_value () # Restore order if things were permuted. if perm is not None : revperm = np . argsort ( perm ) locs , vals , cats = locs [ revperm ], vals [ revperm ], cats [ revperm ] self . locs = locs self . vals = vals self . cats = cats return self def mcmc ( self , locs , vals , cats = None , chains = 4 , step_size = 0.1 , move_prob = 0.5 , samples = 1000 , burnin = 500 , report_interval = 100 ): assert samples % report_interval == 0 , '`samples` must be a multiple of `report_interval`' assert burnin % report_interval == 0 , '`burnin` must be a multiple of `report_interval`' # Permute datapoints if cats is given. if cats is not None : cats = np . array ( cats ) perm = np . argsort ( cats ) locs , vals , cats = locs [ perm ], vals [ perm ], cats [ perm ] # Data dict. self . data = { 'locs' : tf . constant ( locs , dtype = tf . float32 ), 'vals' : tf . constant ( vals , dtype = tf . float32 ), 'cats' : None if cats is None else tf . constant ( cats , dtype = tf . int32 )} # Initial MCMC state. initial_up = self . parameter_space . get_underlying ( self . parameters ) # Unnormalized log posterior distribution. def g ( up ): sp = self . parameter_space . get_surface ( up ) return gp_log_likelihood ( self . data , sp , self . gp ) def f ( * up_flat ): up = tf . nest . pack_sequence_as ( initial_up , up_flat ) ll = tf . map_fn ( g , up , fn_output_signature = tf . float32 ) # log_prior = -tf.reduce_sum(tf.math.log(1. + tf.square(up_flat)), axis=0) return ll # + log_prior # Run the chain for a burst. @tf . function def run_chain ( current_state , final_results , kernel , iters ): samples , results , final_results = tfp . mcmc . sample_chain ( num_results = iters , current_state = current_state , kernel = kernel , return_final_kernel_results = True , trace_fn = lambda _ , results : results ) return samples , results , final_results def new_state_fn ( scale , dtype ): direction_dist = tfd . Normal ( loc = dtype ( 0 ), scale = dtype ( 1 )) scale_dist = tfd . Exponential ( rate = dtype ( 1 / scale )) pick_dist = tfd . Bernoulli ( probs = move_prob ) def _fn ( state_parts , seed ): next_state_parts = [] part_seeds = tfp . random . split_seed ( seed , n = len ( state_parts ), salt = 'rwmcauchy' ) for sp , ps in zip ( state_parts , part_seeds ): pick = tf . cast ( pick_dist . sample ( sample_shape = sp . shape , seed = ps ), tf . float32 ) direction = direction_dist . sample ( sample_shape = sp . shape , seed = ps ) scale_val = scale_dist . sample ( seed = ps ) next_state_parts . append ( sp + tf . einsum ( 'a...,a->a...' , pick * direction , scale_val )) return next_state_parts return _fn inv_temps = 0.5 ** np . arange ( chains , dtype = np . float32 ) def make_kernel_fn ( target_log_prob_fn ): return tfp . mcmc . RandomWalkMetropolis ( target_log_prob_fn = target_log_prob_fn , new_state_fn = new_state_fn ( scale = step_size / np . sqrt ( inv_temps ), dtype = np . float32 )) kernel = tfp . mcmc . ReplicaExchangeMC ( target_log_prob_fn = f , inverse_temperatures = inv_temps , make_kernel_fn = make_kernel_fn ) # Do bursts. current_state = tf . nest . flatten ( initial_up ) final_results = None acc_states = [] num_bursts = ( samples + burnin ) // report_interval burnin_bursts = burnin // report_interval for i in range ( num_bursts ): is_burnin = i < burnin_bursts if self . verbose and ( i == 0 or i == burnin_bursts ): print ( 'BURNIN \\n ' if is_burnin else ' \\n SAMPLING' ) t0 = time . time () states , results , final_results = run_chain ( current_state , final_results , kernel , report_interval ) if self . verbose == True : if not is_burnin : print () accept_rates = results . post_swap_replica_results . is_accepted . numpy () . mean ( axis = 0 ) print ( '[iter {:4d} ] [time {:.1f} ] [accept rates {} ]' . format ( (( i if is_burnin else i - burnin_bursts ) + 1 ) * report_interval , time . time () - t0 , ' ' . join ([ f ' { x : .2f } ' for x in accept_rates . tolist ()]))) if not is_burnin : acc_states . append ( tf . nest . map_structure ( lambda x : x . numpy (), states )) all_states = [ np . concatenate ( x , 0 ) for x in zip ( * acc_states )] up = tf . nest . pack_sequence_as ( initial_up , all_states ) sp = self . parameter_space . get_surface ( up , numpy = True ) # Reporting if self . verbose == True : for p in [ 5 , 50 , 95 ]: x = tf . nest . map_structure ( lambda x : np . percentile ( x , p , axis = 0 ), sp ) self . report ( x , prefix = f ' { p : 02d } %ile' ) current_state = [ s [ - 1 ] for s in states ] posterior = self . parameter_space . get_surface ( up , numpy = True ) # Restore order if things were permuted. if cats is not None : revperm = np . argsort ( perm ) locs , vals , cats = locs [ revperm ], vals [ revperm ], cats [ revperm ] return replace ( self , parameters = posterior , parameter_sample_size = samples , locs = locs , vals = vals , cats = cats ) def generate ( self , locs , cats = None ): assert self . locs is None and self . vals is None , 'Conditional generation not yet supported' assert self . parameter_sample_size is None , 'Generation from a distribution not yet supported' locs = np . array ( locs ) # Permute datapoints if cats is given. if cats is not None : cats = np . array ( cats ) perm = np . argsort ( cats ) locs , cats = locs [ perm ], cats [ perm ] else : cats = np . zeros ( locs . shape [: 1 ], np . int32 ) perm = None m , S = gp_covariance ( self . gp , self . warp ( locs ) . run ({}), None if cats is None else tf . constant ( cats , dtype = tf . int32 )) vals = MVN ( m , tf . linalg . cholesky ( S )) . sample () . numpy () # Restore order if things were permuted. if perm is not None : revperm = np . argsort ( perm ) locs , vals , cats = locs [ revperm ], vals [ revperm ], cats [ revperm ] self . locs = locs self . vals = vals self . cats = cats return self def predict ( self , locs2 , cats2 = None , * , subsample = None , reduce = None , tracker = None , pair = False ): ''' Performs GP predictions of the mean and variance. Has support for batch predictions for large data sets. ''' assert subsample is None or self . parameter_sample_size is not None , \\ '`subsample` is only valid with sampled parameters' assert reduce is None or self . parameter_sample_size is not None , \\ '`reduce` is only valid with sampled parameters' assert subsample is None or reduce is None , \\ '`subsample` and `reduce` cannot both be given' if tracker is None : tracker = lambda x : x assert self . locs . shape [ - 1 ] == locs2 . shape [ - 1 ], 'Mismatch in location dimensions' if cats2 is not None : assert cats2 . shape == locs2 . shape [: 1 ], 'Mismatched shapes in cats and locs' else : cats2 = np . zeros ( locs2 . shape [: 1 ], np . int32 ) def interpolate_batch ( A11i , locs1 , vals1diff , cats1 , locs2 , cats2 ): \"\"\" Inputs: locs1.shape = [N1, K] vals1diff.shape = [N1] cats1.shape = [N1] locs2.shape = [N2, K] cats2.shape = [N2] Outputs: u2_mean.shape = [N2] u2_var.shape = [N2] \"\"\" N1 = len ( locs1 ) # Number of measurements. # Permute datapoints if cats is given. if cats2 is not None : perm = np . argsort ( cats2 ) locs2 , cats2 = locs2 [ perm ], cats2 [ perm ] locs2 = self . warp ( locs2 ) . run ({}) _ , A12 = gp_covariance2 ( self . gp , tf . constant ( locs1 , dtype = tf . float32 ), tf . constant ( cats1 , dtype = tf . int32 ), tf . constant ( locs2 , dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 ), N1 ) m2 , A22 = gp_covariance ( self . gp , tf . constant ( locs2 , dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 )) # Restore order if things were permuted. if cats2 is not None : revperm = np . argsort ( perm ) m2 = tf . gather ( m2 , revperm ) A12 = tf . gather ( A12 , revperm , axis =- 1 ) A22 = tf . gather ( tf . gather ( A22 , revperm ), revperm , axis =- 1 ) u2_mean = m2 + tf . einsum ( 'ab,a->b' , A12 , tf . einsum ( 'ab,b->a' , A11i , vals1diff )) u2_var = tf . linalg . diag_part ( A22 ) - tf . einsum ( 'ab,ab->b' , A12 , tf . matmul ( A11i , A12 )) return u2_mean , u2_var def interpolate_pair_batch ( A11i , locs1 , vals1diff , cats1 , locs2 , cats2 ): \"\"\" Inputs: locs1.shape = [N1, K] vals1diff.shape = [N1] cats1.shape = [N1] locs2.shape = [N2, 2, K] cats2.shape = [N2] Outputs: u2_mean.shape = [N2, 2] u2_var.shape = [N2, 2, 2] \"\"\" N1 = len ( locs1 ) # Number of measurements. N2 = len ( locs2 ) # Number of prediction pairs. # Permute datapoints if cats is given. perm = np . argsort ( cats2 ) locs2 , cats2 = locs2 [ perm ], cats2 [ perm ] # Warp locs2. locs2_shape = locs2 . shape locs2 = locs2 . reshape ([ - 1 , locs2_shape [ - 1 ]]) # Shape into matrix. locs2 = self . warp ( locs2 ) . run ({}) locs2 = tf . reshape ( locs2 , locs2_shape ) # Revert shape. _ , A12 = gp_covariance2 ( self . gp , tf . constant ( locs1 , dtype = tf . float32 ), tf . constant ( cats1 , dtype = tf . int32 ), tf . constant ( locs2 [:, 0 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 ), N1 ) _ , A13 = gp_covariance2 ( self . gp , tf . constant ( locs1 , dtype = tf . float32 ), tf . constant ( cats1 , dtype = tf . int32 ), tf . constant ( locs2 [:, 1 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 ), N1 ) m2 , A22 = gp_covariance ( self . gp , tf . constant ( locs2 [:, 0 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 )) m3 , A33 = gp_covariance ( self . gp , tf . constant ( locs2 [:, 1 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 )) _ , A23 = gp_covariance2 ( self . gp , tf . constant ( locs2 [:, 0 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 ), tf . constant ( locs2 [:, 1 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 ), N2 ) # Reassemble into more useful shapes. A12 = tf . stack ([ A12 , A13 ], axis =- 1 ) # [N1, N2, 2] m2 = tf . stack ([ m2 , m3 ], axis =- 1 ) # [N2, 2] A22 = tf . linalg . diag_part ( A22 ) A33 = tf . linalg . diag_part ( A33 ) A23 = tf . linalg . diag_part ( A23 ) A22 = tf . stack ([ tf . stack ([ A22 , A23 ], axis =- 1 ), tf . stack ([ A23 , A33 ], axis =- 1 )], axis =- 2 ) # [N2, 2, 2] # Restore order if things were permuted. revperm = np . argsort ( perm ) m2 = tf . gather ( m2 , revperm ) A12 = tf . gather ( A12 , revperm , axis = 1 ) A22 = tf . gather ( A22 , revperm ) u2_mean = m2 + tf . einsum ( 'abc,a->bc' , A12 , tf . einsum ( 'ab,b->a' , A11i , vals1diff )) u2_var = A22 - tf . einsum ( 'abc,abd->bcd' , A12 , tf . einsum ( 'ae,ebd->abd' , A11i , A12 )) return u2_mean , u2_var def interpolate ( locs1 , vals1 , cats1 , locs2 , cats2 , pair = False ): # Interpolate in batches. batch_size = locs1 . shape [ 0 ] // 2 for_gp = [] for start in np . arange ( 0 , len ( locs2 ), batch_size ): stop = start + batch_size subset = locs2 [ start : stop ], cats2 [ start : stop ] for_gp . append ( subset ) # Permute datapoints if cats is given. if cats1 is not None : perm = np . argsort ( cats1 ) locs1 , vals1 , cats1 = locs1 [ perm ], vals1 [ perm ], cats1 [ perm ] locs1 = self . warp ( locs1 ) . run ({}) m1 , A11 = gp_covariance ( self . gp , tf . constant ( locs1 , dtype = tf . float32 ), tf . constant ( cats1 , dtype = tf . int32 )) A11i = tf . linalg . inv ( A11 ) u2_mean_s = [] u2_var_s = [] f = interpolate_pair_batch if pair else interpolate_batch for locs_subset , cats_subset in for_gp : u2_mean , u2_var = f ( A11i , locs1 , vals1 - m1 , cats1 , locs_subset , cats_subset ) u2_mean = u2_mean . numpy () u2_var = u2_var . numpy () u2_mean_s . append ( u2_mean ) u2_var_s . append ( u2_var ) u2_mean = np . concatenate ( u2_mean_s ) u2_var = np . concatenate ( u2_var_s ) return u2_mean , u2_var if self . parameter_sample_size is None : m , v = interpolate ( self . locs , self . vals , self . cats , locs2 , cats2 , pair ) elif reduce == 'median' : raise NotImplementedError p = tf . nest . map_structure ( lambda x : np . quantile ( x , 0.5 , axis = 0 ) . astype ( np . float32 ), self . parameters ) m , v = interpolate ( self . locs , self . vals , self . cats , locs2 , cats2 , p , pair ) elif reduce == 'mean' : raise NotImplementedError p = tf . nest . map_structure ( lambda x : x . mean ( axis = 0 ) . astype ( np . float32 ), self . parameters ) m , v = interpolate ( self . locs , self . vals , self . cats , locs2 , cats2 , p , pair ) else : raise NotImplementedError samples = self . parameter_sample_size if subsample is not None : assert subsample <= samples , '`subsample` may not exceed sample size' else : subsample = samples # Thin by picking roughly equally-spaced samples. a = np . arange ( samples ) * subsample / samples % 1 pick = np . concatenate ([[ True ], a [ 1 :] >= a [: - 1 ]]) parameters = tf . nest . map_structure ( lambda x : x [ pick ], self . parameters ) # Make a prediction for each sample. results = [] for i in tracker ( range ( subsample )): p = tf . nest . map_structure ( lambda x : x [ i ], parameters ) results . append ( interpolate ( self . locs , self . vals , self . cats , locs2 , cats2 , p , pair )) mm , vv = [ np . stack ( x ) for x in zip ( * results )] m = mm . mean ( axis = 0 ) v = ( np . square ( mm ) + vv ) . mean ( axis = 0 ) - np . square ( m ) return m , v","title":"Model"},{"location":"api/#src.geostat.model.Model.__post_init__","text":"Parameters: Name Type Description Default x Pandas DataFrame with columns for locations. required u A Pandas Series containing observations. required featurization function, optional Should be a function that takes x1 (n-dim array of input data) and returns the coordinates, i.e., x, y, x 2, y 2. Example: def featurization(x1): return x1[:, 0], x1[:, 1], x1[:, 0] 2, x1[:, 1] 2. Default is None. required latent List[GP] Name of the covariance function to use in the GP. Should be 'squared-exp' or 'gamma-exp'. Default is 'squared-exp'. required verbose boolean, optional Whether or not to print parameters. Default is True. required Performs Gaussian process training and prediction. Source code in src/geostat/model.py 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 def __post_init__ ( self ): ''' Parameters: x : Pandas DataFrame with columns for locations. u : A Pandas Series containing observations. featurization : function, optional Should be a function that takes x1 (n-dim array of input data) and returns the coordinates, i.e., x, y, x**2, y**2. Example: def featurization(x1): return x1[:, 0], x1[:, 1], x1[:, 0]**2, x1[:, 1]**2. Default is None. latent : List[GP] Name of the covariance function to use in the GP. Should be 'squared-exp' or 'gamma-exp'. Default is 'squared-exp'. verbose : boolean, optional Whether or not to print parameters. Default is True. Performs Gaussian process training and prediction. ''' if self . warp is None : self . warp = NoWarp () # Default reporting function. def default_report ( p , prefix = None ): if prefix : print ( prefix , end = ' ' ) def fmt ( x ): if isinstance ( x , tf . Tensor ): x = x . numpy () if isinstance ( x , ( int , np . int32 , np . int64 )): return ' {:5d} ' . format ( x ) if isinstance ( x , ( float , np . float32 , np . float64 )): return ' {:5.2f} ' . format ( x ) else : with np . printoptions ( precision = 2 , formatter = { 'floatkind' : ' {:5.2f} ' . format }): return str ( x ) print ( '[ %s ]' % ( ' ' . join ( ' %s %s ' % ( k , fmt ( v )) for k , v in p . items ()))) if self . report == None : self . report = default_report if self . locs is not None : self . locs = np . array ( self . locs ) if self . vals is not None : self . vals = np . array ( self . vals ) if self . cats is not None : self . cats = np . array ( self . cats ) # Collect parameters and create TF parameters. for p in self . gather_vars () . values (): p . create_tf_variable ()","title":"__post_init__"},{"location":"api/#src.geostat.model.Model.predict","text":"Performs GP predictions of the mean and variance. Has support for batch predictions for large data sets. Source code in src/geostat/model.py 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 def predict ( self , locs2 , cats2 = None , * , subsample = None , reduce = None , tracker = None , pair = False ): ''' Performs GP predictions of the mean and variance. Has support for batch predictions for large data sets. ''' assert subsample is None or self . parameter_sample_size is not None , \\ '`subsample` is only valid with sampled parameters' assert reduce is None or self . parameter_sample_size is not None , \\ '`reduce` is only valid with sampled parameters' assert subsample is None or reduce is None , \\ '`subsample` and `reduce` cannot both be given' if tracker is None : tracker = lambda x : x assert self . locs . shape [ - 1 ] == locs2 . shape [ - 1 ], 'Mismatch in location dimensions' if cats2 is not None : assert cats2 . shape == locs2 . shape [: 1 ], 'Mismatched shapes in cats and locs' else : cats2 = np . zeros ( locs2 . shape [: 1 ], np . int32 ) def interpolate_batch ( A11i , locs1 , vals1diff , cats1 , locs2 , cats2 ): \"\"\" Inputs: locs1.shape = [N1, K] vals1diff.shape = [N1] cats1.shape = [N1] locs2.shape = [N2, K] cats2.shape = [N2] Outputs: u2_mean.shape = [N2] u2_var.shape = [N2] \"\"\" N1 = len ( locs1 ) # Number of measurements. # Permute datapoints if cats is given. if cats2 is not None : perm = np . argsort ( cats2 ) locs2 , cats2 = locs2 [ perm ], cats2 [ perm ] locs2 = self . warp ( locs2 ) . run ({}) _ , A12 = gp_covariance2 ( self . gp , tf . constant ( locs1 , dtype = tf . float32 ), tf . constant ( cats1 , dtype = tf . int32 ), tf . constant ( locs2 , dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 ), N1 ) m2 , A22 = gp_covariance ( self . gp , tf . constant ( locs2 , dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 )) # Restore order if things were permuted. if cats2 is not None : revperm = np . argsort ( perm ) m2 = tf . gather ( m2 , revperm ) A12 = tf . gather ( A12 , revperm , axis =- 1 ) A22 = tf . gather ( tf . gather ( A22 , revperm ), revperm , axis =- 1 ) u2_mean = m2 + tf . einsum ( 'ab,a->b' , A12 , tf . einsum ( 'ab,b->a' , A11i , vals1diff )) u2_var = tf . linalg . diag_part ( A22 ) - tf . einsum ( 'ab,ab->b' , A12 , tf . matmul ( A11i , A12 )) return u2_mean , u2_var def interpolate_pair_batch ( A11i , locs1 , vals1diff , cats1 , locs2 , cats2 ): \"\"\" Inputs: locs1.shape = [N1, K] vals1diff.shape = [N1] cats1.shape = [N1] locs2.shape = [N2, 2, K] cats2.shape = [N2] Outputs: u2_mean.shape = [N2, 2] u2_var.shape = [N2, 2, 2] \"\"\" N1 = len ( locs1 ) # Number of measurements. N2 = len ( locs2 ) # Number of prediction pairs. # Permute datapoints if cats is given. perm = np . argsort ( cats2 ) locs2 , cats2 = locs2 [ perm ], cats2 [ perm ] # Warp locs2. locs2_shape = locs2 . shape locs2 = locs2 . reshape ([ - 1 , locs2_shape [ - 1 ]]) # Shape into matrix. locs2 = self . warp ( locs2 ) . run ({}) locs2 = tf . reshape ( locs2 , locs2_shape ) # Revert shape. _ , A12 = gp_covariance2 ( self . gp , tf . constant ( locs1 , dtype = tf . float32 ), tf . constant ( cats1 , dtype = tf . int32 ), tf . constant ( locs2 [:, 0 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 ), N1 ) _ , A13 = gp_covariance2 ( self . gp , tf . constant ( locs1 , dtype = tf . float32 ), tf . constant ( cats1 , dtype = tf . int32 ), tf . constant ( locs2 [:, 1 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 ), N1 ) m2 , A22 = gp_covariance ( self . gp , tf . constant ( locs2 [:, 0 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 )) m3 , A33 = gp_covariance ( self . gp , tf . constant ( locs2 [:, 1 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 )) _ , A23 = gp_covariance2 ( self . gp , tf . constant ( locs2 [:, 0 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 ), tf . constant ( locs2 [:, 1 , :], dtype = tf . float32 ), tf . constant ( cats2 , dtype = tf . int32 ), N2 ) # Reassemble into more useful shapes. A12 = tf . stack ([ A12 , A13 ], axis =- 1 ) # [N1, N2, 2] m2 = tf . stack ([ m2 , m3 ], axis =- 1 ) # [N2, 2] A22 = tf . linalg . diag_part ( A22 ) A33 = tf . linalg . diag_part ( A33 ) A23 = tf . linalg . diag_part ( A23 ) A22 = tf . stack ([ tf . stack ([ A22 , A23 ], axis =- 1 ), tf . stack ([ A23 , A33 ], axis =- 1 )], axis =- 2 ) # [N2, 2, 2] # Restore order if things were permuted. revperm = np . argsort ( perm ) m2 = tf . gather ( m2 , revperm ) A12 = tf . gather ( A12 , revperm , axis = 1 ) A22 = tf . gather ( A22 , revperm ) u2_mean = m2 + tf . einsum ( 'abc,a->bc' , A12 , tf . einsum ( 'ab,b->a' , A11i , vals1diff )) u2_var = A22 - tf . einsum ( 'abc,abd->bcd' , A12 , tf . einsum ( 'ae,ebd->abd' , A11i , A12 )) return u2_mean , u2_var def interpolate ( locs1 , vals1 , cats1 , locs2 , cats2 , pair = False ): # Interpolate in batches. batch_size = locs1 . shape [ 0 ] // 2 for_gp = [] for start in np . arange ( 0 , len ( locs2 ), batch_size ): stop = start + batch_size subset = locs2 [ start : stop ], cats2 [ start : stop ] for_gp . append ( subset ) # Permute datapoints if cats is given. if cats1 is not None : perm = np . argsort ( cats1 ) locs1 , vals1 , cats1 = locs1 [ perm ], vals1 [ perm ], cats1 [ perm ] locs1 = self . warp ( locs1 ) . run ({}) m1 , A11 = gp_covariance ( self . gp , tf . constant ( locs1 , dtype = tf . float32 ), tf . constant ( cats1 , dtype = tf . int32 )) A11i = tf . linalg . inv ( A11 ) u2_mean_s = [] u2_var_s = [] f = interpolate_pair_batch if pair else interpolate_batch for locs_subset , cats_subset in for_gp : u2_mean , u2_var = f ( A11i , locs1 , vals1 - m1 , cats1 , locs_subset , cats_subset ) u2_mean = u2_mean . numpy () u2_var = u2_var . numpy () u2_mean_s . append ( u2_mean ) u2_var_s . append ( u2_var ) u2_mean = np . concatenate ( u2_mean_s ) u2_var = np . concatenate ( u2_var_s ) return u2_mean , u2_var if self . parameter_sample_size is None : m , v = interpolate ( self . locs , self . vals , self . cats , locs2 , cats2 , pair ) elif reduce == 'median' : raise NotImplementedError p = tf . nest . map_structure ( lambda x : np . quantile ( x , 0.5 , axis = 0 ) . astype ( np . float32 ), self . parameters ) m , v = interpolate ( self . locs , self . vals , self . cats , locs2 , cats2 , p , pair ) elif reduce == 'mean' : raise NotImplementedError p = tf . nest . map_structure ( lambda x : x . mean ( axis = 0 ) . astype ( np . float32 ), self . parameters ) m , v = interpolate ( self . locs , self . vals , self . cats , locs2 , cats2 , p , pair ) else : raise NotImplementedError samples = self . parameter_sample_size if subsample is not None : assert subsample <= samples , '`subsample` may not exceed sample size' else : subsample = samples # Thin by picking roughly equally-spaced samples. a = np . arange ( samples ) * subsample / samples % 1 pick = np . concatenate ([[ True ], a [ 1 :] >= a [: - 1 ]]) parameters = tf . nest . map_structure ( lambda x : x [ pick ], self . parameters ) # Make a prediction for each sample. results = [] for i in tracker ( range ( subsample )): p = tf . nest . map_structure ( lambda x : x [ i ], parameters ) results . append ( interpolate ( self . locs , self . vals , self . cats , locs2 , cats2 , p , pair )) mm , vv = [ np . stack ( x ) for x in zip ( * results )] m = mm . mean ( axis = 0 ) v = ( np . square ( mm ) + vv ) . mean ( axis = 0 ) - np . square ( m ) return m , v","title":"predict"},{"location":"api/#src.geostat.model.NormalizingFeaturizer","text":"Produces featurized locations (F matrix) and remembers normalization parameters. Adds an intercept feature. Source code in src/geostat/model.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 class NormalizingFeaturizer : \"\"\" Produces featurized locations (F matrix) and remembers normalization parameters. Adds an intercept feature. \"\"\" def __init__ ( self , featurization , locs ): self . unnorm_featurizer = Featurizer ( featurization ) F_unnorm = self . unnorm_featurizer ( locs ) self . unnorm_mean = tf . reduce_mean ( F_unnorm , axis = 0 ) self . unnorm_std = tf . math . reduce_std ( F_unnorm , axis = 0 ) def __call__ ( self , locs ): ones = tf . ones ([ tf . shape ( locs )[ 0 ], 1 ], dtype = tf . float32 ) F_unnorm = self . unnorm_featurizer ( locs ) F_norm = ( F_unnorm - self . unnorm_mean ) / self . unnorm_std return tf . concat ([ ones , F_norm ], axis = 1 )","title":"NormalizingFeaturizer"},{"location":"api/#src.geostat.model.Warp","text":"Source code in src/geostat/model.py 55 56 57 58 59 60 61 62 63 64 65 class Warp : def __call__ ( self , locs , prep ): \"\"\" `locs` is numpy. Returns a WarpLocations. \"\"\" pass def gather_vars ( self ): pass","title":"Warp"},{"location":"api/#src.geostat.model.Warp.__call__","text":"locs is numpy. Returns a WarpLocations. Source code in src/geostat/model.py 56 57 58 59 60 61 62 def __call__ ( self , locs , prep ): \"\"\" `locs` is numpy. Returns a WarpLocations. \"\"\" pass","title":"__call__"},{"location":"api/#src.geostat.model.gp_covariance2","text":"offset is i2-i1, where i1 and i2 are the starting indices of locs1 and locs2. It is used to create the diagonal non-zero elements of a Noise covariance function. An non-zero offset results in a covariance matrix with non-zero entries along an off-center diagonal. Source code in src/geostat/model.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 @tf . function def gp_covariance2 ( gp , locs1 , cats1 , locs2 , cats2 , offset ): \"\"\" `offset` is i2-i1, where i1 and i2 are the starting indices of locs1 and locs2. It is used to create the diagonal non-zero elements of a Noise covariance function. An non-zero offset results in a covariance matrix with non-zero entries along an off-center diagonal. \"\"\" # assert np.all(cats1 == np.sort(cats1)), '`cats1` must be in non-descending order' # assert np.all(cats2 == np.sort(cats2)), '`cats2` must be in non-descending order' cache = {} cache [ 'offset' ] = offset cache [ 'locs1' ] = locs1 cache [ 'locs2' ] = locs2 cache [ 'cats1' ] = cats1 cache [ 'cats2' ] = cats2 cache [ 'per_axis_dist2' ] = PerAxisDist2 () . run ( cache ) cache [ 'euclidean' ] = Euclidean () . run ( cache ) M = gp . mean . run ( cache ) C = gp . kernel . run ( cache ) M = tf . cast ( M , tf . float64 ) C = tf . cast ( C , tf . float64 ) return M , C","title":"gp_covariance2"},{"location":"api/#src.geostat.model.interpolate_1d_tf","text":"src : (batch, breaks) tgt : (batch, breaks) x : (batch) Source code in src/geostat/model.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 @tf . function def interpolate_1d_tf ( src , tgt , x ): \"\"\" `src`: (batch, breaks) `tgt`: (batch, breaks) `x` : (batch) \"\"\" x_shape = tf . shape ( x ) x = tf . reshape ( x , [ - 1 , 1 ]) # (batch, 1) bucket = tf . searchsorted ( src , x ) bucket = tf . clip_by_value ( bucket - 1 , 0 , tf . shape ( tgt )[ 0 ] - 2 ) src0 = tf . gather ( src , bucket , batch_dims = 1 ) src1 = tf . gather ( src , bucket + 1 , batch_dims = 1 ) tgt0 = tf . gather ( tgt , bucket , batch_dims = 1 ) tgt1 = tf . gather ( tgt , bucket + 1 , batch_dims = 1 ) xout = (( x - src0 ) * tgt1 + ( src1 - x ) * tgt0 ) / ( src1 - src0 ) return tf . reshape ( xout , x_shape )","title":"interpolate_1d_tf"},{"location":"api/#src.geostat.model.mvn_log_pdf","text":"Log PDF of a multivariate gaussian. Source code in src/geostat/model.py 309 310 311 312 313 314 def mvn_log_pdf ( u , m , cov ): \"\"\"Log PDF of a multivariate gaussian.\"\"\" u_adj = u - m logdet = tf . linalg . logdet ( 2 * np . pi * cov ) quad = tf . matmul ( e ( u_adj , 0 ), tf . linalg . solve ( cov , e ( u_adj , - 1 )))[ 0 , 0 ] return tf . cast ( - 0.5 * ( logdet + quad ), tf . float32 )","title":"mvn_log_pdf"},{"location":"api/#src.geostat.op","text":"","title":"op"},{"location":"api/#src.geostat.op.Op","text":"The autoinputs parameter contains a blob of upstream ops. The leaves in the blob are either the op itself or a string identifier. In the latter case, the string identifier should be present as a key in the cache that gets passed in. This parameter links ops together in a DAG. We walk the DAG for various reasons. run calls the op and puts the output in self.out , after recursively doing this for autoinputs. gather_vars recursively gathers variables from self and autoinputs. Source code in src/geostat/op.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @dataclass class Op : \"\"\" The `autoinputs` parameter contains a blob of upstream ops. The leaves in the blob are either the op itself or a string identifier. In the latter case, the string identifier should be present as a key in the `cache` that gets passed in. This parameter links ops together in a DAG. We walk the DAG for various reasons. - `run` calls the op and puts the output in `self.out`, after recursively doing this for autoinputs. - `gather_vars` recursively gathers variables from self and autoinputs. \"\"\" fa : Dict [ str , object ] # Formal arguments. autoinputs : object # Blob of Ops and strings. def vars ( self ): # Parameters return [] def gather_vars ( self , cache = None ): \"\"\" `cache` maps from Op ids to sets of variable names. Returns a dict of parameters, keyed by name. \"\"\" if cache is None : cache = {} if id ( self ) not in cache : vv = { k : v for op in tf . nest . flatten ( self . autoinputs ) if isinstance ( op , Op ) for k , v in op . gather_vars ( cache ) . items ()} # print(self, '<-', [x.name for x in vv], '|', [x.name for x in set(self.vars())], '\\n') cache [ id ( self )] = vv | self . vars () return cache [ id ( self )] def __call__ ( self , e ): \"\"\" `e` is a dict of params and evaluated inputs from upstream ops. Other values in `e` are supplied by the caller. \"\"\" pass def run ( self , cache ): \"\"\" If op has already been run, return result. Else: - Assemble inputs by recursively calling upstream ops. - Execute op by calling `__call__`. - Store result in cache. \"\"\" def eval ( op ): \"\"\" Evaluate `op`. If `op` is a string, look up its value. Otherwise execute it. \"\"\" if isinstance ( op , str ): return cache [ op ] else : return op . run ( cache ) if id ( self ) not in cache : e = tf . nest . map_structure ( lambda op : eval ( op ), self . autoinputs ) e |= get_parameter_values ( self . fa ) cache [ id ( self )] = self ( e ) # Save the Op so that its ID remains unique. if '__save__' not in cache : cache [ '__save__' ] = [] cache [ '__save__' ] . append ( self ) return cache [ id ( self )] def __tf_tracing_type__ ( self , context ): return SingletonTraceType ( self )","title":"Op"},{"location":"api/#src.geostat.op.Op.__call__","text":"e is a dict of params and evaluated inputs from upstream ops. Other values in e are supplied by the caller. Source code in src/geostat/op.py 50 51 52 53 54 55 def __call__ ( self , e ): \"\"\" `e` is a dict of params and evaluated inputs from upstream ops. Other values in `e` are supplied by the caller. \"\"\" pass","title":"__call__"},{"location":"api/#src.geostat.op.Op.gather_vars","text":"cache maps from Op ids to sets of variable names. Returns a dict of parameters, keyed by name. Source code in src/geostat/op.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def gather_vars ( self , cache = None ): \"\"\" `cache` maps from Op ids to sets of variable names. Returns a dict of parameters, keyed by name. \"\"\" if cache is None : cache = {} if id ( self ) not in cache : vv = { k : v for op in tf . nest . flatten ( self . autoinputs ) if isinstance ( op , Op ) for k , v in op . gather_vars ( cache ) . items ()} # print(self, '<-', [x.name for x in vv], '|', [x.name for x in set(self.vars())], '\\n') cache [ id ( self )] = vv | self . vars () return cache [ id ( self )]","title":"gather_vars"},{"location":"api/#src.geostat.op.Op.run","text":"If op has already been run, return result. Else: - Assemble inputs by recursively calling upstream ops. - Execute op by calling __call__ . - Store result in cache. Source code in src/geostat/op.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def run ( self , cache ): \"\"\" If op has already been run, return result. Else: - Assemble inputs by recursively calling upstream ops. - Execute op by calling `__call__`. - Store result in cache. \"\"\" def eval ( op ): \"\"\" Evaluate `op`. If `op` is a string, look up its value. Otherwise execute it. \"\"\" if isinstance ( op , str ): return cache [ op ] else : return op . run ( cache ) if id ( self ) not in cache : e = tf . nest . map_structure ( lambda op : eval ( op ), self . autoinputs ) e |= get_parameter_values ( self . fa ) cache [ id ( self )] = self ( e ) # Save the Op so that its ID remains unique. if '__save__' not in cache : cache [ '__save__' ] = [] cache [ '__save__' ] . append ( self ) return cache [ id ( self )]","title":"run"},{"location":"api/#src.geostat.op.SingletonTraceType","text":"Bases: TraceType A trace type to override TF's default behavior, which is to treat dataclass-based onjects as dicts. Source code in src/geostat/op.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 class SingletonTraceType ( TraceType ): \"\"\" A trace type to override TF's default behavior, which is to treat dataclass-based onjects as dicts. \"\"\" def __init__ ( self , thing ): self . value = thing def is_subtype_of ( self , other ): return self . value is other . value def most_specific_common_supertype ( self , other ): if self . value is other . value : return self . value else : return None def placeholder_value ( self , placeholder_context ): return self . value def __eq__ ( self , other ): return self . value is other . value def __hash__ ( self ): return hash ( id ( self . value ))","title":"SingletonTraceType"},{"location":"api/#src.geostat.param","text":"","title":"param"},{"location":"api/#src.geostat.param.Parameter","text":"Source code in src/geostat/param.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 @dataclass class Parameter : name : str value : float lo : float = np . nan hi : float = np . nan underlying : tf . Variable = None def update_bounds ( self , lo : float , hi : float ): if np . isnan ( self . lo ): self . lo = lo else : assert self . lo == lo , f 'Conflicting bounds for parameter { self . name } ' if np . isnan ( self . hi ): self . hi = hi else : assert self . hi == hi , f 'Conflicting bounds for parameter { self . name } ' def bounding ( self ): return \\ ( 'u' if self . lo == float ( '-inf' ) else 'b' ) + \\ ( 'u' if self . hi == float ( 'inf' ) else 'b' ) def create_tf_variable ( self ): \"\"\"Create TF variable for underlying parameter or update it\"\"\" # Create underlying parameter. b = self . bounding () if b == 'bb' : init = logit (( self . value - self . lo ) / ( self . hi - self . lo )) elif b == 'bu' : init = np . log ( self . value - self . lo ) elif b == 'ub' : init = - np . log ( self . hi - self . value ) else : init = self . value if self . underlying is None : self . underlying = tf . Variable ( init , name = self . name , dtype = tf . float32 ) else : self . underlying . assign ( init ) def surface ( self ): \"\"\" Create tensor for surface parameter\"\"\" # Create surface parameter. b = self . bounding () v = self . underlying if b == 'bb' : v = tf . math . sigmoid ( v ) * ( self . hi - self . lo ) + self . lo elif b == 'bu' : v = tf . exp ( v ) + self . lo elif b == 'ub' : v = self . hi - tf . exp ( - v ) else : v = v + tf . constant ( 0. ) return v def update_value ( self ): self . value = self . surface () . numpy ()","title":"Parameter"},{"location":"api/#src.geostat.param.Parameter.create_tf_variable","text":"Create TF variable for underlying parameter or update it Source code in src/geostat/param.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def create_tf_variable ( self ): \"\"\"Create TF variable for underlying parameter or update it\"\"\" # Create underlying parameter. b = self . bounding () if b == 'bb' : init = logit (( self . value - self . lo ) / ( self . hi - self . lo )) elif b == 'bu' : init = np . log ( self . value - self . lo ) elif b == 'ub' : init = - np . log ( self . hi - self . value ) else : init = self . value if self . underlying is None : self . underlying = tf . Variable ( init , name = self . name , dtype = tf . float32 ) else : self . underlying . assign ( init )","title":"create_tf_variable"},{"location":"api/#src.geostat.param.Parameter.surface","text":"Create tensor for surface parameter Source code in src/geostat/param.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def surface ( self ): \"\"\" Create tensor for surface parameter\"\"\" # Create surface parameter. b = self . bounding () v = self . underlying if b == 'bb' : v = tf . math . sigmoid ( v ) * ( self . hi - self . lo ) + self . lo elif b == 'bu' : v = tf . exp ( v ) + self . lo elif b == 'ub' : v = self . hi - tf . exp ( - v ) else : v = v + tf . constant ( 0. ) return v","title":"surface"},{"location":"api/#src.geostat.param.bpp","text":"Bounded paper parameter (maybe). Source code in src/geostat/param.py 107 108 109 110 111 112 113 def bpp ( param , lo , hi ): \"\"\"Bounded paper parameter (maybe).\"\"\" if isinstance ( param , Parameter ): param . update_bounds ( lo , hi ) return { param . name : param } else : return {}","title":"bpp"},{"location":"api/#src.geostat.param.get_parameter_values","text":"For each Parameter encountered in the nested blob, replace it with its surface tensor. Source code in src/geostat/param.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def get_parameter_values ( blob : object ): \"\"\" For each Parameter encountered in the nested blob, replace it with its surface tensor. \"\"\" if isinstance ( blob , dict ): return { k : get_parameter_values ( a ) for k , a in blob . items ()} elif isinstance ( blob , ( list , tuple )): return [ get_parameter_values ( a ) for a in blob ] elif isinstance ( blob , Parameter ): return blob . surface () elif isinstance ( blob , str ): raise ValueError ( f 'Bad parameter { blob } is a string' ) else : return blob","title":"get_parameter_values"},{"location":"api/#src.geostat.param.ppp","text":"Positive paper parameter (maybe). Source code in src/geostat/param.py 91 92 93 94 95 96 97 def ppp ( param ): \"\"\"Positive paper parameter (maybe).\"\"\" if isinstance ( param , Parameter ): param . update_bounds ( 0. , np . inf ) return { param . name : param } else : return {}","title":"ppp"},{"location":"api/#src.geostat.param.upp","text":"Unbounded paper parameter (maybe). Source code in src/geostat/param.py 99 100 101 102 103 104 105 def upp ( param ): \"\"\"Unbounded paper parameter (maybe).\"\"\" if isinstance ( param , Parameter ): param . update_bounds ( - np . inf , np . inf ) return { param . name : param } else : return {}","title":"upp"},{"location":"examples/","text":"Examples An introduction to Geostat In Geostat, we create one model that is used to create synthetic data according to provided parameters, and we create a second model that does the inverse: it takes the data and infers the parameters. Structured covariance functions Here we show how a progressively more complex covariance function fits data better than simpler ones. Making predictions in a shape Geostat has utility functions to make it easier to work with shapes. Gaussian processes in Tensorflow A tutorial on how to implement Gaussian processes in Tensorflow.","title":"Examples"},{"location":"examples/#examples","text":"","title":"Examples"},{"location":"examples/#an-introduction-to-geostat","text":"In Geostat, we create one model that is used to create synthetic data according to provided parameters, and we create a second model that does the inverse: it takes the data and infers the parameters.","title":"An introduction to Geostat"},{"location":"examples/#structured-covariance-functions","text":"Here we show how a progressively more complex covariance function fits data better than simpler ones.","title":"Structured covariance functions"},{"location":"examples/#making-predictions-in-a-shape","text":"Geostat has utility functions to make it easier to work with shapes.","title":"Making predictions in a shape"},{"location":"examples/#gaussian-processes-in-tensorflow","text":"A tutorial on how to implement Gaussian processes in Tensorflow.","title":"Gaussian processes in Tensorflow"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/","text":"from geostat import GP, Model, Mesh, Parameters import geostat import geostat.kernel as krn import matplotlib.pyplot as pp import numpy as np import tensorflow as tf Overview In this notebook we will: * Use a Gaussian process with a complex stacked covariance function to generate synthetic data. * Use Gaussian processes with covariance functions of increasing complexity to infer the geospatial parameters from the synthetic data. Each time, log likelihood improves and the nugget decreases. A smaller nugget means that predictions are more confident. Synthesizing data We will synthesize data at random locations in 3D space near the origin. np.random.seed(111) locs = np.random.normal(size=[500, 3]) * [1., 1., 0.333] There will be a cubic depth trend, but no horizontal trends. The decorator converts the function trend_featurizer into a featurizer that Geostat can use. The normalize argument causes each feature to have zero mean and unit variance after being applied to the given locations. It also adds a constant one feature. @geostat.featurizer(normalize=locs.reshape([-1, 3])) def trend_featurizer(x, y, z): return z, z*z, z*z*z Model parameters are specified here, along with their values. The return value p is a namespace. p = Parameters(alpha=0.1, zs=10., r=0.33, s1=1., s2=0.5, g1=1., g2=0.5, nugget=0.25) The covariance function will include a trend based on the featurizer, and will combine two gamma-exponentials: one that respects depth with z-anisotropy, and one that ignores depth altogether. We will set the range for both to be the same parameter to show that it is possible to tie parameters together. In TrendPrior , alpha parameterizes the normal distribution prior for trend coefficients. kernel = \\ krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\ krn.GammaExponential(range=p.r, sill=p.s1, gamma=p.g1, scale=[1., 1., p.zs]) + \\ krn.GammaExponential(range=p.r, sill=p.s2, gamma=p.g2, scale=[1., 1., 0.]) + \\ krn.Noise(p.nugget) Define a Gaussian process with zero mean and a covariance function given by kernel . gp = GP(0, kernel) Instantiate a Model and immediately call generate to generate synthetic observations. tf.random.set_seed(113) obs = Model(gp).generate(locs).vals When the data is plotted, you can see an overall trend with some localized variations. fig, axs = pp.subplots(3, figsize=(7, 6), dpi=120, sharex=True, sharey=True) vmin, vmax = obs.min(), obs.max() pane = np.round((locs[:, 1] + 2) / 2).astype(int) for i, ymid in enumerate(np.linspace(-2, 2, 3)): ymin, ymax = ymid - 1, ymid + 1 c = axs[i].scatter(locs[pane == i, 0], locs[pane == i, 2], c=obs[pane == i], vmin=vmin, vmax=vmax) axs[i].set_title('y = %0.1f' % ymid) axs[i].set_aspect(0.9) axs[2].set_xlabel('x-axis') axs[1].set_ylabel('z-axis') fig.subplots_adjust(right=0.9) cbar_ax = fig.add_axes([0.88, 0.1, 0.02, 0.8]) fig.colorbar(c, cax=cbar_ax) fig.suptitle('Synthetic data, projected to nearest cross section') pp.show() Before we continue, let's define a function that takes a model and plots predictions for the three slices shown above. def plot(model): fig, axs = pp.subplots(3, figsize=(7, 6), dpi=120, sharex=True, sharey=True) for i, ymid in enumerate(np.linspace(-2, 2, 3)): mesh = Mesh.from_bounds([-3, -1, 3, 1], nx=200) mesh_locs = mesh.locations(proj=[[1, 0, 0], [0, 0, 1], [0, ymid, 0]]) # [x, z, 1] -> [x, y, z]. mean, var = model.predict(mesh_locs) meshx, meshy, out = mesh.slice(mean) c = axs[i].pcolormesh(meshx, meshy, out, vmin=vmin, vmax=vmax) axs[i].set_title('y = %0.1f' % ymid) axs[i].set_aspect(0.9) axs[2].set_xlabel('x-axis') axs[1].set_ylabel('z-axis') fig.subplots_adjust(right=0.9) cbar_ax = fig.add_axes([0.88, 0.1, 0.02, 0.8]) fig.colorbar(c, cax=cbar_ax) fig.suptitle('Predictions for 3 cross sections') pp.show() Model 1: Bayesian regression First let's try modeling the data with a Bayesian regression, which is a model with just trends and uncorrelated noise. p = Parameters(alpha=1.0, nugget=0.5) kernel = krn.TrendPrior(trend_featurizer, alpha=p.alpha) + krn.Noise(nugget=p.nugget) model1 = Model(GP(0, kernel)).fit(locs, obs, iters=500) [iter 50 ll -910.60 time 1.73 reg 0.00 alpha 0.61 nugget 0.79] [iter 100 ll -853.59 time 0.69 reg 0.00 alpha 0.37 nugget 1.06] [iter 150 ll -836.20 time 0.69 reg 0.00 alpha 0.23 nugget 1.28] [iter 200 ll -830.86 time 0.68 reg 0.00 alpha 0.15 nugget 1.42] [iter 250 ll -829.16 time 0.69 reg 0.00 alpha 0.10 nugget 1.50] [iter 300 ll -828.57 time 0.68 reg 0.00 alpha 0.07 nugget 1.54] [iter 350 ll -828.35 time 0.69 reg 0.00 alpha 0.06 nugget 1.56] [iter 400 ll -828.27 time 0.68 reg 0.00 alpha 0.05 nugget 1.57] [iter 450 ll -828.25 time 0.69 reg 0.00 alpha 0.04 nugget 1.58] [iter 500 ll -828.24 time 0.68 reg 0.00 alpha 0.04 nugget 1.58] And here are predictions for the three slices shown above. plot(model1) Model 2: GP with isotropic sq-exp covariance function Now let's layer on an isotropic squared exponential covariance function to the above model. p = Parameters(alpha=1.0, range=1.0, sill=0.5, nugget=0.5) kernel = \\ krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\ krn.SquaredExponential(range=p.range, sill=p.sill) + \\ krn.Noise(nugget=p.nugget) model2 = Model(GP(0, kernel)).fit(locs, obs, iters=500) [iter 50 ll -864.77 time 1.41 reg 0.00 alpha 0.61 sill 0.75 range 0.62 nugget 0.77] [iter 100 ll -829.58 time 0.80 reg 0.00 alpha 0.37 sill 0.53 range 0.46 nugget 0.99] [iter 150 ll -820.58 time 0.80 reg 0.00 alpha 0.23 sill 0.38 range 0.40 nugget 1.13] [iter 200 ll -817.97 time 0.80 reg 0.00 alpha 0.15 sill 0.33 range 0.38 nugget 1.22] [iter 250 ll -816.98 time 0.80 reg 0.00 alpha 0.10 sill 0.31 range 0.38 nugget 1.27] [iter 300 ll -816.48 time 0.80 reg 0.00 alpha 0.07 sill 0.30 range 0.38 nugget 1.29] [iter 350 ll -816.18 time 0.80 reg 0.00 alpha 0.05 sill 0.29 range 0.39 nugget 1.31] [iter 400 ll -816.00 time 0.80 reg 0.00 alpha 0.04 sill 0.28 range 0.40 nugget 1.32] [iter 450 ll -815.90 time 0.80 reg 0.00 alpha 0.04 sill 0.27 range 0.42 nugget 1.33] [iter 500 ll -815.84 time 0.80 reg 0.00 alpha 0.03 sill 0.27 range 0.43 nugget 1.33] The log-likelihood is improved as a result of a more complex model. Nugget is much lower. Predictions: plot(model2) Model 3: GP with anisotropic sq-exp covariance function Now we switch from isotropic to anisotropic for the covariance function. p = Parameters(alpha=1.0, range=1.0, sill=0.5, nugget=0.5, zs=5.0) kernel = \\ krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\ krn.SquaredExponential(range=p.range, sill=p.sill, scale=[1., 1., p.zs]) + \\ krn.Noise(nugget=p.nugget) model3 = Model(GP(0, kernel)).fit(locs, obs, iters=500) [iter 50 ll -841.04 time 1.50 reg 0.00 alpha 0.61 zs 7.43 sill 0.75 range 0.65 nugget 0.76] [iter 100 ll -821.03 time 0.86 reg 0.00 alpha 0.37 zs 8.75 sill 0.74 range 0.50 nugget 0.90] [iter 150 ll -817.17 time 0.86 reg 0.00 alpha 0.23 zs 8.73 sill 0.66 range 0.43 nugget 0.95] [iter 200 ll -815.63 time 0.86 reg 0.00 alpha 0.15 zs 8.13 sill 0.63 range 0.39 nugget 0.97] [iter 250 ll -814.62 time 0.86 reg 0.00 alpha 0.10 zs 7.37 sill 0.63 range 0.37 nugget 0.98] [iter 300 ll -813.89 time 0.87 reg 0.00 alpha 0.07 zs 6.66 sill 0.63 range 0.36 nugget 0.97] [iter 350 ll -813.41 time 0.87 reg 0.00 alpha 0.05 zs 6.08 sill 0.64 range 0.35 nugget 0.97] [iter 400 ll -813.12 time 0.87 reg 0.00 alpha 0.04 zs 5.64 sill 0.65 range 0.34 nugget 0.96] [iter 450 ll -812.97 time 0.87 reg 0.00 alpha 0.04 zs 5.32 sill 0.66 range 0.33 nugget 0.95] [iter 500 ll -812.89 time 0.87 reg 0.00 alpha 0.04 zs 5.11 sill 0.67 range 0.32 nugget 0.94] Log-likelihood and nugget both improve further. Predictions: plot(model3) Model 4: GP with anisotropic gamma-exp covariance function Now we switch from a squared-exponential to a gamma-exponential covariance function, which has an extra shape parameter. p = Parameters(alpha=1.0, range=1.0, sill=0.5, nugget=0.5, zs=5.0, gamma=1.0) kernel = \\ krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\ krn.GammaExponential(range=p.range, sill=p.sill, gamma=p.gamma, scale=[1., 1., p.zs]) + \\ krn.Noise(nugget=p.nugget) model4 = Model(GP(0, kernel)).fit(locs, obs, iters=500) [iter 50 ll -819.50 time 1.83 reg 0.00 alpha 0.61 zs 7.16 sill 0.76 range 0.67 gamma 0.84 nugget 0.74] [iter 100 ll -815.43 time 0.94 reg 0.00 alpha 0.37 zs 6.94 sill 0.86 range 0.58 gamma 0.81 nugget 0.82] [iter 150 ll -814.43 time 0.94 reg 0.00 alpha 0.23 zs 6.19 sill 0.87 range 0.56 gamma 0.81 nugget 0.82] [iter 200 ll -813.66 time 0.94 reg 0.00 alpha 0.15 zs 5.77 sill 0.87 range 0.53 gamma 0.80 nugget 0.82] [iter 250 ll -813.07 time 0.94 reg 0.00 alpha 0.10 zs 5.56 sill 0.86 range 0.50 gamma 0.80 nugget 0.82] [iter 300 ll -812.64 time 0.94 reg 0.00 alpha 0.07 zs 5.45 sill 0.86 range 0.48 gamma 0.81 nugget 0.81] [iter 350 ll -812.36 time 0.95 reg 0.00 alpha 0.05 zs 5.39 sill 0.86 range 0.46 gamma 0.82 nugget 0.80] [iter 400 ll -812.19 time 0.95 reg 0.00 alpha 0.04 zs 5.36 sill 0.86 range 0.45 gamma 0.83 nugget 0.79] [iter 450 ll -812.10 time 0.95 reg 0.00 alpha 0.04 zs 5.33 sill 0.86 range 0.43 gamma 0.84 nugget 0.78] [iter 500 ll -812.06 time 0.95 reg 0.00 alpha 0.03 zs 5.31 sill 0.87 range 0.43 gamma 0.85 nugget 0.77] The log-likelihood has improved very slightly but nugget improves more significantly, since the pointy peak in the gamma-exponential does some of the work of a nugget. Predictions: plot(model4) Model 5: GP with stacked covariance functions Finally we switch to using the same covariance function used to generate the synthetic data. p = Parameters(alpha=1., zs=5., r=0.5, s1=2., s2=1., g1=1., g2=1., nugget=1.) kernel = \\ krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\ krn.GammaExponential(range=p.r, sill=p.s1, gamma=p.g1, scale=[1., 1., p.zs]) + \\ krn.GammaExponential(range=p.r, sill=p.s2, gamma=p.g2, scale=[1., 1., 0.]) + \\ krn.Noise(nugget=p.nugget) model5 = Model(GP(0, kernel)).fit(locs, obs, iters=500) [iter 50 ll -824.02 time 2.05 reg 0.00 alpha 0.61 zs 4.04 s1 1.32 r 0.75 g1 0.85 s2 0.65 g2 1.16 nugget 0.67] [iter 100 ll -816.07 time 1.10 reg 0.00 alpha 0.38 zs 6.56 s1 1.11 r 0.81 g1 0.57 s2 0.50 g2 1.00 nugget 0.62] [iter 150 ll -813.38 time 1.10 reg 0.00 alpha 0.24 zs 8.73 s1 1.02 r 0.79 g1 0.45 s2 0.43 g2 0.79 nugget 0.60] [iter 200 ll -812.33 time 1.10 reg 0.00 alpha 0.16 zs 9.27 s1 0.97 r 0.74 g1 0.41 s2 0.39 g2 0.63 nugget 0.59] [iter 250 ll -811.74 time 1.11 reg 0.00 alpha 0.11 zs 9.15 s1 0.94 r 0.67 g1 0.42 s2 0.38 g2 0.57 nugget 0.59] [iter 300 ll -811.27 time 1.11 reg 0.00 alpha 0.08 zs 9.08 s1 0.91 r 0.60 g1 0.45 s2 0.38 g2 0.54 nugget 0.58] [iter 350 ll -810.74 time 1.11 reg 0.00 alpha 0.06 zs 9.15 s1 0.88 r 0.52 g1 0.52 s2 0.37 g2 0.54 nugget 0.57] [iter 400 ll -809.82 time 1.10 reg 0.00 alpha 0.05 zs 9.39 s1 0.82 r 0.43 g1 0.66 s2 0.37 g2 0.54 nugget 0.56] [iter 450 ll -808.27 time 1.11 reg 0.00 alpha 0.04 zs 9.57 s1 0.76 r 0.36 g1 0.93 s2 0.36 g2 0.52 nugget 0.54] [iter 500 ll -807.77 time 1.11 reg 0.00 alpha 0.04 zs 8.73 s1 0.73 r 0.35 g1 1.11 s2 0.37 g2 0.49 nugget 0.53] Not surprisingly, log likelihood and nugget both improve further. You can see faint vertical stripes that correspond the the \"depth-invariant\" component of the covariance function. plot(model5)","title":"3d gaussian processes"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/#overview","text":"In this notebook we will: * Use a Gaussian process with a complex stacked covariance function to generate synthetic data. * Use Gaussian processes with covariance functions of increasing complexity to infer the geospatial parameters from the synthetic data. Each time, log likelihood improves and the nugget decreases. A smaller nugget means that predictions are more confident.","title":"Overview"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/#synthesizing-data","text":"We will synthesize data at random locations in 3D space near the origin. np.random.seed(111) locs = np.random.normal(size=[500, 3]) * [1., 1., 0.333] There will be a cubic depth trend, but no horizontal trends. The decorator converts the function trend_featurizer into a featurizer that Geostat can use. The normalize argument causes each feature to have zero mean and unit variance after being applied to the given locations. It also adds a constant one feature. @geostat.featurizer(normalize=locs.reshape([-1, 3])) def trend_featurizer(x, y, z): return z, z*z, z*z*z Model parameters are specified here, along with their values. The return value p is a namespace. p = Parameters(alpha=0.1, zs=10., r=0.33, s1=1., s2=0.5, g1=1., g2=0.5, nugget=0.25) The covariance function will include a trend based on the featurizer, and will combine two gamma-exponentials: one that respects depth with z-anisotropy, and one that ignores depth altogether. We will set the range for both to be the same parameter to show that it is possible to tie parameters together. In TrendPrior , alpha parameterizes the normal distribution prior for trend coefficients. kernel = \\ krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\ krn.GammaExponential(range=p.r, sill=p.s1, gamma=p.g1, scale=[1., 1., p.zs]) + \\ krn.GammaExponential(range=p.r, sill=p.s2, gamma=p.g2, scale=[1., 1., 0.]) + \\ krn.Noise(p.nugget) Define a Gaussian process with zero mean and a covariance function given by kernel . gp = GP(0, kernel) Instantiate a Model and immediately call generate to generate synthetic observations. tf.random.set_seed(113) obs = Model(gp).generate(locs).vals When the data is plotted, you can see an overall trend with some localized variations. fig, axs = pp.subplots(3, figsize=(7, 6), dpi=120, sharex=True, sharey=True) vmin, vmax = obs.min(), obs.max() pane = np.round((locs[:, 1] + 2) / 2).astype(int) for i, ymid in enumerate(np.linspace(-2, 2, 3)): ymin, ymax = ymid - 1, ymid + 1 c = axs[i].scatter(locs[pane == i, 0], locs[pane == i, 2], c=obs[pane == i], vmin=vmin, vmax=vmax) axs[i].set_title('y = %0.1f' % ymid) axs[i].set_aspect(0.9) axs[2].set_xlabel('x-axis') axs[1].set_ylabel('z-axis') fig.subplots_adjust(right=0.9) cbar_ax = fig.add_axes([0.88, 0.1, 0.02, 0.8]) fig.colorbar(c, cax=cbar_ax) fig.suptitle('Synthetic data, projected to nearest cross section') pp.show() Before we continue, let's define a function that takes a model and plots predictions for the three slices shown above. def plot(model): fig, axs = pp.subplots(3, figsize=(7, 6), dpi=120, sharex=True, sharey=True) for i, ymid in enumerate(np.linspace(-2, 2, 3)): mesh = Mesh.from_bounds([-3, -1, 3, 1], nx=200) mesh_locs = mesh.locations(proj=[[1, 0, 0], [0, 0, 1], [0, ymid, 0]]) # [x, z, 1] -> [x, y, z]. mean, var = model.predict(mesh_locs) meshx, meshy, out = mesh.slice(mean) c = axs[i].pcolormesh(meshx, meshy, out, vmin=vmin, vmax=vmax) axs[i].set_title('y = %0.1f' % ymid) axs[i].set_aspect(0.9) axs[2].set_xlabel('x-axis') axs[1].set_ylabel('z-axis') fig.subplots_adjust(right=0.9) cbar_ax = fig.add_axes([0.88, 0.1, 0.02, 0.8]) fig.colorbar(c, cax=cbar_ax) fig.suptitle('Predictions for 3 cross sections') pp.show()","title":"Synthesizing data"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/#model-1-bayesian-regression","text":"First let's try modeling the data with a Bayesian regression, which is a model with just trends and uncorrelated noise. p = Parameters(alpha=1.0, nugget=0.5) kernel = krn.TrendPrior(trend_featurizer, alpha=p.alpha) + krn.Noise(nugget=p.nugget) model1 = Model(GP(0, kernel)).fit(locs, obs, iters=500) [iter 50 ll -910.60 time 1.73 reg 0.00 alpha 0.61 nugget 0.79] [iter 100 ll -853.59 time 0.69 reg 0.00 alpha 0.37 nugget 1.06] [iter 150 ll -836.20 time 0.69 reg 0.00 alpha 0.23 nugget 1.28] [iter 200 ll -830.86 time 0.68 reg 0.00 alpha 0.15 nugget 1.42] [iter 250 ll -829.16 time 0.69 reg 0.00 alpha 0.10 nugget 1.50] [iter 300 ll -828.57 time 0.68 reg 0.00 alpha 0.07 nugget 1.54] [iter 350 ll -828.35 time 0.69 reg 0.00 alpha 0.06 nugget 1.56] [iter 400 ll -828.27 time 0.68 reg 0.00 alpha 0.05 nugget 1.57] [iter 450 ll -828.25 time 0.69 reg 0.00 alpha 0.04 nugget 1.58] [iter 500 ll -828.24 time 0.68 reg 0.00 alpha 0.04 nugget 1.58] And here are predictions for the three slices shown above. plot(model1)","title":"Model 1: Bayesian regression"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/#model-2-gp-with-isotropic-sq-exp-covariance-function","text":"Now let's layer on an isotropic squared exponential covariance function to the above model. p = Parameters(alpha=1.0, range=1.0, sill=0.5, nugget=0.5) kernel = \\ krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\ krn.SquaredExponential(range=p.range, sill=p.sill) + \\ krn.Noise(nugget=p.nugget) model2 = Model(GP(0, kernel)).fit(locs, obs, iters=500) [iter 50 ll -864.77 time 1.41 reg 0.00 alpha 0.61 sill 0.75 range 0.62 nugget 0.77] [iter 100 ll -829.58 time 0.80 reg 0.00 alpha 0.37 sill 0.53 range 0.46 nugget 0.99] [iter 150 ll -820.58 time 0.80 reg 0.00 alpha 0.23 sill 0.38 range 0.40 nugget 1.13] [iter 200 ll -817.97 time 0.80 reg 0.00 alpha 0.15 sill 0.33 range 0.38 nugget 1.22] [iter 250 ll -816.98 time 0.80 reg 0.00 alpha 0.10 sill 0.31 range 0.38 nugget 1.27] [iter 300 ll -816.48 time 0.80 reg 0.00 alpha 0.07 sill 0.30 range 0.38 nugget 1.29] [iter 350 ll -816.18 time 0.80 reg 0.00 alpha 0.05 sill 0.29 range 0.39 nugget 1.31] [iter 400 ll -816.00 time 0.80 reg 0.00 alpha 0.04 sill 0.28 range 0.40 nugget 1.32] [iter 450 ll -815.90 time 0.80 reg 0.00 alpha 0.04 sill 0.27 range 0.42 nugget 1.33] [iter 500 ll -815.84 time 0.80 reg 0.00 alpha 0.03 sill 0.27 range 0.43 nugget 1.33] The log-likelihood is improved as a result of a more complex model. Nugget is much lower. Predictions: plot(model2)","title":"Model 2: GP with isotropic sq-exp covariance function"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/#model-3-gp-with-anisotropic-sq-exp-covariance-function","text":"Now we switch from isotropic to anisotropic for the covariance function. p = Parameters(alpha=1.0, range=1.0, sill=0.5, nugget=0.5, zs=5.0) kernel = \\ krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\ krn.SquaredExponential(range=p.range, sill=p.sill, scale=[1., 1., p.zs]) + \\ krn.Noise(nugget=p.nugget) model3 = Model(GP(0, kernel)).fit(locs, obs, iters=500) [iter 50 ll -841.04 time 1.50 reg 0.00 alpha 0.61 zs 7.43 sill 0.75 range 0.65 nugget 0.76] [iter 100 ll -821.03 time 0.86 reg 0.00 alpha 0.37 zs 8.75 sill 0.74 range 0.50 nugget 0.90] [iter 150 ll -817.17 time 0.86 reg 0.00 alpha 0.23 zs 8.73 sill 0.66 range 0.43 nugget 0.95] [iter 200 ll -815.63 time 0.86 reg 0.00 alpha 0.15 zs 8.13 sill 0.63 range 0.39 nugget 0.97] [iter 250 ll -814.62 time 0.86 reg 0.00 alpha 0.10 zs 7.37 sill 0.63 range 0.37 nugget 0.98] [iter 300 ll -813.89 time 0.87 reg 0.00 alpha 0.07 zs 6.66 sill 0.63 range 0.36 nugget 0.97] [iter 350 ll -813.41 time 0.87 reg 0.00 alpha 0.05 zs 6.08 sill 0.64 range 0.35 nugget 0.97] [iter 400 ll -813.12 time 0.87 reg 0.00 alpha 0.04 zs 5.64 sill 0.65 range 0.34 nugget 0.96] [iter 450 ll -812.97 time 0.87 reg 0.00 alpha 0.04 zs 5.32 sill 0.66 range 0.33 nugget 0.95] [iter 500 ll -812.89 time 0.87 reg 0.00 alpha 0.04 zs 5.11 sill 0.67 range 0.32 nugget 0.94] Log-likelihood and nugget both improve further. Predictions: plot(model3)","title":"Model 3: GP with anisotropic sq-exp covariance function"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/#model-4-gp-with-anisotropic-gamma-exp-covariance-function","text":"Now we switch from a squared-exponential to a gamma-exponential covariance function, which has an extra shape parameter. p = Parameters(alpha=1.0, range=1.0, sill=0.5, nugget=0.5, zs=5.0, gamma=1.0) kernel = \\ krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\ krn.GammaExponential(range=p.range, sill=p.sill, gamma=p.gamma, scale=[1., 1., p.zs]) + \\ krn.Noise(nugget=p.nugget) model4 = Model(GP(0, kernel)).fit(locs, obs, iters=500) [iter 50 ll -819.50 time 1.83 reg 0.00 alpha 0.61 zs 7.16 sill 0.76 range 0.67 gamma 0.84 nugget 0.74] [iter 100 ll -815.43 time 0.94 reg 0.00 alpha 0.37 zs 6.94 sill 0.86 range 0.58 gamma 0.81 nugget 0.82] [iter 150 ll -814.43 time 0.94 reg 0.00 alpha 0.23 zs 6.19 sill 0.87 range 0.56 gamma 0.81 nugget 0.82] [iter 200 ll -813.66 time 0.94 reg 0.00 alpha 0.15 zs 5.77 sill 0.87 range 0.53 gamma 0.80 nugget 0.82] [iter 250 ll -813.07 time 0.94 reg 0.00 alpha 0.10 zs 5.56 sill 0.86 range 0.50 gamma 0.80 nugget 0.82] [iter 300 ll -812.64 time 0.94 reg 0.00 alpha 0.07 zs 5.45 sill 0.86 range 0.48 gamma 0.81 nugget 0.81] [iter 350 ll -812.36 time 0.95 reg 0.00 alpha 0.05 zs 5.39 sill 0.86 range 0.46 gamma 0.82 nugget 0.80] [iter 400 ll -812.19 time 0.95 reg 0.00 alpha 0.04 zs 5.36 sill 0.86 range 0.45 gamma 0.83 nugget 0.79] [iter 450 ll -812.10 time 0.95 reg 0.00 alpha 0.04 zs 5.33 sill 0.86 range 0.43 gamma 0.84 nugget 0.78] [iter 500 ll -812.06 time 0.95 reg 0.00 alpha 0.03 zs 5.31 sill 0.87 range 0.43 gamma 0.85 nugget 0.77] The log-likelihood has improved very slightly but nugget improves more significantly, since the pointy peak in the gamma-exponential does some of the work of a nugget. Predictions: plot(model4)","title":"Model 4: GP with anisotropic gamma-exp covariance function"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/#model-5-gp-with-stacked-covariance-functions","text":"Finally we switch to using the same covariance function used to generate the synthetic data. p = Parameters(alpha=1., zs=5., r=0.5, s1=2., s2=1., g1=1., g2=1., nugget=1.) kernel = \\ krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\ krn.GammaExponential(range=p.r, sill=p.s1, gamma=p.g1, scale=[1., 1., p.zs]) + \\ krn.GammaExponential(range=p.r, sill=p.s2, gamma=p.g2, scale=[1., 1., 0.]) + \\ krn.Noise(nugget=p.nugget) model5 = Model(GP(0, kernel)).fit(locs, obs, iters=500) [iter 50 ll -824.02 time 2.05 reg 0.00 alpha 0.61 zs 4.04 s1 1.32 r 0.75 g1 0.85 s2 0.65 g2 1.16 nugget 0.67] [iter 100 ll -816.07 time 1.10 reg 0.00 alpha 0.38 zs 6.56 s1 1.11 r 0.81 g1 0.57 s2 0.50 g2 1.00 nugget 0.62] [iter 150 ll -813.38 time 1.10 reg 0.00 alpha 0.24 zs 8.73 s1 1.02 r 0.79 g1 0.45 s2 0.43 g2 0.79 nugget 0.60] [iter 200 ll -812.33 time 1.10 reg 0.00 alpha 0.16 zs 9.27 s1 0.97 r 0.74 g1 0.41 s2 0.39 g2 0.63 nugget 0.59] [iter 250 ll -811.74 time 1.11 reg 0.00 alpha 0.11 zs 9.15 s1 0.94 r 0.67 g1 0.42 s2 0.38 g2 0.57 nugget 0.59] [iter 300 ll -811.27 time 1.11 reg 0.00 alpha 0.08 zs 9.08 s1 0.91 r 0.60 g1 0.45 s2 0.38 g2 0.54 nugget 0.58] [iter 350 ll -810.74 time 1.11 reg 0.00 alpha 0.06 zs 9.15 s1 0.88 r 0.52 g1 0.52 s2 0.37 g2 0.54 nugget 0.57] [iter 400 ll -809.82 time 1.10 reg 0.00 alpha 0.05 zs 9.39 s1 0.82 r 0.43 g1 0.66 s2 0.37 g2 0.54 nugget 0.56] [iter 450 ll -808.27 time 1.11 reg 0.00 alpha 0.04 zs 9.57 s1 0.76 r 0.36 g1 0.93 s2 0.36 g2 0.52 nugget 0.54] [iter 500 ll -807.77 time 1.11 reg 0.00 alpha 0.04 zs 8.73 s1 0.73 r 0.35 g1 1.11 s2 0.37 g2 0.49 nugget 0.53] Not surprisingly, log likelihood and nugget both improve further. You can see faint vertical stripes that correspond the the \"depth-invariant\" component of the covariance function. plot(model5)","title":"Model 5: GP with stacked covariance functions"},{"location":"notebooks/gaussian-processes-in-geostat/gaussian-processes-in-geostat/","text":"from geostat import GP, Model, Mesh, Parameters import geostat import geostat.kernel as krn import matplotlib.pyplot as pp import numpy as np Overview In this notebook we will: * Use a Gaussian process to generate synthetic data with known geospatial parameters. * Use a second Gaussian process to infer the geospatial parameters from the synthetic data. * Use the fitted Gaussian process to interpolate locations on a mesh. Synthesizing data We will synthesize data at mesh locations in a square centered on the origin. First define mesh locations using a Mesh object. The nx argument specifies 80 mesh coordinates in the x direction, and keeps the pitch the same in the y direction (which results in 80 mesh coordinates in that direction as well). mesh = Mesh.from_bounds([-1, -1, 1, 1], nx=80) Declare the terms of the spatial trend. The decorator converts the function trend_featurizer into a featurizer that Geostat can use. The normalize argument causes each feature to have zero mean and unit variance after being applied to mesh locations. It also adds a constant one feature. The method mesh.locations() returns an array of shape [N, 2] , where N is the number of locations. @geostat.featurizer(normalize=mesh.locations()) def trend_featurizer(x, y): return x, y, x*x, x*y, y*y Model parameters are specified here, along with their values. The return value p is a namespace. p = Parameters(alpha=0.25, range=0.33, sill=1., nugget=0.25) The covariance function has three terms: TrendPrior specifies a trend based on trend_featurizer . In TrendPrior , alpha parameterizes the normal distribution prior for trend coefficients. SquaredExponential , a stationary covariance function. Noise , uncorrelated noise. kernel = krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\ krn.SquaredExponential(range=p.range, sill=p.sill) + \\ krn.Noise(nugget=p.nugget) Define a Gaussian process with zero mean and a covariance function given by kernel . gp = GP(0, kernel) Instantiate a Model and then call generate to generate synthetic observations. The result mesh_obs has shape [N] . model = Model(gp) mesh_obs = model.generate(mesh.locations()).vals When the data is plotted, you can see an overall trend with some localized variations. The method mesh.slice() forms the observations into a 2d array suitable for use with pcolormesh . vmin, vmax = mesh_obs.min(), mesh_obs.max() meshx, meshy, mesh_obs_2d = mesh.slice(mesh_obs) # Each return value is a 2d array. c = pp.pcolormesh(meshx, meshy, mesh_obs_2d, vmin=vmin, vmax=vmax) pp.colorbar(c) pp.title('Synthetic data') pp.show() Of these synthetic datapoints we'll sample just 200, with which we'll try to reconstruct the rest of the data. sample_indices = np.random.choice(len(mesh_obs), [200], replace=False) locs = mesh.locations()[sample_indices, :] obs = mesh_obs[sample_indices] c = pp.scatter(locs[:, 0], locs[:, 1], c=obs, vmin=vmin, vmax=vmax) pp.colorbar(c) pp.title('Synthetic observations') pp.show() Inferring parameters Now we set the parameters in model to something arbitrary and see if the model can infer the correct parameters from the data, which consists of locs and obs . We don't expect alpha to converge to what it used to be, since TrendPrior generates only a small number of trend coefficients using alpha . However, sill , range , and nugget should all converge to something close. (The None at the end suppresses extraneous output.) model.set(alpha=1.0, range=1.0, sill=0.5, nugget=0.5) model.fit(locs, obs, iters=500) None [iter 50 ll -237.51 time 1.86 reg 0.00 alpha 0.61 sill 0.84 range 0.60 nugget 0.56] [iter 100 ll -200.69 time 0.59 reg 0.00 alpha 0.37 sill 1.24 range 0.42 nugget 0.34] [iter 150 ll -194.76 time 0.59 reg 0.00 alpha 0.23 sill 1.44 range 0.37 nugget 0.26] [iter 200 ll -194.28 time 0.58 reg 0.00 alpha 0.17 sill 1.48 range 0.36 nugget 0.25] [iter 250 ll -194.12 time 0.58 reg 0.00 alpha 0.13 sill 1.47 range 0.36 nugget 0.25] [iter 300 ll -194.05 time 0.58 reg 0.00 alpha 0.11 sill 1.46 range 0.35 nugget 0.25] [iter 350 ll -194.01 time 0.58 reg 0.00 alpha 0.09 sill 1.46 range 0.35 nugget 0.25] [iter 400 ll -194.00 time 0.58 reg 0.00 alpha 0.09 sill 1.46 range 0.35 nugget 0.25] [iter 450 ll -193.99 time 0.58 reg 0.00 alpha 0.08 sill 1.47 range 0.35 nugget 0.25] [iter 500 ll -193.99 time 0.58 reg 0.00 alpha 0.08 sill 1.48 range 0.36 nugget 0.25] Generating predictions Call model to get predictions at the same mesh locations as before: mean, var = model.predict(mesh.locations()) meshx, meshy, mean2d = mesh.slice(mean) # Each return value is a 2d array. c = pp.pcolormesh(meshx, meshy, mean2d, vmin=vmin, vmax=vmax) pp.colorbar(c) pp.title('Prediction mean') pp.show() For comparison, here's the original synthetic data: c = pp.pcolormesh(meshx, meshy, mesh_obs_2d, vmin=vmin, vmax=vmax) pp.colorbar(c) pp.title('Synthetic data') pp.show() And here's a plot of prediction variance, which accounts for, among other things, the noise that the model is unable to reconstruct. meshx, meshy, var2d = mesh.slice(var) # Each return value is a 2d array. c = pp.pcolormesh(meshx, meshy, var2d, cmap='gist_heat_r') pp.colorbar(c) pp.title('Prediction variance') pp.show()","title":"Gaussian processes in geostat"},{"location":"notebooks/gaussian-processes-in-geostat/gaussian-processes-in-geostat/#overview","text":"In this notebook we will: * Use a Gaussian process to generate synthetic data with known geospatial parameters. * Use a second Gaussian process to infer the geospatial parameters from the synthetic data. * Use the fitted Gaussian process to interpolate locations on a mesh.","title":"Overview"},{"location":"notebooks/gaussian-processes-in-geostat/gaussian-processes-in-geostat/#synthesizing-data","text":"We will synthesize data at mesh locations in a square centered on the origin. First define mesh locations using a Mesh object. The nx argument specifies 80 mesh coordinates in the x direction, and keeps the pitch the same in the y direction (which results in 80 mesh coordinates in that direction as well). mesh = Mesh.from_bounds([-1, -1, 1, 1], nx=80) Declare the terms of the spatial trend. The decorator converts the function trend_featurizer into a featurizer that Geostat can use. The normalize argument causes each feature to have zero mean and unit variance after being applied to mesh locations. It also adds a constant one feature. The method mesh.locations() returns an array of shape [N, 2] , where N is the number of locations. @geostat.featurizer(normalize=mesh.locations()) def trend_featurizer(x, y): return x, y, x*x, x*y, y*y Model parameters are specified here, along with their values. The return value p is a namespace. p = Parameters(alpha=0.25, range=0.33, sill=1., nugget=0.25) The covariance function has three terms: TrendPrior specifies a trend based on trend_featurizer . In TrendPrior , alpha parameterizes the normal distribution prior for trend coefficients. SquaredExponential , a stationary covariance function. Noise , uncorrelated noise. kernel = krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\ krn.SquaredExponential(range=p.range, sill=p.sill) + \\ krn.Noise(nugget=p.nugget) Define a Gaussian process with zero mean and a covariance function given by kernel . gp = GP(0, kernel) Instantiate a Model and then call generate to generate synthetic observations. The result mesh_obs has shape [N] . model = Model(gp) mesh_obs = model.generate(mesh.locations()).vals When the data is plotted, you can see an overall trend with some localized variations. The method mesh.slice() forms the observations into a 2d array suitable for use with pcolormesh . vmin, vmax = mesh_obs.min(), mesh_obs.max() meshx, meshy, mesh_obs_2d = mesh.slice(mesh_obs) # Each return value is a 2d array. c = pp.pcolormesh(meshx, meshy, mesh_obs_2d, vmin=vmin, vmax=vmax) pp.colorbar(c) pp.title('Synthetic data') pp.show() Of these synthetic datapoints we'll sample just 200, with which we'll try to reconstruct the rest of the data. sample_indices = np.random.choice(len(mesh_obs), [200], replace=False) locs = mesh.locations()[sample_indices, :] obs = mesh_obs[sample_indices] c = pp.scatter(locs[:, 0], locs[:, 1], c=obs, vmin=vmin, vmax=vmax) pp.colorbar(c) pp.title('Synthetic observations') pp.show()","title":"Synthesizing data"},{"location":"notebooks/gaussian-processes-in-geostat/gaussian-processes-in-geostat/#inferring-parameters","text":"Now we set the parameters in model to something arbitrary and see if the model can infer the correct parameters from the data, which consists of locs and obs . We don't expect alpha to converge to what it used to be, since TrendPrior generates only a small number of trend coefficients using alpha . However, sill , range , and nugget should all converge to something close. (The None at the end suppresses extraneous output.) model.set(alpha=1.0, range=1.0, sill=0.5, nugget=0.5) model.fit(locs, obs, iters=500) None [iter 50 ll -237.51 time 1.86 reg 0.00 alpha 0.61 sill 0.84 range 0.60 nugget 0.56] [iter 100 ll -200.69 time 0.59 reg 0.00 alpha 0.37 sill 1.24 range 0.42 nugget 0.34] [iter 150 ll -194.76 time 0.59 reg 0.00 alpha 0.23 sill 1.44 range 0.37 nugget 0.26] [iter 200 ll -194.28 time 0.58 reg 0.00 alpha 0.17 sill 1.48 range 0.36 nugget 0.25] [iter 250 ll -194.12 time 0.58 reg 0.00 alpha 0.13 sill 1.47 range 0.36 nugget 0.25] [iter 300 ll -194.05 time 0.58 reg 0.00 alpha 0.11 sill 1.46 range 0.35 nugget 0.25] [iter 350 ll -194.01 time 0.58 reg 0.00 alpha 0.09 sill 1.46 range 0.35 nugget 0.25] [iter 400 ll -194.00 time 0.58 reg 0.00 alpha 0.09 sill 1.46 range 0.35 nugget 0.25] [iter 450 ll -193.99 time 0.58 reg 0.00 alpha 0.08 sill 1.47 range 0.35 nugget 0.25] [iter 500 ll -193.99 time 0.58 reg 0.00 alpha 0.08 sill 1.48 range 0.36 nugget 0.25]","title":"Inferring parameters"},{"location":"notebooks/gaussian-processes-in-geostat/gaussian-processes-in-geostat/#generating-predictions","text":"Call model to get predictions at the same mesh locations as before: mean, var = model.predict(mesh.locations()) meshx, meshy, mean2d = mesh.slice(mean) # Each return value is a 2d array. c = pp.pcolormesh(meshx, meshy, mean2d, vmin=vmin, vmax=vmax) pp.colorbar(c) pp.title('Prediction mean') pp.show() For comparison, here's the original synthetic data: c = pp.pcolormesh(meshx, meshy, mesh_obs_2d, vmin=vmin, vmax=vmax) pp.colorbar(c) pp.title('Synthetic data') pp.show() And here's a plot of prediction variance, which accounts for, among other things, the noise that the model is unable to reconstruct. meshx, meshy, var2d = mesh.slice(var) # Each return value is a 2d array. c = pp.pcolormesh(meshx, meshy, var2d, cmap='gist_heat_r') pp.colorbar(c) pp.title('Prediction variance') pp.show()","title":"Generating predictions"},{"location":"notebooks/gaussian-processes-in-tensorflow/gaussian-processes-in-tensorflow/","text":"Gaussian processes in Tensorflow import tensorflow as tf tf.__version__ '1.8.0' Geostatistical datasets are often a set of measurements with locations. Nearby measurements covary a lot. Distant measurements are nearly independent. Let's simulate this: Define \\(N\\) locations \\(x\\) by drawing uniformly at random from a square area. Create \\(N\\times N\\) distance matrix \\(D\\) (euclidean distance between locations). Define a covariance function: \\(c(d; r, s, n) = s \\cdot \\exp(-(d/r)^2) + n \\cdot \\delta_d\\) , where \\(d\\) is distance, and \\((r, s, n)\\) correspond to range/sill/nugget on a variogram. (I think the convention varies between calling either \\(s+n\\) or \\(s\\) the sill.) Also, \\(\\delta_d\\) is 1 when \\(d\\) is 0, and 0 otherwise. Use the covariance function to map \\(D\\) elementwise to a covariance matrix \\(C\\) . Draw \\(u \\sim \\textrm{Normal}(\\beta_1 \\cdot \\mathbb{1}, C)\\) to obtain values, where \\(\\beta_1\\) is the (scalar) mean value of a single draw, and \\(\\mathbb{1}\\) is a vector of \\(N\\) ones. This is implemented as simulate_gp below. The range is called vrange to avoid conflicting with Python's range . The data consists of locations x (or equivalently, the distance matrix D ) and measurements u . import numpy as np from scipy.spatial.distance import cdist np.set_printoptions(precision=2, threshold=50) def simulate_gp(N, vrange, sill, nugget, offset): # Sample N locations from square with corners at [\u00b110, \u00b110]. x = np.random.uniform(-10.0, 10.0, [N, 2]) # Compute distance matrix for sampled locations. D = cdist(x, x) # Compute corresponding covariance matrix. C = sill * np.exp(-np.square(D/vrange)) + nugget * np.eye(N) # The mean is just a vector where every entry is the offset. m = np.zeros([N]) + offset # Simulate geospatial measurements by sampling using covariance matrix u = np.random.multivariate_normal(m, C) return x, D, C, m, u Now we call simulate_gp and plot the result. (You may have to run this twice to get the plot to show up.) x, D, C, m, u = simulate_gp( N = 300, vrange = 5.0, sill = 2.0, nugget = 2.0, offset = 1.0) print(\"Locations\") print(x) print(\"Distance matrix\") print(D) print(\"Covariance matrix\") print(C) print(\"Simulated measurements\") print(u) import matplotlib.pyplot as pp pp.scatter(x[:, 0], x[:, 1], c=u) pp.show() Locations [[ 0.83 9.5 ] [-6.51 0.99] [ 2.84 6.14] ... [-4.99 0.45] [-3.49 2.24] [ 5.49 -0.34]] Distance matrix [[ 0. 11.24 3.92 ... 10.76 8.45 10.89] [11.24 0. 10.68 ... 1.61 3.26 12.07] [ 3.92 10.68 0. ... 9.68 7.44 7. ] ... [10.76 1.61 9.68 ... 0. 2.33 10.51] [ 8.45 3.26 7.44 ... 2.33 0. 9.34] [10.89 12.07 7. ... 10.51 9.34 0. ]] Covariance matrix [[4. 0.01 1.08 ... 0.02 0.11 0.02] [0.01 4. 0.02 ... 1.8 1.31 0.01] [1.08 0.02 4. ... 0.05 0.22 0.28] ... [0.02 1.8 0.05 ... 4. 1.61 0.02] [0.11 1.31 0.22 ... 1.61 4. 0.06] [0.02 0.01 0.28 ... 0.02 0.06 4. ]] Simulated measurements [ 2.78 -0.64 -0.56 ... -0.27 2.02 2. ] Now we make a function infer_gp to infer the gaussian process parameters (range, sill, nugget, offset) using maximum likelihood. We implement the same graph in Tensorflow as we did in NumPy, and tack on the negative log PDF of a multivariate normal distribution at the end, which we minimize. That is, we minimize: \\( \\(-\\log p(u \\mid m, C) = \\frac{1}{2}\\bigg[\\log\\,\\big|\\, 2\\pi C\\,\\big| + (u-m)^T C^{-1} (u-m) \\bigg],\\) \\) where \\(m = \\beta_1 \\cdot \\mathbb{1}\\) . The function has two arguments: * inputs is a list of numpy arrays: * Distance matrix D (shape: [N, N]) * Measurements u (shape: [N]) * parameters is a list of tensors for range, sill, nugget, and offset. Each tensor can be a tf.Variables (if it is to be inferred) or a constant (if it's a given). def infer_gp(inputs, parameters): D, u = inputs vrange, sill, nugget, offset = parameters # Construct covariance; boost diagonal by 1e-6 for numerical stability. covariance = sill * tf.exp(-tf.square(tf.constant(D)/vrange)) \\ + (nugget + 1e-6) * tf.eye(D.shape[0], dtype=tf.float64) # Log likelihood is the PDF of a multivariate gaussian. u_adj = tf.constant(u) - offset logdet = tf.linalg.logdet(2 * np.pi * covariance) quad = tf.matmul(tf.expand_dims(u_adj, 0), tf.matrix_solve(covariance, tf.expand_dims(u_adj, -1)))[0, 0] ll = -0.5 * (logdet + quad) # Infer using an adaptive gradient descent optimizer. train = tf.train.AdamOptimizer(1e-2).minimize(-ll) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for i in range(20): for j in range(100): sess.run(train) print('[ll %7.2f] [range %4.2f] [sill %4.2f] [nugget %4.2f] [offset %4.2f]' % tuple(sess.run([ll, vrange, sill, nugget, offset]))) return sess.run([vrange, sill, nugget, offset]) Here we make a tf.Variable for each parameter, using a log as the underlying representation if the parameter is positive-only. # Define parameters using tf.Variable. log_vrange = tf.Variable(0.0, dtype=tf.float64) log_sill = tf.Variable(0.0, dtype=tf.float64) log_nugget = tf.Variable(0.0, dtype=tf.float64) vrange = tf.exp(log_vrange) sill = tf.exp(log_sill) nugget = tf.exp(log_nugget) offset = tf.Variable(0.0, dtype=tf.float64) vrange_val, sill_val, nugget_val, offset_val = infer_gp([D, u], [vrange, sill, nugget, offset]) [ll -552.48] [range 2.25] [sill 1.48] [nugget 1.66] [offset 0.63] [ll -537.88] [range 3.71] [sill 1.36] [nugget 1.68] [offset 0.84] [ll -536.83] [range 4.20] [sill 1.38] [nugget 1.70] [offset 0.96] [ll -536.62] [range 4.28] [sill 1.42] [nugget 1.71] [offset 1.05] [ll -536.51] [range 4.28] [sill 1.43] [nugget 1.71] [offset 1.12] [ll -536.45] [range 4.27] [sill 1.42] [nugget 1.71] [offset 1.18] [ll -536.42] [range 4.26] [sill 1.41] [nugget 1.71] [offset 1.23] [ll -536.41] [range 4.25] [sill 1.40] [nugget 1.71] [offset 1.26] [ll -536.40] [range 4.25] [sill 1.40] [nugget 1.71] [offset 1.27] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.29] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.29] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30] We can do better inference by integrating over all possibilities for \\(\\beta_1\\) . (Integrating over range, sill, and nugget, which are parameters in \\(C\\) , is hard; but integrating over parameters in \\(m\\) is relatively easy.) This corresponds to the following generative model. First, draw \\(\\beta_1\\) from a normal distribution: \\[\\beta_1 \\sim \\mathcal{N}(0, 100).\\] The variance should be large enough that the distribution assigns reasonably large probabilities to any plausible value for \\(\\beta_1\\) . Next, draw \\(u\\) from a multivariate normal distribution, as we've been doing all along: \\[u \\mid \\beta_1 \\sim \\mathcal{N}(\\beta_1 \\cdot \\mathbb{1}, C).\\] From this we can derive a distribution for \\(u\\) by marginalizing (integrating) over \\(\\beta_1\\) . That is, we can compute: \\[p(u) = \\int_{-\\infty}^{\\infty} p(u\\mid\\beta_1) \\, p(\\beta_1) \\, d\\beta_1.\\] We rely on the abstract fact that if \\(X_1 \\sim \\mathcal{N}(\\mu_1, \\Sigma_1)\\) and \\(X_2 \\mid X_1 \\sim \\mathcal{N}(\\mu_2, \\Sigma_2)\\) , then \\(X_2 \\sim \\mathcal{N}(\\mu_1, \\Sigma_1 + \\Sigma_2)\\) . ( \\(X_1\\) and \\(X_2\\) are vectors equal in length.) To apply this, we note that since \\(\\beta_1 \\sim \\mathcal{N}(0, 100)\\) , then \\(\\beta_1 \\cdot \\mathbb{1} \\sim \\mathcal{N}(0, 100 \\cdot \\mathbb{1}\\mathbb{1}^T)\\) , where \\(\\mathbb{1}\\mathbb{1}^T\\) is a matrix of all ones. From this it follows that \\[u \\sim \\mathcal{N}(0, A)\\] where \\(A = 100 \\cdot \\mathbb{1}\\mathbb{1}^T + C.\\) The function below implements inference with this model, where the offset is marginalized out. def infer_gp_marginalize_over_offset(inputs, parameters, offset_prior): D, u = inputs vrange, sill, nugget = parameters # Construct covariance; boost diagonal by 1e-6 for numerical stability. covariance = sill * tf.exp(-tf.square(tf.constant(D)/vrange)) \\ + (nugget + 1e-6) * tf.eye(D.shape[0], dtype=tf.float64) \\ + offset_prior # Log likelihood is the PDF of a multivariate gaussian. u_adj = tf.constant(u) - offset logdet = tf.linalg.logdet(2 * np.pi * covariance) quad = tf.matmul(tf.expand_dims(u_adj, 0), tf.matrix_solve(covariance, tf.expand_dims(u_adj, -1)))[0, 0] ll = -0.5 * (logdet + quad) # Infer using an adaptive gradient descent optimizer. train = tf.train.AdamOptimizer(1e-2).minimize(-ll) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for i in range(20): for j in range(100): sess.run(train) print('[ll %7.2f] [range %4.2f] [sill %4.2f] [nugget %4.2f]' % tuple(sess.run([ll, vrange, sill, nugget]))) return sess.run([vrange, sill, nugget]) When we run this, we don't get an estimate for the offset, but the other estimates are often improved. Bear in mind that we can actually solve for the posterior distribution of the offset if we want to. # Define parameters using tf.Variable. log_vrange = tf.Variable(0.0, dtype=tf.float64) log_sill = tf.Variable(0.0, dtype=tf.float64) log_nugget = tf.Variable(0.0, dtype=tf.float64) vrange = tf.exp(log_vrange) sill = tf.exp(log_sill) nugget = tf.exp(log_nugget) offset_prior = 100.0 vrange_val, sill_val, nugget_val = infer_gp_marginalize_over_offset([D, u], [vrange, sill, nugget], offset_prior) [ll -541.34] [range 3.02] [sill 0.96] [nugget 1.69] [ll -537.97] [range 4.51] [sill 1.13] [nugget 1.73] [ll -537.93] [range 4.65] [sill 1.25] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] Interpolation takes values at \\(N_1\\) locations, and gives means and variances for \\(N_2\\) locations. Formally, interpolation takes input locations \\(x_1\\) , an \\(N_1 \\times 2\\) matrix, input values \\(u_1\\) , a vector of \\(N_1\\) elements, output locations \\(x_2\\) , an \\(N_2 \\times 2\\) matrix. and gives a distribution for output values \\(u_2\\) , a vector of \\(N_2\\) elements. For notational convenience, define \\[x = \\begin{bmatrix}x_1\\\\x_2\\end{bmatrix} \\textrm{ and } u = \\begin{bmatrix}u_1\\\\u_2\\end{bmatrix}.\\] The model remains the same as before, so \\(u \\sim \\mathcal{N}(0, A)\\) , where \\(A\\) is constructed from a distance matrix of all locations \\(x\\) as before. For clarity, let's expand \\(u\\) and \\(A\\) : \\[\\begin{bmatrix}u_1\\\\u_2\\end{bmatrix} \\sim \\mathcal{N}\\bigg(0, \\begin{bmatrix}A_{11} & A_{12}\\\\A_{21} & A_{22}\\end{bmatrix}\\bigg).\\] Interpolation consists of getting a distribution for \\(u_2\\) given \\(u_1\\) . This is a textbook thing to do with a multivariate normal distribution, and the solution is: \\[u_2 \\mid u_1 \\sim \\mathcal{N}(A_{21}A_{11}^{-1}u_1,\\ A_{22} - A_{21}A_{11}^{-1}A_{12}).\\] Bear in mind, if we just want the marginal variance of each element in \\(u_2\\) , we only need to compute the diagonal entries of \\(A_{22} - A_{21}A_{11}^{-1}A_{12}\\) . Refer to Wikipedia for more details. def interpolate_gp(x1, u1, x2, parameter_vals, offset_prior): vrange, sill, nugget = parameter_vals # Compute distance matrices for sampled locations. D11 = cdist(x1, x1) D12 = cdist(x1, x2) D21 = cdist(x2, x1) D22 = cdist(x2, x2) # Compute covariance matrices. C11 = sill * np.exp(-np.square(D11/vrange)) + nugget * np.eye(len(x1)) + offset_prior C12 = sill * np.exp(-np.square(D12/vrange)) + offset_prior # No nugget for off-diagonal entries C21 = sill * np.exp(-np.square(D21/vrange)) + offset_prior # No nugget for off-diagonal entries C22 = sill * np.exp(-np.square(D22/vrange)) + nugget * np.eye(len(x2)) + offset_prior u2_mean = np.matmul(C21, np.linalg.solve(C11, u)) u2_var = np.diag(C22) - np.sum(C12 * np.linalg.solve(C11, C12), axis=0) return u2_mean, u2_var MX = 61 MY = 61 M = MX * MY # Number of points to infer. # Gross code to get mesh locations. xx, yy = np.meshgrid(np.linspace(-12, 12, MX), np.linspace(-12, 12, MY)) x2 = np.hstack([xx.reshape((M, 1)), yy.reshape((M, 1))]) # Interpolate! u2_mean, u2_var = interpolate_gp(x, u, x2, [vrange_val, sill_val, nugget_val], offset_prior) # Plot old values, new value means, new value variances for locations, values in [(x, u), (x2, u2_mean), (x2, u2_var)]: pp.scatter(locations[:, 0], locations[:, 1], c=values) pp.xlim(-12, 12) pp.ylim(-12, 12) pp.show()","title":"Gaussian processes in Tensorflow"},{"location":"notebooks/gaussian-processes-in-tensorflow/gaussian-processes-in-tensorflow/#gaussian-processes-in-tensorflow","text":"import tensorflow as tf tf.__version__ '1.8.0' Geostatistical datasets are often a set of measurements with locations. Nearby measurements covary a lot. Distant measurements are nearly independent. Let's simulate this: Define \\(N\\) locations \\(x\\) by drawing uniformly at random from a square area. Create \\(N\\times N\\) distance matrix \\(D\\) (euclidean distance between locations). Define a covariance function: \\(c(d; r, s, n) = s \\cdot \\exp(-(d/r)^2) + n \\cdot \\delta_d\\) , where \\(d\\) is distance, and \\((r, s, n)\\) correspond to range/sill/nugget on a variogram. (I think the convention varies between calling either \\(s+n\\) or \\(s\\) the sill.) Also, \\(\\delta_d\\) is 1 when \\(d\\) is 0, and 0 otherwise. Use the covariance function to map \\(D\\) elementwise to a covariance matrix \\(C\\) . Draw \\(u \\sim \\textrm{Normal}(\\beta_1 \\cdot \\mathbb{1}, C)\\) to obtain values, where \\(\\beta_1\\) is the (scalar) mean value of a single draw, and \\(\\mathbb{1}\\) is a vector of \\(N\\) ones. This is implemented as simulate_gp below. The range is called vrange to avoid conflicting with Python's range . The data consists of locations x (or equivalently, the distance matrix D ) and measurements u . import numpy as np from scipy.spatial.distance import cdist np.set_printoptions(precision=2, threshold=50) def simulate_gp(N, vrange, sill, nugget, offset): # Sample N locations from square with corners at [\u00b110, \u00b110]. x = np.random.uniform(-10.0, 10.0, [N, 2]) # Compute distance matrix for sampled locations. D = cdist(x, x) # Compute corresponding covariance matrix. C = sill * np.exp(-np.square(D/vrange)) + nugget * np.eye(N) # The mean is just a vector where every entry is the offset. m = np.zeros([N]) + offset # Simulate geospatial measurements by sampling using covariance matrix u = np.random.multivariate_normal(m, C) return x, D, C, m, u Now we call simulate_gp and plot the result. (You may have to run this twice to get the plot to show up.) x, D, C, m, u = simulate_gp( N = 300, vrange = 5.0, sill = 2.0, nugget = 2.0, offset = 1.0) print(\"Locations\") print(x) print(\"Distance matrix\") print(D) print(\"Covariance matrix\") print(C) print(\"Simulated measurements\") print(u) import matplotlib.pyplot as pp pp.scatter(x[:, 0], x[:, 1], c=u) pp.show() Locations [[ 0.83 9.5 ] [-6.51 0.99] [ 2.84 6.14] ... [-4.99 0.45] [-3.49 2.24] [ 5.49 -0.34]] Distance matrix [[ 0. 11.24 3.92 ... 10.76 8.45 10.89] [11.24 0. 10.68 ... 1.61 3.26 12.07] [ 3.92 10.68 0. ... 9.68 7.44 7. ] ... [10.76 1.61 9.68 ... 0. 2.33 10.51] [ 8.45 3.26 7.44 ... 2.33 0. 9.34] [10.89 12.07 7. ... 10.51 9.34 0. ]] Covariance matrix [[4. 0.01 1.08 ... 0.02 0.11 0.02] [0.01 4. 0.02 ... 1.8 1.31 0.01] [1.08 0.02 4. ... 0.05 0.22 0.28] ... [0.02 1.8 0.05 ... 4. 1.61 0.02] [0.11 1.31 0.22 ... 1.61 4. 0.06] [0.02 0.01 0.28 ... 0.02 0.06 4. ]] Simulated measurements [ 2.78 -0.64 -0.56 ... -0.27 2.02 2. ] Now we make a function infer_gp to infer the gaussian process parameters (range, sill, nugget, offset) using maximum likelihood. We implement the same graph in Tensorflow as we did in NumPy, and tack on the negative log PDF of a multivariate normal distribution at the end, which we minimize. That is, we minimize: \\( \\(-\\log p(u \\mid m, C) = \\frac{1}{2}\\bigg[\\log\\,\\big|\\, 2\\pi C\\,\\big| + (u-m)^T C^{-1} (u-m) \\bigg],\\) \\) where \\(m = \\beta_1 \\cdot \\mathbb{1}\\) . The function has two arguments: * inputs is a list of numpy arrays: * Distance matrix D (shape: [N, N]) * Measurements u (shape: [N]) * parameters is a list of tensors for range, sill, nugget, and offset. Each tensor can be a tf.Variables (if it is to be inferred) or a constant (if it's a given). def infer_gp(inputs, parameters): D, u = inputs vrange, sill, nugget, offset = parameters # Construct covariance; boost diagonal by 1e-6 for numerical stability. covariance = sill * tf.exp(-tf.square(tf.constant(D)/vrange)) \\ + (nugget + 1e-6) * tf.eye(D.shape[0], dtype=tf.float64) # Log likelihood is the PDF of a multivariate gaussian. u_adj = tf.constant(u) - offset logdet = tf.linalg.logdet(2 * np.pi * covariance) quad = tf.matmul(tf.expand_dims(u_adj, 0), tf.matrix_solve(covariance, tf.expand_dims(u_adj, -1)))[0, 0] ll = -0.5 * (logdet + quad) # Infer using an adaptive gradient descent optimizer. train = tf.train.AdamOptimizer(1e-2).minimize(-ll) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for i in range(20): for j in range(100): sess.run(train) print('[ll %7.2f] [range %4.2f] [sill %4.2f] [nugget %4.2f] [offset %4.2f]' % tuple(sess.run([ll, vrange, sill, nugget, offset]))) return sess.run([vrange, sill, nugget, offset]) Here we make a tf.Variable for each parameter, using a log as the underlying representation if the parameter is positive-only. # Define parameters using tf.Variable. log_vrange = tf.Variable(0.0, dtype=tf.float64) log_sill = tf.Variable(0.0, dtype=tf.float64) log_nugget = tf.Variable(0.0, dtype=tf.float64) vrange = tf.exp(log_vrange) sill = tf.exp(log_sill) nugget = tf.exp(log_nugget) offset = tf.Variable(0.0, dtype=tf.float64) vrange_val, sill_val, nugget_val, offset_val = infer_gp([D, u], [vrange, sill, nugget, offset]) [ll -552.48] [range 2.25] [sill 1.48] [nugget 1.66] [offset 0.63] [ll -537.88] [range 3.71] [sill 1.36] [nugget 1.68] [offset 0.84] [ll -536.83] [range 4.20] [sill 1.38] [nugget 1.70] [offset 0.96] [ll -536.62] [range 4.28] [sill 1.42] [nugget 1.71] [offset 1.05] [ll -536.51] [range 4.28] [sill 1.43] [nugget 1.71] [offset 1.12] [ll -536.45] [range 4.27] [sill 1.42] [nugget 1.71] [offset 1.18] [ll -536.42] [range 4.26] [sill 1.41] [nugget 1.71] [offset 1.23] [ll -536.41] [range 4.25] [sill 1.40] [nugget 1.71] [offset 1.26] [ll -536.40] [range 4.25] [sill 1.40] [nugget 1.71] [offset 1.27] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.29] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.29] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30] [ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30] We can do better inference by integrating over all possibilities for \\(\\beta_1\\) . (Integrating over range, sill, and nugget, which are parameters in \\(C\\) , is hard; but integrating over parameters in \\(m\\) is relatively easy.) This corresponds to the following generative model. First, draw \\(\\beta_1\\) from a normal distribution: \\[\\beta_1 \\sim \\mathcal{N}(0, 100).\\] The variance should be large enough that the distribution assigns reasonably large probabilities to any plausible value for \\(\\beta_1\\) . Next, draw \\(u\\) from a multivariate normal distribution, as we've been doing all along: \\[u \\mid \\beta_1 \\sim \\mathcal{N}(\\beta_1 \\cdot \\mathbb{1}, C).\\] From this we can derive a distribution for \\(u\\) by marginalizing (integrating) over \\(\\beta_1\\) . That is, we can compute: \\[p(u) = \\int_{-\\infty}^{\\infty} p(u\\mid\\beta_1) \\, p(\\beta_1) \\, d\\beta_1.\\] We rely on the abstract fact that if \\(X_1 \\sim \\mathcal{N}(\\mu_1, \\Sigma_1)\\) and \\(X_2 \\mid X_1 \\sim \\mathcal{N}(\\mu_2, \\Sigma_2)\\) , then \\(X_2 \\sim \\mathcal{N}(\\mu_1, \\Sigma_1 + \\Sigma_2)\\) . ( \\(X_1\\) and \\(X_2\\) are vectors equal in length.) To apply this, we note that since \\(\\beta_1 \\sim \\mathcal{N}(0, 100)\\) , then \\(\\beta_1 \\cdot \\mathbb{1} \\sim \\mathcal{N}(0, 100 \\cdot \\mathbb{1}\\mathbb{1}^T)\\) , where \\(\\mathbb{1}\\mathbb{1}^T\\) is a matrix of all ones. From this it follows that \\[u \\sim \\mathcal{N}(0, A)\\] where \\(A = 100 \\cdot \\mathbb{1}\\mathbb{1}^T + C.\\) The function below implements inference with this model, where the offset is marginalized out. def infer_gp_marginalize_over_offset(inputs, parameters, offset_prior): D, u = inputs vrange, sill, nugget = parameters # Construct covariance; boost diagonal by 1e-6 for numerical stability. covariance = sill * tf.exp(-tf.square(tf.constant(D)/vrange)) \\ + (nugget + 1e-6) * tf.eye(D.shape[0], dtype=tf.float64) \\ + offset_prior # Log likelihood is the PDF of a multivariate gaussian. u_adj = tf.constant(u) - offset logdet = tf.linalg.logdet(2 * np.pi * covariance) quad = tf.matmul(tf.expand_dims(u_adj, 0), tf.matrix_solve(covariance, tf.expand_dims(u_adj, -1)))[0, 0] ll = -0.5 * (logdet + quad) # Infer using an adaptive gradient descent optimizer. train = tf.train.AdamOptimizer(1e-2).minimize(-ll) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for i in range(20): for j in range(100): sess.run(train) print('[ll %7.2f] [range %4.2f] [sill %4.2f] [nugget %4.2f]' % tuple(sess.run([ll, vrange, sill, nugget]))) return sess.run([vrange, sill, nugget]) When we run this, we don't get an estimate for the offset, but the other estimates are often improved. Bear in mind that we can actually solve for the posterior distribution of the offset if we want to. # Define parameters using tf.Variable. log_vrange = tf.Variable(0.0, dtype=tf.float64) log_sill = tf.Variable(0.0, dtype=tf.float64) log_nugget = tf.Variable(0.0, dtype=tf.float64) vrange = tf.exp(log_vrange) sill = tf.exp(log_sill) nugget = tf.exp(log_nugget) offset_prior = 100.0 vrange_val, sill_val, nugget_val = infer_gp_marginalize_over_offset([D, u], [vrange, sill, nugget], offset_prior) [ll -541.34] [range 3.02] [sill 0.96] [nugget 1.69] [ll -537.97] [range 4.51] [sill 1.13] [nugget 1.73] [ll -537.93] [range 4.65] [sill 1.25] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] [ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73] Interpolation takes values at \\(N_1\\) locations, and gives means and variances for \\(N_2\\) locations. Formally, interpolation takes input locations \\(x_1\\) , an \\(N_1 \\times 2\\) matrix, input values \\(u_1\\) , a vector of \\(N_1\\) elements, output locations \\(x_2\\) , an \\(N_2 \\times 2\\) matrix. and gives a distribution for output values \\(u_2\\) , a vector of \\(N_2\\) elements. For notational convenience, define \\[x = \\begin{bmatrix}x_1\\\\x_2\\end{bmatrix} \\textrm{ and } u = \\begin{bmatrix}u_1\\\\u_2\\end{bmatrix}.\\] The model remains the same as before, so \\(u \\sim \\mathcal{N}(0, A)\\) , where \\(A\\) is constructed from a distance matrix of all locations \\(x\\) as before. For clarity, let's expand \\(u\\) and \\(A\\) : \\[\\begin{bmatrix}u_1\\\\u_2\\end{bmatrix} \\sim \\mathcal{N}\\bigg(0, \\begin{bmatrix}A_{11} & A_{12}\\\\A_{21} & A_{22}\\end{bmatrix}\\bigg).\\] Interpolation consists of getting a distribution for \\(u_2\\) given \\(u_1\\) . This is a textbook thing to do with a multivariate normal distribution, and the solution is: \\[u_2 \\mid u_1 \\sim \\mathcal{N}(A_{21}A_{11}^{-1}u_1,\\ A_{22} - A_{21}A_{11}^{-1}A_{12}).\\] Bear in mind, if we just want the marginal variance of each element in \\(u_2\\) , we only need to compute the diagonal entries of \\(A_{22} - A_{21}A_{11}^{-1}A_{12}\\) . Refer to Wikipedia for more details. def interpolate_gp(x1, u1, x2, parameter_vals, offset_prior): vrange, sill, nugget = parameter_vals # Compute distance matrices for sampled locations. D11 = cdist(x1, x1) D12 = cdist(x1, x2) D21 = cdist(x2, x1) D22 = cdist(x2, x2) # Compute covariance matrices. C11 = sill * np.exp(-np.square(D11/vrange)) + nugget * np.eye(len(x1)) + offset_prior C12 = sill * np.exp(-np.square(D12/vrange)) + offset_prior # No nugget for off-diagonal entries C21 = sill * np.exp(-np.square(D21/vrange)) + offset_prior # No nugget for off-diagonal entries C22 = sill * np.exp(-np.square(D22/vrange)) + nugget * np.eye(len(x2)) + offset_prior u2_mean = np.matmul(C21, np.linalg.solve(C11, u)) u2_var = np.diag(C22) - np.sum(C12 * np.linalg.solve(C11, C12), axis=0) return u2_mean, u2_var MX = 61 MY = 61 M = MX * MY # Number of points to infer. # Gross code to get mesh locations. xx, yy = np.meshgrid(np.linspace(-12, 12, MX), np.linspace(-12, 12, MY)) x2 = np.hstack([xx.reshape((M, 1)), yy.reshape((M, 1))]) # Interpolate! u2_mean, u2_var = interpolate_gp(x, u, x2, [vrange_val, sill_val, nugget_val], offset_prior) # Plot old values, new value means, new value variances for locations, values in [(x, u), (x2, u2_mean), (x2, u2_var)]: pp.scatter(locations[:, 0], locations[:, 1], c=values) pp.xlim(-12, 12) pp.ylim(-12, 12) pp.show()","title":"Gaussian processes in Tensorflow"},{"location":"notebooks/predictions-with-mesh/predictions-with-mesh/","text":"from geostat import GP, Model, Mesh, NormalizingFeaturizer, Parameters import geostat.kernel as krn import matplotlib.pyplot as pp import numpy as np from shapely.geometry import Point, Polygon import shapely.vectorized as shv import geopandas as gpd import contextily as ctx Overview In this notebook we will show how Mesh is used to make prediction locations. We will: * Generate synthetic data on a map of watersheds in Berkeley, California. * Fit a Gaussian Process Model to the data. * Make predictions using Mesh . Synthesizing data We will synthesize data at 200 random locations drawn from inside a polygon for Berkeley watersheds. berkeleydf = gpd.read_file(\"./berkeley-watershed.zip\") berkeley = berkeleydf['geometry'].iloc[0] x0, y0, x1, y1 = berkeley.bounds locs = np.random.uniform(size = [2000, 2]) * [x1-x0, y1-y0] + [x0, y0] # Generate 2000 points. mask = [berkeley.contains(Point(p)) for p in locs] locs = locs[mask, :][:200, :] # Filter away points outside of shape and keep just 200. Declare the terms of the spatial trend: def trend_terms(x, y): return x, y Create a featurizer that the Gaussian process class GP will use to convert locations into trend features: featurizer = NormalizingFeaturizer(trend_terms, locs) Make geostatistical parameters for the GP , instiate the GP , instantiate a Model , and call generate to generate synthetic observations. p = Parameters(alpha=0.25, range=2000, sill=1., nugget=0.25) gp = GP(0, krn.TrendPrior(featurizer, alpha=p.alpha) + \\ krn.SquaredExponential(range=p.range, sill=p.sill) + \\ krn.Noise(nugget=p.nugget)) model = Model(gp) obs = model.generate(locs).vals vmin, vmax = obs.min(), obs.max() When the data is plotted, you can see an overall trend with some localized variations. fig, ax = pp.subplots(figsize=(7, 7), dpi=120) cax = pp.scatter(locs[:, 0], locs[:, 1], c=obs, vmin=vmin, vmax=vmax) berkeleydf.plot(ax=ax, fc='none', ec='black', lw=1) pp.colorbar(cax, shrink=0.7) ctx.add_basemap(ax, attribution=False, crs=berkeleydf.crs.to_string(), source='https://basemap.nationalmap.gov/arcgis/rest/services/' 'USGSImageryOnly/MapServer/tile/{z}/{y}/{x}') pp.title('Synthetic data') pp.tight_layout() pp.show() Inferring parameters Now set the parameters to something arbitrary to see if we can successfully infer model parameters. We call fit with the data ( locs and obs ). model.set(alpha=1.0, range=1000.0, sill=0.5, nugget=0.5) model.fit(locs, obs, iters=300) None [iter 30 ll -194.17 time 1.49 reg 0.00 alpha 1.24 sill 0.42 range 1336.73 nugget 0.37] [iter 60 ll -184.73 time 0.36 reg 0.00 alpha 1.12 sill 0.52 range 1687.33 nugget 0.29] [iter 90 ll -181.58 time 0.35 reg 0.00 alpha 1.01 sill 0.68 range 1964.81 nugget 0.25] [iter 120 ll -180.89 time 0.34 reg 0.00 alpha 0.96 sill 0.85 range 2135.51 nugget 0.24] [iter 150 ll -180.72 time 0.35 reg 0.00 alpha 0.93 sill 0.99 range 2234.71 nugget 0.24] [iter 180 ll -180.67 time 0.35 reg 0.00 alpha 0.92 sill 1.08 range 2291.47 nugget 0.24] [iter 210 ll -180.66 time 0.35 reg 0.00 alpha 0.91 sill 1.13 range 2321.38 nugget 0.24] [iter 240 ll -180.66 time 0.35 reg 0.00 alpha 0.91 sill 1.16 range 2336.16 nugget 0.24] [iter 270 ll -180.65 time 0.34 reg 0.00 alpha 0.90 sill 1.17 range 2342.93 nugget 0.24] [iter 300 ll -180.65 time 0.34 reg 0.00 alpha 0.90 sill 1.18 range 2345.77 nugget 0.24] Generating predictions in convex hull Create a mesh using a convex hull for making predictions. mesh = Mesh.from_convex_hull(locs, nx=200) Call Model to get predictions at mesh locations: mean, var = model.predict(mesh.locations()) Create a slice for prediction mean and plot: meshx, meshy, value = mesh.slice(mean) fig, ax = pp.subplots(figsize=(7, 7), dpi=120) cax = pp.pcolormesh(meshx, meshy, value, vmin=vmin, vmax=vmax) berkeleydf.plot(ax=ax, fc='none', ec='black', lw=1) pp.colorbar(cax, shrink=0.7) ctx.add_basemap(ax, attribution=False, crs=berkeleydf.crs.to_string(), source='https://basemap.nationalmap.gov/arcgis/rest/services/' 'USGSImageryOnly/MapServer/tile/{z}/{y}/{x}') pp.title('Prediction mean') pp.tight_layout() pp.show() Do the same for prediction variance: meshx, meshy, value = mesh.slice(var) fig, ax = pp.subplots(figsize=(7, 7), dpi=120) cax = pp.pcolormesh(meshx, meshy, value, cmap='gist_heat_r') berkeleydf.plot(ax=ax, fc='none', ec='black', lw=1) pp.colorbar(cax, shrink=0.7) ctx.add_basemap(ax, attribution=False, crs=berkeleydf.crs.to_string(), source='https://basemap.nationalmap.gov/arcgis/rest/services/' 'USGSImageryOnly/MapServer/tile/{z}/{y}/{x}') pp.title('Prediction variance') pp.tight_layout() pp.show() Generating predictions in arbitrary shape The convex hull produces predictions both inside and outside the shape for Berkeley watersheds, which can be a bit awkward. Now instead, let's create a mesh using the shape, for making predictions. mesh = Mesh.from_polygon(berkeley, nx=200) Make predictions: mean, var = model.predict(mesh.locations()) Create a slice for prediction mean and plot: meshx, meshy, value = mesh.slice(mean) fig, ax = pp.subplots(figsize=(7, 7), dpi=120) cax = pp.pcolormesh(meshx, meshy, value, vmin=vmin, vmax=vmax) # Add contour value_contains = shv.contains(berkeleydf.geometry.item(), meshx, meshy) value_mask = np.where(value_contains, value, np.nan) pp.contour(meshx, meshy, value_mask, colors='k', linewidths=0.5, alpha=0.8) berkeleydf.plot(ax=ax, fc='none', ec='black', lw=1) pp.colorbar(cax, shrink=0.7) ctx.add_basemap(ax, attribution=False, crs=berkeleydf.crs.to_string(), source='https://basemap.nationalmap.gov/arcgis/rest/services/' 'USGSImageryOnly/MapServer/tile/{z}/{y}/{x}') pp.title('Prediction mean') pp.tight_layout() pp.show() Do the same for prediction variance: meshx, meshy, value = mesh.slice(var) fig, ax = pp.subplots(figsize=(7, 7), dpi=120) cax = pp.pcolormesh(meshx, meshy, value, cmap='gist_heat_r') berkeleydf.plot(ax=ax, fc='none', ec='black', lw=1) pp.colorbar(cax, shrink=0.7) ctx.add_basemap(ax, attribution=False, crs=berkeleydf.crs.to_string(), source='https://basemap.nationalmap.gov/arcgis/rest/services/' 'USGSImageryOnly/MapServer/tile/{z}/{y}/{x}') pp.title('Prediction variance') pp.tight_layout() pp.show()","title":"Predictions with mesh"},{"location":"notebooks/predictions-with-mesh/predictions-with-mesh/#overview","text":"In this notebook we will show how Mesh is used to make prediction locations. We will: * Generate synthetic data on a map of watersheds in Berkeley, California. * Fit a Gaussian Process Model to the data. * Make predictions using Mesh .","title":"Overview"},{"location":"notebooks/predictions-with-mesh/predictions-with-mesh/#synthesizing-data","text":"We will synthesize data at 200 random locations drawn from inside a polygon for Berkeley watersheds. berkeleydf = gpd.read_file(\"./berkeley-watershed.zip\") berkeley = berkeleydf['geometry'].iloc[0] x0, y0, x1, y1 = berkeley.bounds locs = np.random.uniform(size = [2000, 2]) * [x1-x0, y1-y0] + [x0, y0] # Generate 2000 points. mask = [berkeley.contains(Point(p)) for p in locs] locs = locs[mask, :][:200, :] # Filter away points outside of shape and keep just 200. Declare the terms of the spatial trend: def trend_terms(x, y): return x, y Create a featurizer that the Gaussian process class GP will use to convert locations into trend features: featurizer = NormalizingFeaturizer(trend_terms, locs) Make geostatistical parameters for the GP , instiate the GP , instantiate a Model , and call generate to generate synthetic observations. p = Parameters(alpha=0.25, range=2000, sill=1., nugget=0.25) gp = GP(0, krn.TrendPrior(featurizer, alpha=p.alpha) + \\ krn.SquaredExponential(range=p.range, sill=p.sill) + \\ krn.Noise(nugget=p.nugget)) model = Model(gp) obs = model.generate(locs).vals vmin, vmax = obs.min(), obs.max() When the data is plotted, you can see an overall trend with some localized variations. fig, ax = pp.subplots(figsize=(7, 7), dpi=120) cax = pp.scatter(locs[:, 0], locs[:, 1], c=obs, vmin=vmin, vmax=vmax) berkeleydf.plot(ax=ax, fc='none', ec='black', lw=1) pp.colorbar(cax, shrink=0.7) ctx.add_basemap(ax, attribution=False, crs=berkeleydf.crs.to_string(), source='https://basemap.nationalmap.gov/arcgis/rest/services/' 'USGSImageryOnly/MapServer/tile/{z}/{y}/{x}') pp.title('Synthetic data') pp.tight_layout() pp.show()","title":"Synthesizing data"},{"location":"notebooks/predictions-with-mesh/predictions-with-mesh/#inferring-parameters","text":"Now set the parameters to something arbitrary to see if we can successfully infer model parameters. We call fit with the data ( locs and obs ). model.set(alpha=1.0, range=1000.0, sill=0.5, nugget=0.5) model.fit(locs, obs, iters=300) None [iter 30 ll -194.17 time 1.49 reg 0.00 alpha 1.24 sill 0.42 range 1336.73 nugget 0.37] [iter 60 ll -184.73 time 0.36 reg 0.00 alpha 1.12 sill 0.52 range 1687.33 nugget 0.29] [iter 90 ll -181.58 time 0.35 reg 0.00 alpha 1.01 sill 0.68 range 1964.81 nugget 0.25] [iter 120 ll -180.89 time 0.34 reg 0.00 alpha 0.96 sill 0.85 range 2135.51 nugget 0.24] [iter 150 ll -180.72 time 0.35 reg 0.00 alpha 0.93 sill 0.99 range 2234.71 nugget 0.24] [iter 180 ll -180.67 time 0.35 reg 0.00 alpha 0.92 sill 1.08 range 2291.47 nugget 0.24] [iter 210 ll -180.66 time 0.35 reg 0.00 alpha 0.91 sill 1.13 range 2321.38 nugget 0.24] [iter 240 ll -180.66 time 0.35 reg 0.00 alpha 0.91 sill 1.16 range 2336.16 nugget 0.24] [iter 270 ll -180.65 time 0.34 reg 0.00 alpha 0.90 sill 1.17 range 2342.93 nugget 0.24] [iter 300 ll -180.65 time 0.34 reg 0.00 alpha 0.90 sill 1.18 range 2345.77 nugget 0.24]","title":"Inferring parameters"},{"location":"notebooks/predictions-with-mesh/predictions-with-mesh/#generating-predictions-in-convex-hull","text":"Create a mesh using a convex hull for making predictions. mesh = Mesh.from_convex_hull(locs, nx=200) Call Model to get predictions at mesh locations: mean, var = model.predict(mesh.locations()) Create a slice for prediction mean and plot: meshx, meshy, value = mesh.slice(mean) fig, ax = pp.subplots(figsize=(7, 7), dpi=120) cax = pp.pcolormesh(meshx, meshy, value, vmin=vmin, vmax=vmax) berkeleydf.plot(ax=ax, fc='none', ec='black', lw=1) pp.colorbar(cax, shrink=0.7) ctx.add_basemap(ax, attribution=False, crs=berkeleydf.crs.to_string(), source='https://basemap.nationalmap.gov/arcgis/rest/services/' 'USGSImageryOnly/MapServer/tile/{z}/{y}/{x}') pp.title('Prediction mean') pp.tight_layout() pp.show() Do the same for prediction variance: meshx, meshy, value = mesh.slice(var) fig, ax = pp.subplots(figsize=(7, 7), dpi=120) cax = pp.pcolormesh(meshx, meshy, value, cmap='gist_heat_r') berkeleydf.plot(ax=ax, fc='none', ec='black', lw=1) pp.colorbar(cax, shrink=0.7) ctx.add_basemap(ax, attribution=False, crs=berkeleydf.crs.to_string(), source='https://basemap.nationalmap.gov/arcgis/rest/services/' 'USGSImageryOnly/MapServer/tile/{z}/{y}/{x}') pp.title('Prediction variance') pp.tight_layout() pp.show()","title":"Generating predictions in convex hull"},{"location":"notebooks/predictions-with-mesh/predictions-with-mesh/#generating-predictions-in-arbitrary-shape","text":"The convex hull produces predictions both inside and outside the shape for Berkeley watersheds, which can be a bit awkward. Now instead, let's create a mesh using the shape, for making predictions. mesh = Mesh.from_polygon(berkeley, nx=200) Make predictions: mean, var = model.predict(mesh.locations()) Create a slice for prediction mean and plot: meshx, meshy, value = mesh.slice(mean) fig, ax = pp.subplots(figsize=(7, 7), dpi=120) cax = pp.pcolormesh(meshx, meshy, value, vmin=vmin, vmax=vmax) # Add contour value_contains = shv.contains(berkeleydf.geometry.item(), meshx, meshy) value_mask = np.where(value_contains, value, np.nan) pp.contour(meshx, meshy, value_mask, colors='k', linewidths=0.5, alpha=0.8) berkeleydf.plot(ax=ax, fc='none', ec='black', lw=1) pp.colorbar(cax, shrink=0.7) ctx.add_basemap(ax, attribution=False, crs=berkeleydf.crs.to_string(), source='https://basemap.nationalmap.gov/arcgis/rest/services/' 'USGSImageryOnly/MapServer/tile/{z}/{y}/{x}') pp.title('Prediction mean') pp.tight_layout() pp.show() Do the same for prediction variance: meshx, meshy, value = mesh.slice(var) fig, ax = pp.subplots(figsize=(7, 7), dpi=120) cax = pp.pcolormesh(meshx, meshy, value, cmap='gist_heat_r') berkeleydf.plot(ax=ax, fc='none', ec='black', lw=1) pp.colorbar(cax, shrink=0.7) ctx.add_basemap(ax, attribution=False, crs=berkeleydf.crs.to_string(), source='https://basemap.nationalmap.gov/arcgis/rest/services/' 'USGSImageryOnly/MapServer/tile/{z}/{y}/{x}') pp.title('Prediction variance') pp.tight_layout() pp.show()","title":"Generating predictions in arbitrary shape"},{"location":"notebooks/wiener-process/wiener-process/","text":"from geostat import GP, Model, Featurizer, Parameters import geostat.kernel as krn import matplotlib.pyplot as plt import numpy as np x = np.linspace(-8, 8, 321)[:, np.newaxis] locs = np.array([-4, -3, -2, -1, 0, 1, 2, 3, 4], dtype=float)[:, np.newaxis] obs = np.array([-2, -2, -2, -1, 0, 1, 2, 2, 2], dtype=float) featurizer = Featurizer(lambda x: (1., x)) featurizer2 = Featurizer(lambda x: (1.,)) p = Parameters(alpha=0.25) kernel = krn.TrendPrior(featurizer, alpha=p.alpha) + krn.Noise(0.25) model = Model(GP(0, kernel)).fit(locs, obs, iters=500) mu1, _ = model.predict(x) p = Parameters(range=0.33, sill=1.) kernel = krn.SquaredExponential(range=p.range, sill=p.sill) + krn.Noise(0.25) model = Model(GP(0, kernel)).fit(locs, obs, iters=500) mu2, _ = model.predict(x) p = Parameters(alpha=0.25, sill=1.) kernel = krn.TrendPrior(featurizer2, alpha=p.alpha) \\ + krn.Constant(sill=p.sill) \\ * krn.Wiener(axis=0, start=-4) + krn.Noise(nugget=0.25) model = Model(GP(0, kernel)).fit(locs, obs, iters=500) mu3, _ = model.predict(x) [iter 50 ll -9.88 time 0.43 reg 0.00 alpha 0.21] [iter 100 ll -9.88 time 0.23 reg 0.00 alpha 0.21] [iter 150 ll -9.88 time 0.23 reg 0.00 alpha 0.21] [iter 200 ll -9.88 time 0.23 reg 0.00 alpha 0.21] [iter 250 ll -9.88 time 0.23 reg 0.00 alpha 0.21] [iter 300 ll -9.88 time 0.23 reg 0.00 alpha 0.21] [iter 350 ll -9.88 time 0.23 reg 0.00 alpha 0.21] [iter 400 ll -9.88 time 0.23 reg 0.00 alpha 0.21] [iter 450 ll -9.88 time 0.23 reg 0.00 alpha 0.21] [iter 500 ll -9.88 time 0.23 reg 0.00 alpha 0.21] [iter 50 ll -16.37 time 0.50 reg 0.00 sill 1.54 range 0.59] [iter 100 ll -13.03 time 0.21 reg 0.00 sill 1.72 range 1.09] [iter 150 ll -10.63 time 0.21 reg 0.00 sill 1.75 range 1.85] [iter 200 ll -9.75 time 0.21 reg 0.00 sill 1.93 range 2.56] [iter 250 ll -9.57 time 0.21 reg 0.00 sill 2.25 range 2.91] [iter 300 ll -9.52 time 0.21 reg 0.00 sill 2.55 range 3.03] [iter 350 ll -9.50 time 0.20 reg 0.00 sill 2.76 range 3.10] [iter 400 ll -9.50 time 0.21 reg 0.00 sill 2.90 range 3.14] [iter 450 ll -9.50 time 0.20 reg 0.00 sill 2.98 range 3.16] [iter 500 ll -9.50 time 0.21 reg 0.00 sill 3.02 range 3.18] [iter 50 ll -14.39 time 1.96 reg 0.00 alpha 0.41 sill 0.68] [iter 100 ll -13.64 time 0.21 reg 0.00 alpha 0.64 sill 0.65] [iter 150 ll -13.18 time 0.22 reg 0.00 alpha 0.93 sill 0.64] [iter 200 ll -12.91 time 0.22 reg 0.00 alpha 1.25 sill 0.64] [iter 250 ll -12.75 time 0.22 reg 0.00 alpha 1.59 sill 0.64] [iter 300 ll -12.66 time 0.22 reg 0.00 alpha 1.93 sill 0.64] [iter 350 ll -12.61 time 0.22 reg 0.00 alpha 2.24 sill 0.64] [iter 400 ll -12.58 time 0.22 reg 0.00 alpha 2.52 sill 0.64] [iter 450 ll -12.57 time 0.22 reg 0.00 alpha 2.77 sill 0.64] [iter 500 ll -12.56 time 0.22 reg 0.00 alpha 2.98 sill 0.64] plt.scatter(locs[:, 0], obs, marker='o', color='black') plt.plot(x[:, 0], mu1, label='Trend') plt.plot(x[:, 0], mu2, label='Stationary') plt.plot(x[:, 0], mu3, label='Wiener process') plt.legend(title='GP models') <matplotlib.legend.Legend at 0x7f9df0363ee0>","title":"Wiener process"}]}