{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to GeoStat","text":"<p>Model space-time data with Gaussian processes.</p> <p>Geostat makes it easy to write Gaussian Process (GP) models with complex covariance functions. It uses maximum likelihood to fit model parameters. Under the hood it uses Tensorflow to fit models and do inference on GPUs. A good consumer GPU such as an Nvidia RTX 4090 can handle 10k data points.</p> <p>Visit our GitHub repository here.</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Install Geostat using pip:</p> <pre><code>pip install geostat\n</code></pre>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>README.md     # The readme file.\nmkdocs.yml    # The configuration file.\ndoc/\ndocs/\n    about.md  # The about page.\n    index.md  # The documentation homepage.\nsrc/geostat/\n    __init__.py\n    custom_op.py\n    kernel.py\n    krige.py\n    mean.py\n    mesh.py\n    metric.py\n    model.py\n    op.py\n    param.py\ntests/\n</code></pre>"},{"location":"about/","title":"About","text":"<p>Some background about the library and it's intended uses. Mention contributors, licences, and changelog.</p>"},{"location":"about/#disclaimer","title":"Disclaimer","text":"<p>This software is preliminary or provisional and is subject to revision. It is being provided to meet the need for timely best science. The software has not received final approval by the U.S. Geological Survey (USGS). No warranty, expressed or implied, is made by the USGS or the U.S. Government as to the functionality of the software and related material nor shall the fact of release constitute any such warranty. The software is provided on the condition that neither the USGS nor the U.S. Government shall be held liable for any damages resulting from the authorized or unauthorized use of the software.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#src.geostat.model.Featurizer","title":"<code>src.geostat.model.Featurizer</code>","text":"<p>Featurizer class for producing feature matrices (F matrix) from location data.</p> <p>The <code>Featurizer</code> applies a specified featurization function to the input location data  and generates the corresponding feature matrix. If no featurization function is provided,  it produces a matrix with appropriate dimensions containing only ones.</p> <p>Parameters:</p> Name Type Description Default <code>featurization</code> <code>Callable or None</code> <p>A function that takes in the individual components of location data and returns the features. If set to <code>None</code>, the featurizer will produce an empty feature matrix (i.e., only ones).</p> required <p>Examples:</p> <p>Creating a <code>Featurizer</code> using a custom featurization function:</p> <pre><code>import tensorflow as tf\nfrom geostat.model import Featurizer\n\n# Define a custom featurization function\ndef simple_featurizer(x, y):\n    return x, y, x * y\n\n# Initialize the Featurizer\nfeaturizer = Featurizer(simple_featurizer)\n</code></pre> <p>Using the <code>Featurizer</code> to transform location data:</p> <pre><code>locs = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\nF_matrix = featurizer(locs)\nprint(F_matrix) # F_matrix will contain the features: (x, y, x*y) for each location\n# tf.Tensor(\n# [[ 1.  2.  2.]\n#  [ 3.  4. 12.]\n#  [ 5.  6. 30.]], shape=(3, 3), dtype=float32)\n</code></pre> <p>Handling the case where no featurization is provided:</p> <pre><code>featurizer_no_feat = Featurizer(None)\nF_matrix = featurizer_no_feat(locs)\nprint(F_matrix) # Since no featurization function is provided, F_matrix will have shape (3, 0)\n# tf.Tensor([], shape=(3, 0), dtype=float32)\n</code></pre> Notes <ul> <li>The <code>__call__</code> method is used to apply the featurization to input location data.</li> <li>If <code>featurization</code> returns a tuple, it is assumed to represent multiple features,  which will be stacked to form the feature matrix.</li> </ul> Source code in <code>src/geostat/model.py</code> <pre><code>class Featurizer:\n    \"\"\"\n    Featurizer class for producing feature matrices (F matrix) from location data.\n\n    The `Featurizer` applies a specified featurization function to the input location data \n    and generates the corresponding feature matrix. If no featurization function is provided, \n    it produces a matrix with appropriate dimensions containing only ones.\n\n    Parameters:\n        featurization (Callable or None):\n            A function that takes in the individual components of location data and returns the features.\n            If set to `None`, the featurizer will produce an empty feature matrix (i.e., only ones).\n\n    Examples:\n        Creating a `Featurizer` using a custom featurization function:\n\n        ```\n        import tensorflow as tf\n        from geostat.model import Featurizer\n\n        # Define a custom featurization function\n        def simple_featurizer(x, y):\n            return x, y, x * y\n\n        # Initialize the Featurizer\n        featurizer = Featurizer(simple_featurizer)\n        ```\n\n        Using the `Featurizer` to transform location data:\n\n        ```\n        locs = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n        F_matrix = featurizer(locs)\n        print(F_matrix) # F_matrix will contain the features: (x, y, x*y) for each location\n        # tf.Tensor(\n        # [[ 1.  2.  2.]\n        #  [ 3.  4. 12.]\n        #  [ 5.  6. 30.]], shape=(3, 3), dtype=float32)\n        ```\n\n        Handling the case where no featurization is provided:\n\n        ```\n        featurizer_no_feat = Featurizer(None)\n        F_matrix = featurizer_no_feat(locs)\n        print(F_matrix) # Since no featurization function is provided, F_matrix will have shape (3, 0)\n        # tf.Tensor([], shape=(3, 0), dtype=float32)\n        ```\n\n    Notes:\n        - The `__call__` method is used to apply the featurization to input location data.\n        - If `featurization` returns a tuple, it is assumed to represent multiple features, \n        which will be stacked to form the feature matrix.\n    \"\"\"\n\n    def __init__(self, featurization):\n        self.featurization = featurization\n\n    def __call__(self, locs):\n        locs = tf.cast(locs, tf.float32)\n        if self.featurization is None: # No features.\n            return tf.ones([tf.shape(locs)[0], 0], dtype=tf.float32)\n\n        feats = self.featurization(*tf.unstack(locs, axis=1))\n        if isinstance(feats, tuple): # One or many features.\n            if len(feats) == 0:\n                return tf.ones([tf.shape(locs)[0], 0], dtype=tf.float32)\n            else:\n                feats = self.featurization(*tf.unstack(locs, axis=1))\n                feats = [tf.broadcast_to(tf.cast(f, tf.float32), [tf.shape(locs)[0]]) for f in feats]\n                return tf.stack(feats, axis=1)\n        else: # One feature.\n            return e(feats)\n</code></pre>"},{"location":"api/#src.geostat.kernel","title":"<code>src.geostat.kernel</code>","text":""},{"location":"api/#src.geostat.kernel.Constant","title":"<code>Constant</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>Constant kernel class for Gaussian Processes (GPs).</p> <p>The <code>Constant</code> class defines a simple kernel that produces a constant covariance value across all pairs of input locations. This kernel is typically used to represent a baseline level of variance (sill) in the GP model.</p> <p>Parameters:</p> Name Type Description Default <code>sill</code> <code>float or Variable</code> <p>The constant value representing the sill (baseline variance) of the kernel.</p> required <p>Examples:</p> <p>Creating and using a <code>Constant</code> kernel:</p> <pre><code>from geostat import Parameters\nfrom geostat.kernel import Constant\nimport numpy as np\n\n# Create parameters.\np = Parameters(sill=2.0)\n\n# Create a Constant kernel with a sill value of 2.0 and call it\nlocs1 = np.array([[0.0], [1.0], [2.0]])\nlocs2 = np.array([[0.0], [1.0], [2.0]])\nconstant_kernel = Constant(sill=p.sill)\ncovariance_matrix = constant_kernel({'locs1': locs1, 'locs2': locs2, 'sill': 2.0})\nprint(covariance_matrix)\n# tf.Tensor(\n# [[2. 2. 2.]\n#  [2. 2. 2.]\n#  [2. 2. 2.]], shape=(3, 3), dtype=float32)\n</code></pre> Notes <ul> <li>The <code>call</code> method returns the constant value specified by <code>sill</code> for all pairs of input locations.</li> <li>The <code>vars</code> method returns the parameter dictionary for <code>sill</code> using the <code>ppp</code> function.</li> <li>The <code>Constant</code> kernel is useful when you want to add a fixed variance component to your GP model.</li> </ul> Source code in <code>src/geostat/kernel.py</code> <pre><code>class Constant(Kernel):\n    \"\"\"\n    Constant kernel class for Gaussian Processes (GPs).\n\n    The `Constant` class defines a simple kernel that produces a constant covariance value across\n    all pairs of input locations. This kernel is typically used to represent a baseline level of\n    variance (sill) in the GP model.\n\n    Parameters:\n        sill (float or tf.Variable):\n            The constant value representing the sill (baseline variance) of the kernel.\n\n    Examples:\n        Creating and using a `Constant` kernel:\n\n        ```\n        from geostat import Parameters\n        from geostat.kernel import Constant\n        import numpy as np\n\n        # Create parameters.\n        p = Parameters(sill=2.0)\n\n        # Create a Constant kernel with a sill value of 2.0 and call it\n        locs1 = np.array([[0.0], [1.0], [2.0]])\n        locs2 = np.array([[0.0], [1.0], [2.0]])\n        constant_kernel = Constant(sill=p.sill)\n        covariance_matrix = constant_kernel({'locs1': locs1, 'locs2': locs2, 'sill': 2.0})\n        print(covariance_matrix)\n        # tf.Tensor(\n        # [[2. 2. 2.]\n        #  [2. 2. 2.]\n        #  [2. 2. 2.]], shape=(3, 3), dtype=float32)\n        ```\n\n    Notes:\n        - The `call` method returns the constant value specified by `sill` for all pairs of input locations.\n        - The `vars` method returns the parameter dictionary for `sill` using the `ppp` function.\n        - The `Constant` kernel is useful when you want to add a fixed variance component to your GP model.\n    \"\"\"\n\n    def __init__(self, sill):\n        fa = dict(sill=sill)\n        super().__init__(fa, dict())\n\n    def vars(self):\n        return ppp(self.fa['sill'])\n\n    def call(self, e):\n        return e['sill']\n</code></pre>"},{"location":"api/#src.geostat.kernel.Delta","title":"<code>Delta</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>Delta kernel class for Gaussian Processes (GPs).</p> <p>The <code>Delta</code> class defines a kernel that models a Dirac delta function effect, where covariance is non-zero only when the inputs are identical. This kernel is useful for capturing exact matches between input points, weighted by the specified <code>sill</code> parameter.</p> <p>Parameters:</p> Name Type Description Default <code>sill</code> <code>float or Variable</code> <p>The variance (sill) representing the weight of the delta function. This value is applied  when input locations match exactly.</p> required <code>axes</code> <code>list or None</code> <p>A list of axes over which to apply the delta function. If not specified, the delta function  is applied across all axes.</p> <code>None</code> <p>Examples:</p> <p>Creating and using a <code>Delta</code> kernel:</p> <pre><code>from geostat.kernel import Delta\n\n# Create a Delta kernel with a sill of 1.0, applied across all axes\ndelta_kernel = Delta(sill=1.0)\n\nlocs1 = np.array([[0.0], [1.0], [2.0]])\nlocs2 = np.array([[0.0], [1.0], [2.0]])\ncovariance_matrix = delta_kernel({'locs1': locs1, 'locs2': locs2, 'sill': 1.0})\n</code></pre> <p>Using the <code>Delta</code> kernel with specified axes:</p> <pre><code>delta_kernel_axes = Delta(sill=1.0, axes=[0])\n</code></pre> Notes <ul> <li>The <code>call</code> method computes a covariance matrix using a delta function, returning <code>sill</code> when the      squared distances are zero along the specified axes, and 0 otherwise.</li> <li>The <code>vars</code> method returns the parameter dictionary for <code>sill</code> using the <code>ppp</code> function.</li> <li>The <code>Delta</code> kernel is useful for modeling processes that exhibit exact matches or sharp changes      in covariance when inputs coincide, making it ideal for capturing discrete effects.</li> </ul> Source code in <code>src/geostat/kernel.py</code> <pre><code>class Delta(Kernel):\n    \"\"\"\n    Delta kernel class for Gaussian Processes (GPs).\n\n    The `Delta` class defines a kernel that models a Dirac delta function effect, where covariance\n    is non-zero only when the inputs are identical. This kernel is useful for capturing exact matches\n    between input points, weighted by the specified `sill` parameter.\n\n    Parameters:\n        sill (float or tf.Variable):\n            The variance (sill) representing the weight of the delta function. This value is applied \n            when input locations match exactly.\n        axes (list or None, optional):\n            A list of axes over which to apply the delta function. If not specified, the delta function \n            is applied across all axes.\n\n    Examples:\n        Creating and using a `Delta` kernel:\n\n        ```\n        from geostat.kernel import Delta\n\n        # Create a Delta kernel with a sill of 1.0, applied across all axes\n        delta_kernel = Delta(sill=1.0)\n\n        locs1 = np.array([[0.0], [1.0], [2.0]])\n        locs2 = np.array([[0.0], [1.0], [2.0]])\n        covariance_matrix = delta_kernel({'locs1': locs1, 'locs2': locs2, 'sill': 1.0})\n        ```\n\n        Using the `Delta` kernel with specified axes:\n\n        ```\n        delta_kernel_axes = Delta(sill=1.0, axes=[0])\n        ```\n\n    Notes:\n        - The `call` method computes a covariance matrix using a delta function, returning `sill` when the \n            squared distances are zero along the specified axes, and 0 otherwise.\n        - The `vars` method returns the parameter dictionary for `sill` using the `ppp` function.\n        - The `Delta` kernel is useful for modeling processes that exhibit exact matches or sharp changes \n            in covariance when inputs coincide, making it ideal for capturing discrete effects.\n    \"\"\"\n\n    def __init__(self, sill, axes=None):\n        fa = dict(sill=sill)\n        self.axes = axes\n        super().__init__(fa, dict(pa_d2='per_axis_dist2'))\n\n    def vars(self):\n        return ppp(self.fa['sill'])\n\n    def call(self, e):\n\n        if self.axes is not None:\n            n = tf.shape(e['pa_d2'])[-1]\n            mask = tf.math.bincount(self.axes, minlength=n, maxlength=n, dtype=tf.float32)\n            d2 = tf.einsum('abc,c-&gt;ab', e['pa_d2'], mask)\n        else:\n            d2 = tf.reduce_sum(e['pa_d2'], axis=-1)\n\n        return e['sill'] * tf.cast(tf.equal(d2, 0.), tf.float32)\n</code></pre>"},{"location":"api/#src.geostat.kernel.GammaExponential","title":"<code>GammaExponential</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>GammaExponential kernel class for Gaussian Processes (GPs).</p> <p>The <code>GammaExponential</code> class defines a kernel that generalizes the Squared Exponential kernel by introducing a gamma parameter, allowing for greater flexibility in modeling covariance structures. It can capture processes with varying degrees of smoothness, depending on the value of <code>gamma</code>.</p> <p>Parameters:</p> Name Type Description Default <code>range</code> <code>float or Variable</code> <p>The length scale parameter that controls how quickly the covariance decreases with distance.</p> required <code>sill</code> <code>float or Variable</code> <p>The variance (sill) of the kernel, representing the maximum covariance value.</p> required <code>gamma</code> <code>float or Variable</code> <p>The smoothness parameter. A value of 1 results in the standard exponential kernel, while a value of 2  recovers the Squared Exponential kernel. Values between 0 and 2 adjust the smoothness of the kernel.</p> required <code>scale</code> <code>optional</code> <p>An optional scale parameter that can be used to modify the metric. Default is None.</p> <code>None</code> <code>metric</code> <code>optional</code> <p>An optional metric used for distance calculation. Default is None.</p> <code>None</code> <p>Examples:</p> <p>Creating and using a <code>GammaExponential</code> kernel:</p> <pre><code>from geostat.kernel import GammaExponential\n\n# Create a GammaExponential kernel with sill=1.0, range=2.0, and gamma=1.5\ngamma_exp_kernel = GammaExponential(range=2.0, sill=1.0, gamma=1.5)\n\nlocs1 = np.array([[0.0], [1.0], [2.0]])\nlocs2 = np.array([[0.0], [1.0], [2.0]])\ncovariance_matrix = gamma_exp_kernel({'locs1': locs1, 'locs2': locs2, 'sill': 1.0, 'range': 2.0, 'gamma': 1.5})\n</code></pre> Notes <ul> <li>The <code>call</code> method computes the covariance matrix using the gamma-exponential formula:     \\( C(x, x') = \\text{sill} \\cdot \\exp\\left(-\\left(\\frac{d^2}{\\text{range}^2}\\right)^{\\text{gamma} / 2}\\right) \\),     where \\(d^2\\) is the squared distance between <code>locs1</code> and <code>locs2</code>.</li> <li>The <code>vars</code> method returns the parameter dictionary for <code>sill</code>, <code>range</code>, and <code>gamma</code> using the <code>ppp</code> and <code>bpp</code> functions.</li> <li>The <code>GammaExponential</code> kernel provides a more flexible covariance structure than the Squared Exponential kernel,     allowing for varying degrees of smoothness.</li> </ul> Source code in <code>src/geostat/kernel.py</code> <pre><code>class GammaExponential(Kernel):\n    \"\"\"\n    GammaExponential kernel class for Gaussian Processes (GPs).\n\n    The `GammaExponential` class defines a kernel that generalizes the Squared Exponential kernel by introducing\n    a gamma parameter, allowing for greater flexibility in modeling covariance structures. It can capture processes\n    with varying degrees of smoothness, depending on the value of `gamma`.\n\n    Parameters:\n        range (float or tf.Variable):\n            The length scale parameter that controls how quickly the covariance decreases with distance.\n        sill (float or tf.Variable):\n            The variance (sill) of the kernel, representing the maximum covariance value.\n        gamma (float or tf.Variable):\n            The smoothness parameter. A value of 1 results in the standard exponential kernel, while a value of 2 \n            recovers the Squared Exponential kernel. Values between 0 and 2 adjust the smoothness of the kernel.\n        scale (optional):\n            An optional scale parameter that can be used to modify the metric. Default is None.\n        metric (optional):\n            An optional metric used for distance calculation. Default is None.\n\n    Examples:\n        Creating and using a `GammaExponential` kernel:\n\n        ```\n        from geostat.kernel import GammaExponential\n\n        # Create a GammaExponential kernel with sill=1.0, range=2.0, and gamma=1.5\n        gamma_exp_kernel = GammaExponential(range=2.0, sill=1.0, gamma=1.5)\n\n        locs1 = np.array([[0.0], [1.0], [2.0]])\n        locs2 = np.array([[0.0], [1.0], [2.0]])\n        covariance_matrix = gamma_exp_kernel({'locs1': locs1, 'locs2': locs2, 'sill': 1.0, 'range': 2.0, 'gamma': 1.5})\n        ```\n\n    Notes:\n        - The `call` method computes the covariance matrix using the gamma-exponential formula:\n            \\\\( C(x, x') = \\\\text{sill} \\cdot \\exp\\left(-\\left(\\\\frac{d^2}{\\\\text{range}^2}\\\\right)^{\\\\text{gamma} / 2}\\\\right) \\\\),\n            where \\\\(d^2\\\\) is the squared distance between `locs1` and `locs2`.\n        - The `vars` method returns the parameter dictionary for `sill`, `range`, and `gamma` using the `ppp` and `bpp` functions.\n        - The `GammaExponential` kernel provides a more flexible covariance structure than the Squared Exponential kernel,\n            allowing for varying degrees of smoothness.\n    \"\"\"\n\n    def __init__(self, range, sill, gamma, scale=None, metric=None):\n        fa = dict(sill=sill, range=range, gamma=gamma, scale=scale)\n        autoinputs = scale_to_metric(scale, metric)\n        super().__init__(fa, dict(d2=autoinputs))\n\n    def vars(self):\n        return ppp(self.fa['sill']) | ppp(self.fa['range']) | bpp(self.fa['gamma'], 0., 2.)\n\n    def call(self, e):\n        return e['sill'] * gamma_exp(e['d2'] / tf.square(e['range']), e['gamma'])\n</code></pre>"},{"location":"api/#src.geostat.kernel.IntExponential","title":"<code>IntExponential</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>Integrated Exponential (IntExponential) kernel class for Gaussian Processes (GPs).</p> <p>The <code>IntExponential</code> class defines a kernel that integrates the Exponential kernel along a specified axis. This kernel is useful for modeling processes with exponential  decay along one dimension, starting from a given point.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>The axis along which the integration is performed (e.g., 0 for x-axis, 1 for y-axis).</p> required <code>start</code> <code>float</code> <p>The starting point of the integration along the specified axis.</p> required <code>range</code> <code>float or Variable</code> <p>The length scale parameter that controls how quickly the covariance decreases with distance.</p> required <p>Examples:</p> <p>Creating and using an <code>IntExponential</code> kernel:</p> <pre><code>from geostat.kernel import IntExponential\n\n# Create an IntExponential kernel integrating along the x-axis starting from 0.0 with a range of 2.0\nint_exp_kernel = IntExponential(axis=0, start=0.0, range=2.0)\n\nlocs1 = np.array([[0.0], [1.0], [2.0]])\nlocs2 = np.array([[0.0], [1.0], [2.0]])\ncovariance_matrix = int_exp_kernel({'locs1': locs1, 'locs2': locs2, 'range': 2.0})\n</code></pre> Notes <ul> <li>The <code>call</code> method computes the integrated exponential covariance matrix based on the      specified axis, starting point, and range.</li> <li>The <code>vars</code> method returns the parameter dictionary for <code>range</code> using the <code>ppp</code> function.</li> <li>The <code>IntExponential</code> kernel is suitable for modeling processes with exponential decay      along one dimension with integrated covariance structures.</li> </ul> Source code in <code>src/geostat/kernel.py</code> <pre><code>class IntExponential(Kernel):\n    \"\"\"\n    Integrated Exponential (IntExponential) kernel class for Gaussian Processes (GPs).\n\n    The `IntExponential` class defines a kernel that integrates the Exponential kernel\n    along a specified axis. This kernel is useful for modeling processes with exponential \n    decay along one dimension, starting from a given point.\n\n    Parameters:\n        axis (int):\n            The axis along which the integration is performed (e.g., 0 for x-axis, 1 for y-axis).\n        start (float):\n            The starting point of the integration along the specified axis.\n        range (float or tf.Variable):\n            The length scale parameter that controls how quickly the covariance decreases with distance.\n\n    Examples:\n        Creating and using an `IntExponential` kernel:\n\n        ```\n        from geostat.kernel import IntExponential\n\n        # Create an IntExponential kernel integrating along the x-axis starting from 0.0 with a range of 2.0\n        int_exp_kernel = IntExponential(axis=0, start=0.0, range=2.0)\n\n        locs1 = np.array([[0.0], [1.0], [2.0]])\n        locs2 = np.array([[0.0], [1.0], [2.0]])\n        covariance_matrix = int_exp_kernel({'locs1': locs1, 'locs2': locs2, 'range': 2.0})\n        ```\n\n    Notes:\n        - The `call` method computes the integrated exponential covariance matrix based on the \n            specified axis, starting point, and range.\n        - The `vars` method returns the parameter dictionary for `range` using the `ppp` function.\n        - The `IntExponential` kernel is suitable for modeling processes with exponential decay \n            along one dimension with integrated covariance structures.\n    \"\"\"\n\n    def __init__(self, axis, start, range):\n\n        self.axis = axis\n        self.start = start\n\n        # Include the element of scale corresponding to the axis of\n        # integration as an explicit formal argument.\n        fa = dict(range=range)\n\n        super().__init__(fa, dict(locs1='locs1', locs2='locs2'))\n\n    def vars(self):\n        return ppp(self.fa['range'])\n\n    def call(self, e):\n        x1 = tf.pad(e['locs1'][..., self.axis] - self.start, [[1, 0]])\n        x2 = tf.pad(e['locs2'][..., self.axis] - self.start, [[1, 0]])\n\n        r = e['range']\n        sdiff = tf.abs(ed(x1, 1) - ed(x2, 0)) / r\n        k = -tf.square(r) * (sdiff + tf.exp(-sdiff))\n        k -= k[0:1, :]\n        k -= k[:, 0:1]\n        k = k[1:, 1:]\n        k = tf.maximum(0., k)\n\n        return k\n</code></pre>"},{"location":"api/#src.geostat.kernel.IntSquaredExponential","title":"<code>IntSquaredExponential</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>Integrated Squared Exponential (IntSquaredExponential) kernel class for Gaussian Processes (GPs).</p> <p>The <code>IntSquaredExponential</code> class defines a kernel that integrates the Squared Exponential kernel along a specified axis. This kernel is useful for modeling processes with smooth variations along  one dimension, starting from a given point.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>The axis along which the integration is performed (e.g., 0 for x-axis, 1 for y-axis).</p> required <code>start</code> <code>float</code> <p>The starting point of the integration along the specified axis.</p> required <code>range</code> <code>float or Variable</code> <p>The length scale parameter that controls how quickly the covariance decreases with distance.</p> required <p>Examples:</p> <p>Creating and using an <code>IntSquaredExponential</code> kernel:</p> <pre><code>from geostat.kernel import IntSquaredExponential\n\n# Create an IntSquaredExponential kernel integrating along the x-axis starting from 0.0 with a range of 2.0\nint_sq_exp_kernel = IntSquaredExponential(axis=0, start=0.0, range=2.0)\n\nlocs1 = np.array([[0.0], [1.0], [2.0]])\nlocs2 = np.array([[0.0], [1.0], [2.0]])\ncovariance_matrix = int_sq_exp_kernel({'locs1': locs1, 'locs2': locs2, 'range': 2.0})\n</code></pre> Notes <ul> <li>The <code>call</code> method computes the integrated squared exponential covariance matrix based on the      specified axis, starting point, and range.</li> <li>The <code>vars</code> method returns the parameter dictionary for <code>range</code> using the <code>ppp</code> function.</li> <li>The <code>IntSquaredExponential</code> kernel is suitable for modeling smooth processes with integrated      covariance structures along one dimension.</li> </ul> Source code in <code>src/geostat/kernel.py</code> <pre><code>class IntSquaredExponential(Kernel):\n    \"\"\"\n    Integrated Squared Exponential (IntSquaredExponential) kernel class for Gaussian Processes (GPs).\n\n    The `IntSquaredExponential` class defines a kernel that integrates the Squared Exponential kernel\n    along a specified axis. This kernel is useful for modeling processes with smooth variations along \n    one dimension, starting from a given point.\n\n    Parameters:\n        axis (int):\n            The axis along which the integration is performed (e.g., 0 for x-axis, 1 for y-axis).\n        start (float):\n            The starting point of the integration along the specified axis.\n        range (float or tf.Variable):\n            The length scale parameter that controls how quickly the covariance decreases with distance.\n\n    Examples:\n        Creating and using an `IntSquaredExponential` kernel:\n\n        ```\n        from geostat.kernel import IntSquaredExponential\n\n        # Create an IntSquaredExponential kernel integrating along the x-axis starting from 0.0 with a range of 2.0\n        int_sq_exp_kernel = IntSquaredExponential(axis=0, start=0.0, range=2.0)\n\n        locs1 = np.array([[0.0], [1.0], [2.0]])\n        locs2 = np.array([[0.0], [1.0], [2.0]])\n        covariance_matrix = int_sq_exp_kernel({'locs1': locs1, 'locs2': locs2, 'range': 2.0})\n        ```\n\n    Notes:\n        - The `call` method computes the integrated squared exponential covariance matrix based on the \n            specified axis, starting point, and range.\n        - The `vars` method returns the parameter dictionary for `range` using the `ppp` function.\n        - The `IntSquaredExponential` kernel is suitable for modeling smooth processes with integrated \n            covariance structures along one dimension.\n    \"\"\"\n\n    def __init__(self, axis, start, range):\n\n        self.axis = axis\n        self.start = start\n\n        # Include the element of scale corresponding to the axis of\n        # integration as an explicit formal argument.\n        fa = dict(range=range)\n\n        super().__init__(fa, dict(locs1='locs1', locs2='locs2'))\n\n    def vars(self):\n        return ppp(self.fa['range'])\n\n    def call(self, e):\n        x1 = tf.pad(e['locs1'][..., self.axis] - self.start, [[1, 0]])\n        x2 = tf.pad(e['locs2'][..., self.axis] - self.start, [[1, 0]])\n\n        r = e['range']\n        sdiff = (ed(x1, 1) - ed(x2, 0)) / (r * np.sqrt(2.))\n        k = -tf.square(r) * (np.sqrt(np.pi) * sdiff * tf.math.erf(sdiff) + tf.exp(-tf.square(sdiff)))\n        k -= k[0:1, :]\n        k -= k[:, 0:1]\n        k = k[1:, 1:]\n        k = tf.maximum(0., k)\n\n        return k\n</code></pre>"},{"location":"api/#src.geostat.kernel.Kernel","title":"<code>Kernel</code>","text":"<p>               Bases: <code>Op</code></p> <p>Kernel class representing a covariance function for Gaussian Processes (GPs).</p> <p>The <code>Kernel</code> class defines the structure of a GP's covariance function. It supports operations such as addition and multiplication with other kernels, enabling the construction of more complex kernels through combinations. The class also provides methods for computing the covariance matrix between sets of locations.</p> <p>Parameters:</p> Name Type Description Default <code>fa</code> <code>dict or callable</code> <p>A dictionary or callable representing the functional attributes of the kernel.</p> required <code>autoinputs</code> <code>dict</code> <p>A dictionary specifying the automatic input mappings for the kernel. If 'offset', 'locs1', or 'locs2' keys are not present, they are added with default values.</p> required <p>Examples:</p> <p>Creating and using a <code>Kernel</code> object:</p> <pre><code>from geostat.kernel import Kernel\nimport numpy as np\n\n# Construct kernel and call it on locations\nlocs1 = np.array([[0.0, 0.0], [1.0, 1.0]])\nlocs2 = np.array([[2.0, 2.0], [3.0, 3.0]])\nkernel = Kernel(fa={'alpha': 1.0}, autoinputs={})\ncovariance_matrix = kernel({'locs1': locs1, 'locs2': locs2})\nprint(covariance_matrix) # Covariance matrix only has zero entries as no kernel function was given\n# tf.Tensor(\n# [[0. 0.]\n#  [0. 0.]], shape=(2, 2), dtype=float32)\n</code></pre> <p>Combining two kernels using addition and multiplication:</p> <pre><code>kernel1 = Kernel(fa={'alpha': 1.0}, autoinputs={})\nkernel2 = Kernel(fa={'range': 0.5}, autoinputs={})\ncombined_kernel = kernel1 + kernel2  # Adding kernels\nproduct_kernel = kernel1 * kernel2   # Multiplying kernels\n</code></pre> Notes <ul> <li>The <code>__call__</code> method computes the covariance matrix between two sets of locations      (<code>locs1</code> and <code>locs2</code>) and ensures the result is correctly broadcasted to the appropriate shape.</li> <li>The <code>report</code> method provides a summary of the kernel's parameters and their values.</li> <li>This class serves as a base class for more specialized kernel functions in GP modeling.</li> </ul> Source code in <code>src/geostat/kernel.py</code> <pre><code>class Kernel(Op):\n    \"\"\"\n    Kernel class representing a covariance function for Gaussian Processes (GPs).\n\n    The `Kernel` class defines the structure of a GP's covariance function. It supports operations\n    such as addition and multiplication with other kernels, enabling the construction of more\n    complex kernels through combinations. The class also provides methods for computing the\n    covariance matrix between sets of locations.\n\n    Parameters:\n        fa (dict or callable):\n            A dictionary or callable representing the functional attributes of the kernel.\n        autoinputs (dict):\n            A dictionary specifying the automatic input mappings for the kernel. If 'offset',\n            'locs1', or 'locs2' keys are not present, they are added with default values.\n\n    Examples:\n        Creating and using a `Kernel` object:\n\n        ```\n        from geostat.kernel import Kernel\n        import numpy as np\n\n        # Construct kernel and call it on locations\n        locs1 = np.array([[0.0, 0.0], [1.0, 1.0]])\n        locs2 = np.array([[2.0, 2.0], [3.0, 3.0]])\n        kernel = Kernel(fa={'alpha': 1.0}, autoinputs={})\n        covariance_matrix = kernel({'locs1': locs1, 'locs2': locs2})\n        print(covariance_matrix) # Covariance matrix only has zero entries as no kernel function was given\n        # tf.Tensor(\n        # [[0. 0.]\n        #  [0. 0.]], shape=(2, 2), dtype=float32)\n        ```\n\n        Combining two kernels using addition and multiplication:\n\n        ```\n        kernel1 = Kernel(fa={'alpha': 1.0}, autoinputs={})\n        kernel2 = Kernel(fa={'range': 0.5}, autoinputs={})\n        combined_kernel = kernel1 + kernel2  # Adding kernels\n        product_kernel = kernel1 * kernel2   # Multiplying kernels\n        ```\n\n    Notes:\n        - The `__call__` method computes the covariance matrix between two sets of locations \n            (`locs1` and `locs2`) and ensures the result is correctly broadcasted to the appropriate shape.\n        - The `report` method provides a summary of the kernel's parameters and their values.\n        - This class serves as a base class for more specialized kernel functions in GP modeling.\n    \"\"\"\n\n    def __init__(self, fa, autoinputs):\n        if 'offset' not in autoinputs: autoinputs['offset'] = 'offset'\n        if 'locs1' not in autoinputs: autoinputs['locs1'] = 'locs1'\n        if 'locs2' not in autoinputs: autoinputs['locs2'] = 'locs2'\n        super().__init__(fa, autoinputs)\n\n    def __add__(self, other):\n        if other is None:\n            return self\n        else:\n            return Stack([self]) + other\n\n    def __mul__(self, other):\n        return Product([self]) * other\n\n    def call(self, e):\n        \"\"\"\n        Returns covariance for locations.\n        Return values may be unbroadcasted.\n        \"\"\"\n        pass\n\n    def __call__(self, e):\n        \"\"\"\n        Returns covariance for locations.\n        Return values have correct shapes.\n        \"\"\"\n        C = self.call(e)\n        if C is None: C = 0.\n        n1 = tf.shape(e['locs1'])[0]\n        n2 = tf.shape(e['locs2'])[0]\n        C = tf.broadcast_to(C, [n1, n2])\n        return C\n\n    def report(self):\n        string = ', '.join('%s %4.2f' % (v.name, p[v.name]) for v in self.vars())\n        return '[' + string + ']'\n</code></pre>"},{"location":"api/#src.geostat.kernel.Mix","title":"<code>Mix</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>Mix kernel class for combining multiple Gaussian Process (GP) kernels.</p> <p>The <code>Mix</code> class defines a kernel that allows combining multiple input kernels, either using  specified weights or by directly mixing the component kernels. This provides a flexible way to  create complex covariance structures by blending the properties of different kernels.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>list of Kernel objects</code> <p>A list of kernel objects to be combined.</p> required <code>weights</code> <code>matrix</code> <p>A matrix specifying how the input kernels should be combined. If not provided,  the kernels are combined without weighting.</p> <code>None</code> <p>Examples:</p> <p>Combining multiple kernels with specified weights:</p> <pre><code>from geostat.kernel import Mix, SquaredExponential, Noise\n\n# Create individual kernels\nkernel1 = SquaredExponential(sill=1.0, range=2.0)\nkernel2 = Noise(nugget=0.1)\n\n# Combine kernels using the Mix class\nmixed_kernel = Mix(inputs=[kernel1, kernel2], weights=[[0.6, 0.4], [0.4, 0.6]])\n\nlocs1 = np.array([[0.0], [1.0], [2.0]])\nlocs2 = np.array([[0.0], [1.0], [2.0]])\ncovariance_matrix = mixed_kernel({'locs1': locs1, 'locs2': locs2, 'weights': [[0.6, 0.4], [0.4, 0.6]]})\n</code></pre> <p>Using the <code>Mix</code> kernel without weights:</p> <pre><code>mixed_kernel_no_weights = Mix(inputs=[kernel1, kernel2])\n</code></pre> Notes <ul> <li>The <code>call</code> method computes the covariance matrix by either using the specified weights      to combine the input kernels or directly combining them when weights are not provided.</li> <li>The <code>vars</code> method gathers the parameters from all input kernels, allowing for easy      access and manipulation of their coefficients.</li> <li>The <code>Mix</code> kernel is useful for creating complex, multi-faceted covariance structures      by blending different types of kernels, providing enhanced modeling flexibility.</li> </ul> Source code in <code>src/geostat/kernel.py</code> <pre><code>class Mix(Kernel):\n    \"\"\"\n    Mix kernel class for combining multiple Gaussian Process (GP) kernels.\n\n    The `Mix` class defines a kernel that allows combining multiple input kernels, either using \n    specified weights or by directly mixing the component kernels. This provides a flexible way to \n    create complex covariance structures by blending the properties of different kernels.\n\n    Parameters:\n        inputs (list of Kernel objects):\n            A list of kernel objects to be combined.\n        weights (matrix, optional):\n            A matrix specifying how the input kernels should be combined. If not provided, \n            the kernels are combined without weighting.\n\n    Examples:\n        Combining multiple kernels with specified weights:\n\n        ```\n        from geostat.kernel import Mix, SquaredExponential, Noise\n\n        # Create individual kernels\n        kernel1 = SquaredExponential(sill=1.0, range=2.0)\n        kernel2 = Noise(nugget=0.1)\n\n        # Combine kernels using the Mix class\n        mixed_kernel = Mix(inputs=[kernel1, kernel2], weights=[[0.6, 0.4], [0.4, 0.6]])\n\n        locs1 = np.array([[0.0], [1.0], [2.0]])\n        locs2 = np.array([[0.0], [1.0], [2.0]])\n        covariance_matrix = mixed_kernel({'locs1': locs1, 'locs2': locs2, 'weights': [[0.6, 0.4], [0.4, 0.6]]})\n        ```\n\n        Using the `Mix` kernel without weights:\n\n        ```\n        mixed_kernel_no_weights = Mix(inputs=[kernel1, kernel2])\n        ```\n\n    Notes:\n        - The `call` method computes the covariance matrix by either using the specified weights \n            to combine the input kernels or directly combining them when weights are not provided.\n        - The `vars` method gathers the parameters from all input kernels, allowing for easy \n            access and manipulation of their coefficients.\n        - The `Mix` kernel is useful for creating complex, multi-faceted covariance structures \n            by blending different types of kernels, providing enhanced modeling flexibility.\n    \"\"\"\n\n    def __init__(self, inputs, weights=None):\n        self.inputs = inputs\n        fa = {}\n        ai = dict(cats1='cats1', cats2='cats2')\n\n        # Special case if weights is not given.\n        if weights is not None:\n            fa['weights'] = weights\n            ai['inputs'] = inputs\n\n        super().__init__(fa, ai)\n\n    def gather_vars(self, cache=None):\n        \"\"\"Make a special version of gather_vars because\n           we want to gather variables from `inputs`\n           even when it's not in autoinputs\"\"\"\n        vv = super().gather_vars(cache)\n        for iput in self.inputs:\n            cache[id(self)] |= iput.gather_vars(cache)\n        return cache[id(self)]\n\n    def vars(self):\n        if 'weights' in self.fa:\n            return {k: p for row in self.fa['weights']\n                      for k, p in get_trend_coefs(row).items()}\n        else:\n            return {}\n\n    def call(self, e):\n        if 'weights' in e:\n            weights = []\n            for row in e['weights']:\n                if isinstance(row, (tuple, list)):\n                    row = tf.stack(row)\n                    weights.append(row)\n            weights = tf.stack(weights)\n            C = tf.stack(e['inputs'], axis=-1) # [locs, locs, numinputs].\n            Aaug1 = tf.gather(weights, e['cats1']) # [locs, numinputs].\n            Aaug2 = tf.gather(weights, e['cats2']) # [locs, numinputs].\n            outer = tf.einsum('ac,bc-&gt;abc', Aaug1, Aaug2) # [locs, locs, numinputs].\n            C = tf.einsum('abc,abc-&gt;ab', C, outer) # [locs, locs].\n            return C\n        else:\n            # When weights is not given, exploit the fact that we don't have\n            # to compute every element in component covariance matrices.\n            N = len(self.inputs)\n            catcounts1 = tf.math.bincount(e['cats1'], minlength=N, maxlength=N)\n            catcounts2 = tf.math.bincount(e['cats2'], minlength=N, maxlength=N)\n            catindices1 = tf.math.cumsum(catcounts1, exclusive=True)\n            catindices2 = tf.math.cumsum(catcounts2, exclusive=True)\n            catdiffs = tf.unstack(catindices2 - catindices1, num=N)\n            locsegs1 = tf.split(e['locs1'], catcounts1, num=N)\n            locsegs2 = tf.split(e['locs2'], catcounts2, num=N)\n\n            # TODO: Check that the below is still correct.\n            CC = [] # Observation noise submatrices.\n            for sublocs1, sublocs2, catdiff, iput in zip(locsegs1, locsegs2, catdiffs, self.inputs):\n                cache = dict(\n                    offset = e['offset'] + catdiff,\n                    locs1 = sublocs1,\n                    locs2 = sublocs2)\n                cache['per_axis_dist2'] = PerAxisDist2().run(cache)\n                cache['euclidean'] = Euclidean().run(cache)\n                Csub = iput.run(cache)\n                CC.append(Csub)\n\n            return block_diag(CC)\n</code></pre>"},{"location":"api/#src.geostat.kernel.Noise","title":"<code>Noise</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>Noise kernel class for Gaussian Processes (GPs).</p> <p>The <code>Noise</code> class defines a kernel that models the nugget effect, which represents uncorrelated noise in the data. It produces a diagonal covariance matrix with the specified <code>nugget</code> value, indicating  the presence of noise at each location.</p> <p>Parameters:</p> Name Type Description Default <code>nugget</code> <code>float or Variable</code> <p>The variance (nugget) representing the noise level. This value is added to the diagonal  of the covariance matrix.</p> required <p>Examples:</p> <p>Creating and using a <code>Noise</code> kernel:</p> <pre><code>from geostat.kernel import Noise\n\n# Create a Noise kernel with a nugget value of 0.1\nnoise_kernel = Noise(nugget=0.1)\n\nlocs1 = np.array([[0.0], [1.0], [2.0]])\nlocs2 = np.array([[0.0], [1.0], [2.0]])\ncovariance_matrix = noise_kernel({'locs1': locs1, 'locs2': locs2, 'nugget': 0.1})\n</code></pre> Notes <ul> <li>The <code>call</code> method computes a diagonal covariance matrix where the diagonal elements are equal      to <code>nugget</code>, representing noise at each location. Off-diagonal elements are set to 0.</li> <li>The <code>vars</code> method returns the parameter dictionary for <code>nugget</code> using the <code>ppp</code> function.</li> <li>The <code>Noise</code> kernel is useful for modeling independent noise in the data, especially when the      observations contain measurement error or variability that cannot be explained by the model.</li> </ul> Source code in <code>src/geostat/kernel.py</code> <pre><code>class Noise(Kernel):\n    \"\"\"\n    Noise kernel class for Gaussian Processes (GPs).\n\n    The `Noise` class defines a kernel that models the nugget effect, which represents uncorrelated noise\n    in the data. It produces a diagonal covariance matrix with the specified `nugget` value, indicating \n    the presence of noise at each location.\n\n    Parameters:\n        nugget (float or tf.Variable):\n            The variance (nugget) representing the noise level. This value is added to the diagonal \n            of the covariance matrix.\n\n    Examples:\n        Creating and using a `Noise` kernel:\n\n        ```\n        from geostat.kernel import Noise\n\n        # Create a Noise kernel with a nugget value of 0.1\n        noise_kernel = Noise(nugget=0.1)\n\n        locs1 = np.array([[0.0], [1.0], [2.0]])\n        locs2 = np.array([[0.0], [1.0], [2.0]])\n        covariance_matrix = noise_kernel({'locs1': locs1, 'locs2': locs2, 'nugget': 0.1})\n        ```\n\n    Notes:\n        - The `call` method computes a diagonal covariance matrix where the diagonal elements are equal \n            to `nugget`, representing noise at each location. Off-diagonal elements are set to 0.\n        - The `vars` method returns the parameter dictionary for `nugget` using the `ppp` function.\n        - The `Noise` kernel is useful for modeling independent noise in the data, especially when the \n            observations contain measurement error or variability that cannot be explained by the model.\n    \"\"\"\n\n    def __init__(self, nugget):\n        fa = dict(nugget=nugget)\n        super().__init__(fa, dict(locs1='locs1', locs2='locs2'))\n\n    def vars(self):\n        return ppp(self.fa['nugget'])\n\n    def call(self, e):\n\n        indices1 = tf.range(tf.shape(e['locs1'])[0])\n        indices2 = tf.range(tf.shape(e['locs2'])[0]) + e['offset']\n        C = tf.where(tf.equal(tf.expand_dims(indices1, -1), indices2), e['nugget'], 0.)\n        return C\n</code></pre>"},{"location":"api/#src.geostat.kernel.Product","title":"<code>Product</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>Product kernel class for combining multiple Gaussian Process (GP) kernels multiplicatively.</p> <p>The <code>Product</code> class defines a kernel that combines multiple input kernels by multiplying them together. This multiplicative combination allows for capturing interactions between the individual kernels, resulting in a more complex and flexible covariance structure.</p> <p>Parameters:</p> Name Type Description Default <code>parts</code> <code>List[Kernel]</code> <p>A list of kernel objects to be combined multiplicatively.</p> required <p>Examples:</p> <p>Creating and using a <code>Product</code> kernel:</p> <pre><code>from geostat.kernel import Product, SquaredExponential, Noise\n\n# Create individual kernels\nkernel1 = SquaredExponential(sill=1.0, range=2.0)\nkernel2 = Noise(nugget=0.1)\n\n# Combine kernels using the Product class\nproduct_kernel = Product(parts=[kernel1, kernel2])\n\nlocs1 = np.array([[0.0], [1.0], [2.0]])\nlocs2 = np.array([[0.0], [1.0], [2.0]])\ncovariance_matrix = product_kernel({'locs1': locs1, 'locs2': locs2})\n</code></pre> <p>Multiplying another kernel with an existing <code>Product</code> kernel:</p> <pre><code>kernel3 = SquaredExponential(sill=0.5, range=1.0)\ncombined_product = product_kernel * kernel3\n</code></pre> Notes <ul> <li>The <code>call</code> method computes the product of all covariance matrices generated by the multiplied kernels.</li> <li>The <code>vars</code> method gathers parameters from all input kernels, making them accessible for optimization.</li> <li>The <code>Product</code> kernel is useful for building models where the covariance structure results from      multiplicative interactions between different kernels, allowing for more complex GP models.</li> </ul> Source code in <code>src/geostat/kernel.py</code> <pre><code>class Product(Kernel):\n    \"\"\"\n    Product kernel class for combining multiple Gaussian Process (GP) kernels multiplicatively.\n\n    The `Product` class defines a kernel that combines multiple input kernels by multiplying them together.\n    This multiplicative combination allows for capturing interactions between the individual kernels, resulting\n    in a more complex and flexible covariance structure.\n\n    Parameters:\n        parts (List[Kernel]):\n            A list of kernel objects to be combined multiplicatively.\n\n    Examples:\n        Creating and using a `Product` kernel:\n\n        ```\n        from geostat.kernel import Product, SquaredExponential, Noise\n\n        # Create individual kernels\n        kernel1 = SquaredExponential(sill=1.0, range=2.0)\n        kernel2 = Noise(nugget=0.1)\n\n        # Combine kernels using the Product class\n        product_kernel = Product(parts=[kernel1, kernel2])\n\n        locs1 = np.array([[0.0], [1.0], [2.0]])\n        locs2 = np.array([[0.0], [1.0], [2.0]])\n        covariance_matrix = product_kernel({'locs1': locs1, 'locs2': locs2})\n        ```\n\n        Multiplying another kernel with an existing `Product` kernel:\n\n        ```\n        kernel3 = SquaredExponential(sill=0.5, range=1.0)\n        combined_product = product_kernel * kernel3\n        ```\n\n    Notes:\n        - The `call` method computes the product of all covariance matrices generated by the multiplied kernels.\n        - The `vars` method gathers parameters from all input kernels, making them accessible for optimization.\n        - The `Product` kernel is useful for building models where the covariance structure results from \n            multiplicative interactions between different kernels, allowing for more complex GP models.\n    \"\"\"\n\n    def __init__(self, parts: List[Kernel]):\n        self.parts = parts\n        super().__init__({}, dict(locs1='locs1', locs2='locs2', parts=parts))\n\n    def vars(self):\n        return {k: p for part in self.parts for k, p in part.vars().items()}\n\n    def __mul__(self, other):\n        if isinstance(other, Kernel):\n            return Product(self.parts + [other])\n\n    def call(self, e):\n        return tf.reduce_prod(e['parts'], axis=0)\n\n    def report(self):\n        return ' '.join(part.report(p) for part in self.parts)\n</code></pre>"},{"location":"api/#src.geostat.kernel.QuadStack","title":"<code>QuadStack</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>QuadStack kernel class for Gaussian Processes (GPs).</p> <p>The <code>QuadStack</code> class defines a kernel that combines multiple quadratic components to model complex  covariance structures. It allows for multiple sills and ranges, providing flexibility in capturing  covariance that varies across different scales.</p> <p>Parameters:</p> Name Type Description Default <code>range</code> <code>list or Variable</code> <p>A list or TensorFlow variable representing the length scale parameters that control how  quickly the covariance decreases with distance.</p> required <code>sill</code> <code>list or Variable</code> <p>A list or TensorFlow variable representing the variance (sill) values for each quadratic component.</p> required <code>scale</code> <code>optional</code> <p>An optional scale parameter that can be used to modify the metric. Default is None.</p> <code>None</code> <code>metric</code> <code>optional</code> <p>An optional metric used for distance calculation. Default is None.</p> <code>None</code> <p>Examples:</p> <p>Creating and using a <code>QuadStack</code> kernel:</p> <pre><code>from geostat.kernel import QuadStack\n\n# Create a QuadStack kernel with multiple sills and ranges\nquad_stack_kernel = QuadStack(range=[2.0, 3.0], sill=[1.0, 0.5])\n\nlocs1 = np.array([[0.0], [1.0], [2.0]])\nlocs2 = np.array([[0.0], [1.0], [2.0]])\ncovariance_matrix = quad_stack_kernel({'locs1': locs1, 'locs2': locs2, 'sill': [1.0, 0.5], 'range': [2.0, 3.0]})\n</code></pre> Notes <ul> <li>The <code>call</code> method computes the covariance matrix using the <code>quadstack</code> function, which applies      multiple quadratic functions based on the provided <code>sill</code> and <code>range</code> values for each component.</li> <li>The <code>vars</code> method returns the parameter dictionary for both <code>sill</code> and <code>range</code> using the <code>ppp_list</code> function.</li> <li>The <code>QuadStack</code> kernel is useful for modeling processes that exhibit multiple levels of variability      or changes in smoothness at different scales with a quadratic structure.</li> </ul> Source code in <code>src/geostat/kernel.py</code> <pre><code>class QuadStack(Kernel):\n    \"\"\"\n    QuadStack kernel class for Gaussian Processes (GPs).\n\n    The `QuadStack` class defines a kernel that combines multiple quadratic components to model complex \n    covariance structures. It allows for multiple sills and ranges, providing flexibility in capturing \n    covariance that varies across different scales.\n\n    Parameters:\n        range (list or tf.Variable):\n            A list or TensorFlow variable representing the length scale parameters that control how \n            quickly the covariance decreases with distance.\n        sill (list or tf.Variable):\n            A list or TensorFlow variable representing the variance (sill) values for each quadratic component.\n        scale (optional):\n            An optional scale parameter that can be used to modify the metric. Default is None.\n        metric (optional):\n            An optional metric used for distance calculation. Default is None.\n\n    Examples:\n        Creating and using a `QuadStack` kernel:\n\n        ```\n        from geostat.kernel import QuadStack\n\n        # Create a QuadStack kernel with multiple sills and ranges\n        quad_stack_kernel = QuadStack(range=[2.0, 3.0], sill=[1.0, 0.5])\n\n        locs1 = np.array([[0.0], [1.0], [2.0]])\n        locs2 = np.array([[0.0], [1.0], [2.0]])\n        covariance_matrix = quad_stack_kernel({'locs1': locs1, 'locs2': locs2, 'sill': [1.0, 0.5], 'range': [2.0, 3.0]})\n        ```\n\n    Notes:\n        - The `call` method computes the covariance matrix using the `quadstack` function, which applies \n            multiple quadratic functions based on the provided `sill` and `range` values for each component.\n        - The `vars` method returns the parameter dictionary for both `sill` and `range` using the `ppp_list` function.\n        - The `QuadStack` kernel is useful for modeling processes that exhibit multiple levels of variability \n            or changes in smoothness at different scales with a quadratic structure.\n    \"\"\"\n\n    def __init__(self, range, sill, scale=None, metric=None):\n        fa = dict(sill=sill, range=range, scale=scale)\n        autoinputs = scale_to_metric(scale, metric)\n        super().__init__(fa, dict(d2=autoinputs))\n\n    def vars(self):\n        return ppp_list(self.fa['sill']) | ppp_list(self.fa['range'])\n\n    def call(self, e):\n        if isinstance(e['sill'], (tuple, list)):\n            e['sill'] = tf.stack(e['sill'])\n        if isinstance(e['range'], (tuple, list)):\n            e['range'] = tf.stack(e['range'])\n\n        return quadstack(tf.sqrt(e['d2']), e['sill'], e['range'])\n</code></pre>"},{"location":"api/#src.geostat.kernel.Ramp","title":"<code>Ramp</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>Ramp kernel class for Gaussian Processes (GPs).</p> <p>The <code>Ramp</code> class defines a kernel that produces a covariance structure resembling a \"ramp\" function. It is characterized by a sill (variance) and a range (length scale) and can optionally use a metric for scaling.</p> <p>Parameters:</p> Name Type Description Default <code>range</code> <code>float or Variable</code> <p>The length scale parameter that controls how quickly the covariance decreases with distance.</p> required <code>sill</code> <code>float or Variable</code> <p>The variance (sill) of the kernel, representing the maximum covariance value.</p> required <code>scale</code> <code>optional</code> <p>An optional scale parameter that can be used to modify the metric. Default is None.</p> <code>None</code> <code>metric</code> <code>optional</code> <p>An optional metric used for distance calculation. Default is None.</p> <code>None</code> <p>Examples:</p> <p>Creating and using a <code>Ramp</code> kernel:</p> <pre><code>from geostat.kernel import Ramp\n\n# Create a Ramp kernel with sill=1.0 and range=2.0\nramp_kernel = Ramp(range=2.0, sill=1.0)\n\nlocs1 = np.array([[0.0], [1.0], [2.0]])\nlocs2 = np.array([[0.0], [1.0], [2.0]])\ncovariance_matrix = ramp_kernel({'locs1': locs1, 'locs2': locs2, 'sill': 1.0, 'range': 2.0})\n</code></pre> Notes <ul> <li>The <code>call</code> method computes the covariance matrix using the ramp function:     \\( C(x, x') = \\text{sill} \\cdot \\text{ramp}\\left(\\frac{\\sqrt{d^2}}{\\text{range}}\\right) \\),     where \\(d^2\\) is the squared distance between <code>locs1</code> and <code>locs2</code>.</li> <li>The <code>vars</code> method returns the parameter dictionary for both <code>sill</code> and <code>range</code> using the <code>ppp</code> function.</li> <li>The <code>Ramp</code> kernel can be used in cases where the covariance structure exhibits a linear decay with increasing distance.</li> </ul> Source code in <code>src/geostat/kernel.py</code> <pre><code>class Ramp(Kernel):\n    \"\"\"\n    Ramp kernel class for Gaussian Processes (GPs).\n\n    The `Ramp` class defines a kernel that produces a covariance structure resembling a \"ramp\" function.\n    It is characterized by a sill (variance) and a range (length scale) and can optionally use a metric for scaling.\n\n    Parameters:\n        range (float or tf.Variable):\n            The length scale parameter that controls how quickly the covariance decreases with distance.\n        sill (float or tf.Variable):\n            The variance (sill) of the kernel, representing the maximum covariance value.\n        scale (optional):\n            An optional scale parameter that can be used to modify the metric. Default is None.\n        metric (optional):\n            An optional metric used for distance calculation. Default is None.\n\n    Examples:\n        Creating and using a `Ramp` kernel:\n\n        ```\n        from geostat.kernel import Ramp\n\n        # Create a Ramp kernel with sill=1.0 and range=2.0\n        ramp_kernel = Ramp(range=2.0, sill=1.0)\n\n        locs1 = np.array([[0.0], [1.0], [2.0]])\n        locs2 = np.array([[0.0], [1.0], [2.0]])\n        covariance_matrix = ramp_kernel({'locs1': locs1, 'locs2': locs2, 'sill': 1.0, 'range': 2.0})\n        ```\n\n    Notes:\n        - The `call` method computes the covariance matrix using the ramp function:\n            \\\\( C(x, x') = \\\\text{sill} \\cdot \\\\text{ramp}\\left(\\\\frac{\\sqrt{d^2}}{\\\\text{range}}\\\\right) \\\\),\n            where \\\\(d^2\\\\) is the squared distance between `locs1` and `locs2`.\n        - The `vars` method returns the parameter dictionary for both `sill` and `range` using the `ppp` function.\n        - The `Ramp` kernel can be used in cases where the covariance structure exhibits a linear decay with increasing distance.\n    \"\"\"\n\n    def __init__(self, range, sill, scale=None, metric=None):\n        fa = dict(sill=sill, range=range, scale=scale)\n        autoinputs = scale_to_metric(scale, metric)\n        super().__init__(fa, dict(d2=autoinputs))\n\n    def vars(self):\n        return ppp(self.fa['sill']) | ppp(self.fa['range'])\n\n    def call(self, e):\n        return e['sill'] * ramp(tf.sqrt(e['d2']) / e['range'])\n</code></pre>"},{"location":"api/#src.geostat.kernel.RampStack","title":"<code>RampStack</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>RampStack kernel class for Gaussian Processes (GPs).</p> <p>The <code>RampStack</code> class defines a kernel that extends the standard <code>Ramp</code> kernel by allowing for multiple  sills and ranges, effectively creating a \"stacked\" ramp function. This kernel can capture more complex  covariance structures with multiple levels of decay.</p> <p>Parameters:</p> Name Type Description Default <code>range</code> <code>list or Variable</code> <p>A list or TensorFlow variable representing the length scale parameters that control how quickly  the covariance decreases with distance.</p> required <code>sill</code> <code>list or Variable</code> <p>A list or TensorFlow variable representing the variance (sill) values for each ramp component.</p> required <code>scale</code> <code>optional</code> <p>An optional scale parameter that can be used to modify the metric. Default is None.</p> <code>None</code> <code>metric</code> <code>optional</code> <p>An optional metric used for distance calculation. Default is None.</p> <code>None</code> <p>Examples:</p> <p>Creating and using a <code>RampStack</code> kernel:</p> <pre><code>from geostat.kernel import RampStack\n\n# Create a RampStack kernel with multiple sills and ranges\nramp_stack_kernel = RampStack(range=[2.0, 3.0], sill=[1.0, 0.5])\n\nlocs1 = np.array([[0.0], [1.0], [2.0]])\nlocs2 = np.array([[0.0], [1.0], [2.0]])\ncovariance_matrix = ramp_stack_kernel({'locs1': locs1, 'locs2': locs2, 'sill': [1.0, 0.5], 'range': [2.0, 3.0]})\n</code></pre> Notes <ul> <li>The <code>call</code> method computes the covariance matrix using the <code>rampstack</code> function, which applies      multiple ramp functions based on the provided <code>sill</code> and <code>range</code> values for each component.</li> <li>The <code>vars</code> method returns the parameter dictionary for both <code>sill</code> and <code>range</code> using the <code>ppp_list</code> function.</li> <li>The <code>RampStack</code> kernel is useful for modeling complex processes that exhibit multiple levels of variability      or changes in smoothness at different scales.</li> </ul> Source code in <code>src/geostat/kernel.py</code> <pre><code>class RampStack(Kernel):\n    \"\"\"\n    RampStack kernel class for Gaussian Processes (GPs).\n\n    The `RampStack` class defines a kernel that extends the standard `Ramp` kernel by allowing for multiple \n    sills and ranges, effectively creating a \"stacked\" ramp function. This kernel can capture more complex \n    covariance structures with multiple levels of decay.\n\n    Parameters:\n        range (list or tf.Variable):\n            A list or TensorFlow variable representing the length scale parameters that control how quickly \n            the covariance decreases with distance.\n        sill (list or tf.Variable):\n            A list or TensorFlow variable representing the variance (sill) values for each ramp component.\n        scale (optional):\n            An optional scale parameter that can be used to modify the metric. Default is None.\n        metric (optional):\n            An optional metric used for distance calculation. Default is None.\n\n    Examples:\n        Creating and using a `RampStack` kernel:\n\n        ```\n        from geostat.kernel import RampStack\n\n        # Create a RampStack kernel with multiple sills and ranges\n        ramp_stack_kernel = RampStack(range=[2.0, 3.0], sill=[1.0, 0.5])\n\n        locs1 = np.array([[0.0], [1.0], [2.0]])\n        locs2 = np.array([[0.0], [1.0], [2.0]])\n        covariance_matrix = ramp_stack_kernel({'locs1': locs1, 'locs2': locs2, 'sill': [1.0, 0.5], 'range': [2.0, 3.0]})\n        ```\n\n    Notes:\n        - The `call` method computes the covariance matrix using the `rampstack` function, which applies \n            multiple ramp functions based on the provided `sill` and `range` values for each component.\n        - The `vars` method returns the parameter dictionary for both `sill` and `range` using the `ppp_list` function.\n        - The `RampStack` kernel is useful for modeling complex processes that exhibit multiple levels of variability \n            or changes in smoothness at different scales.\n    \"\"\"\n\n    def __init__(self, range, sill, scale=None, metric=None):\n        fa = dict(sill=sill, range=range, scale=scale)\n        autoinputs = scale_to_metric(scale, metric)\n        super().__init__(fa, dict(d2=autoinputs))\n\n    def vars(self):\n        return ppp_list(self.fa['sill']) | ppp_list(self.fa['range'])\n\n    def call(self, e):\n        if isinstance(e['sill'], (tuple, list)):\n            e['sill'] = tf.stack(e['sill'])\n        if isinstance(e['range'], (tuple, list)):\n            e['range'] = tf.stack(e['range'])\n\n        return rampstack(tf.sqrt(e['d2']), e['sill'], e['range'])\n</code></pre>"},{"location":"api/#src.geostat.kernel.SmoothConvex","title":"<code>SmoothConvex</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>SmoothConvex kernel class for Gaussian Processes (GPs).</p> <p>The <code>SmoothConvex</code> class defines a kernel that produces a smooth and convex covariance structure.  It allows for multiple sills and ranges, enabling a more complex representation of covariance that  smoothly transitions across different scales.</p> <p>Parameters:</p> Name Type Description Default <code>range</code> <code>list or Variable</code> <p>A list or TensorFlow variable representing the length scale parameters that control how  quickly the covariance decreases with distance.</p> required <code>sill</code> <code>list or Variable</code> <p>A list or TensorFlow variable representing the variance (sill) values for each smooth convex component.</p> required <code>scale</code> <code>optional</code> <p>An optional scale parameter that can be used to modify the metric. Default is None.</p> <code>None</code> <code>metric</code> <code>optional</code> <p>An optional metric used for distance calculation. Default is None.</p> <code>None</code> <p>Examples:</p> <p>Creating and using a <code>SmoothConvex</code> kernel:</p> <pre><code>from geostat.kernel import SmoothConvex\n\n# Create a SmoothConvex kernel with multiple sills and ranges\nsmooth_convex_kernel = SmoothConvex(range=[2.0, 3.0], sill=[1.0, 0.5])\n\nlocs1 = np.array([[0.0], [1.0], [2.0]])\nlocs2 = np.array([[0.0], [1.0], [2.0]])\ncovariance_matrix = smooth_convex_kernel({'locs1': locs1, 'locs2': locs2, 'sill': [1.0, 0.5], 'range': [2.0, 3.0]})\n</code></pre> Notes <ul> <li>The <code>call</code> method computes the covariance matrix using the <code>smooth_convex</code> function, which applies      multiple convex functions based on the provided <code>sill</code> and <code>range</code> values for each component.</li> <li>The <code>vars</code> method returns the parameter dictionary for both <code>sill</code> and <code>range</code> using the <code>ppp_list</code> function.</li> <li>The <code>SmoothConvex</code> kernel is useful for modeling processes that require smooth transitions and convexity      in their covariance structure across different scales.</li> </ul> Source code in <code>src/geostat/kernel.py</code> <pre><code>class SmoothConvex(Kernel):\n    \"\"\"\n    SmoothConvex kernel class for Gaussian Processes (GPs).\n\n    The `SmoothConvex` class defines a kernel that produces a smooth and convex covariance structure. \n    It allows for multiple sills and ranges, enabling a more complex representation of covariance that \n    smoothly transitions across different scales.\n\n    Parameters:\n        range (list or tf.Variable):\n            A list or TensorFlow variable representing the length scale parameters that control how \n            quickly the covariance decreases with distance.\n        sill (list or tf.Variable):\n            A list or TensorFlow variable representing the variance (sill) values for each smooth convex component.\n        scale (optional):\n            An optional scale parameter that can be used to modify the metric. Default is None.\n        metric (optional):\n            An optional metric used for distance calculation. Default is None.\n\n    Examples:\n        Creating and using a `SmoothConvex` kernel:\n\n        ```\n        from geostat.kernel import SmoothConvex\n\n        # Create a SmoothConvex kernel with multiple sills and ranges\n        smooth_convex_kernel = SmoothConvex(range=[2.0, 3.0], sill=[1.0, 0.5])\n\n        locs1 = np.array([[0.0], [1.0], [2.0]])\n        locs2 = np.array([[0.0], [1.0], [2.0]])\n        covariance_matrix = smooth_convex_kernel({'locs1': locs1, 'locs2': locs2, 'sill': [1.0, 0.5], 'range': [2.0, 3.0]})\n        ```\n\n    Notes:\n        - The `call` method computes the covariance matrix using the `smooth_convex` function, which applies \n            multiple convex functions based on the provided `sill` and `range` values for each component.\n        - The `vars` method returns the parameter dictionary for both `sill` and `range` using the `ppp_list` function.\n        - The `SmoothConvex` kernel is useful for modeling processes that require smooth transitions and convexity \n            in their covariance structure across different scales.\n    \"\"\"\n\n    def __init__(self, range, sill, scale=None, metric=None):\n        fa = dict(sill=sill, range=range, scale=scale)\n        autoinputs = scale_to_metric(scale, metric)\n        super().__init__(fa, dict(d2=autoinputs))\n\n    def vars(self):\n        return ppp_list(self.fa['sill']) | ppp_list(self.fa['range'])\n\n    def call(self, e):\n        if isinstance(e['sill'], (tuple, list)):\n            e['sill'] = tf.stack(e['sill'])\n        if isinstance(e['range'], (tuple, list)):\n            e['range'] = tf.stack(e['range'])\n\n        return smooth_convex(tf.sqrt(e['d2']), e['sill'], e['range'])\n</code></pre>"},{"location":"api/#src.geostat.kernel.SquaredExponential","title":"<code>SquaredExponential</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>SquaredExponential kernel class for Gaussian Processes (GPs).</p> <p>The <code>SquaredExponential</code> class defines a widely used kernel that models smooth and continuous  covariance structures. It is parameterized by a sill (variance) and a range (length scale)  and can optionally use a metric for scaling.</p> <p>Parameters:</p> Name Type Description Default <code>sill</code> <code>float or Variable</code> <p>The variance (sill) of the kernel, representing the maximum covariance value.</p> required <code>range</code> <code>float or Variable</code> <p>The length scale parameter that controls how quickly the covariance decreases  with distance.</p> required <code>scale</code> <code>optional</code> <p>An optional scale parameter that can be used to modify the metric. Default is None.</p> <code>None</code> <code>metric</code> <code>optional</code> <p>An optional metric used for distance calculation. Default is None.</p> <code>None</code> <p>Examples:</p> <p>Creating and using a <code>SquaredExponential</code> kernel:</p> <pre><code>from geostat.kernel import SquaredExponential\n\n# Create a SquaredExponential kernel with a sill of 1.0 and a range of 2.0\nsq_exp_kernel = SquaredExponential(sill=1.0, range=2.0)\n\nlocs1 = np.array([[0.0], [1.0], [2.0]])\nlocs2 = np.array([[0.0], [1.0], [2.0]])\ncovariance_matrix = sq_exp_kernel({'locs1': locs1, 'locs2': locs2, 'sill': 1.0, 'range': 2.0})\n</code></pre> Notes <ul> <li>The <code>call</code> method computes the covariance matrix using the squared exponential formula:     \\( C(x, x') = \\text{sill} \\cdot \\exp\\left(-0.5 \\frac{d^2}{\\text{range}^2}\\right) \\),     where \\(d^2\\) is the squared distance between <code>locs1</code> and <code>locs2</code>.</li> <li>The <code>vars</code> method returns the parameter dictionary for both <code>sill</code> and <code>range</code> using the <code>ppp</code> function.</li> <li>This kernel is appropriate for modeling smooth, continuous processes with no abrupt changes.</li> </ul> Source code in <code>src/geostat/kernel.py</code> <pre><code>class SquaredExponential(Kernel):\n    \"\"\"\n    SquaredExponential kernel class for Gaussian Processes (GPs).\n\n    The `SquaredExponential` class defines a widely used kernel that models smooth and continuous \n    covariance structures. It is parameterized by a sill (variance) and a range (length scale) \n    and can optionally use a metric for scaling.\n\n    Parameters:\n        sill (float or tf.Variable):\n            The variance (sill) of the kernel, representing the maximum covariance value.\n        range (float or tf.Variable):\n            The length scale parameter that controls how quickly the covariance decreases \n            with distance.\n        scale (optional):\n            An optional scale parameter that can be used to modify the metric. Default is None.\n        metric (optional):\n            An optional metric used for distance calculation. Default is None.\n\n    Examples:\n        Creating and using a `SquaredExponential` kernel:\n\n        ```\n        from geostat.kernel import SquaredExponential\n\n        # Create a SquaredExponential kernel with a sill of 1.0 and a range of 2.0\n        sq_exp_kernel = SquaredExponential(sill=1.0, range=2.0)\n\n        locs1 = np.array([[0.0], [1.0], [2.0]])\n        locs2 = np.array([[0.0], [1.0], [2.0]])\n        covariance_matrix = sq_exp_kernel({'locs1': locs1, 'locs2': locs2, 'sill': 1.0, 'range': 2.0})\n        ```\n\n    Notes:\n        - The `call` method computes the covariance matrix using the squared exponential formula:\n            \\\\( C(x, x') = \\\\text{sill} \\cdot \\exp\\left(-0.5 \\\\frac{d^2}{\\\\text{range}^2}\\\\right) \\\\),\n            where \\\\(d^2\\\\) is the squared distance between `locs1` and `locs2`.\n        - The `vars` method returns the parameter dictionary for both `sill` and `range` using the `ppp` function.\n        - This kernel is appropriate for modeling smooth, continuous processes with no abrupt changes.\n    \"\"\"\n\n    def __init__(self, sill, range, scale=None, metric=None):\n        fa = dict(sill=sill, range=range)\n        autoinputs = scale_to_metric(scale, metric)\n        super().__init__(fa, dict(d2=autoinputs))\n\n    def vars(self):\n        return ppp(self.fa['sill']) | ppp(self.fa['range'])\n\n    def call(self, e):\n        return e['sill'] * tf.exp(-0.5 * e['d2'] / tf.square(e['range']))\n</code></pre>"},{"location":"api/#src.geostat.kernel.Stack","title":"<code>Stack</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>Stack kernel class for combining multiple Gaussian Process (GP) kernels additively.</p> <p>The <code>Stack</code> class defines a kernel that combines multiple input kernels by stacking them together. This additive combination allows for capturing a more complex covariance structure by summing the contributions from each individual kernel.</p> <p>Parameters:</p> Name Type Description Default <code>parts</code> <code>List[Kernel]</code> <p>A list of kernel objects to be combined additively.</p> required <p>Examples:</p> <p>Creating and using a <code>Stack</code> kernel:</p> <pre><code>from geostat.kernel import Stack, SquaredExponential, Noise\n\n# Create individual kernels\nkernel1 = SquaredExponential(sill=1.0, range=2.0)\nkernel2 = Noise(nugget=0.1)\n\n# Combine kernels using the Stack class\nstacked_kernel = Stack(parts=[kernel1, kernel2])\n\nlocs1 = np.array([[0.0], [1.0], [2.0]])\nlocs2 = np.array([[0.0], [1.0], [2.0]])\ncovariance_matrix = stacked_kernel({'locs1': locs1, 'locs2': locs2})\n</code></pre> <p>Adding another kernel to an existing <code>Stack</code>:</p> <pre><code>kernel3 = SquaredExponential(sill=0.5, range=1.0)\ncombined_stack = stacked_kernel + kernel3\n</code></pre> Notes <ul> <li>The <code>call</code> method computes the sum of all covariance matrices generated by the stacked kernels.</li> <li>The <code>vars</code> method gathers parameters from all input kernels, making them accessible for optimization.</li> <li>The <code>Stack</code> kernel is useful for building complex models where multiple covariance structures need to be      combined additively, enabling richer and more flexible GP models.</li> </ul> Source code in <code>src/geostat/kernel.py</code> <pre><code>class Stack(Kernel):\n    \"\"\"\n    Stack kernel class for combining multiple Gaussian Process (GP) kernels additively.\n\n    The `Stack` class defines a kernel that combines multiple input kernels by stacking them together.\n    This additive combination allows for capturing a more complex covariance structure by summing the\n    contributions from each individual kernel.\n\n    Parameters:\n        parts (List[Kernel]):\n            A list of kernel objects to be combined additively.\n\n    Examples:\n        Creating and using a `Stack` kernel:\n\n        ```\n        from geostat.kernel import Stack, SquaredExponential, Noise\n\n        # Create individual kernels\n        kernel1 = SquaredExponential(sill=1.0, range=2.0)\n        kernel2 = Noise(nugget=0.1)\n\n        # Combine kernels using the Stack class\n        stacked_kernel = Stack(parts=[kernel1, kernel2])\n\n        locs1 = np.array([[0.0], [1.0], [2.0]])\n        locs2 = np.array([[0.0], [1.0], [2.0]])\n        covariance_matrix = stacked_kernel({'locs1': locs1, 'locs2': locs2})\n        ```\n\n        Adding another kernel to an existing `Stack`:\n\n        ```\n        kernel3 = SquaredExponential(sill=0.5, range=1.0)\n        combined_stack = stacked_kernel + kernel3\n        ```\n\n    Notes:\n        - The `call` method computes the sum of all covariance matrices generated by the stacked kernels.\n        - The `vars` method gathers parameters from all input kernels, making them accessible for optimization.\n        - The `Stack` kernel is useful for building complex models where multiple covariance structures need to be \n            combined additively, enabling richer and more flexible GP models.\n    \"\"\"\n\n    def __init__(self, parts: List[Kernel]):\n        self.parts = parts\n        super().__init__({}, dict(locs1='locs1', locs2='locs2', parts=parts))\n\n    def vars(self):\n        return {k: p for part in self.parts for k, p in part.vars().items()}\n\n    def __add__(self, other):\n        if isinstance(other, Kernel):\n            return Stack(self.parts + [other])\n\n    def call(self, e):\n        return tf.reduce_sum(e['parts'], axis=0)\n\n    def report(self):\n        return ' '.join(part.report(p) for part in self.parts)\n</code></pre>"},{"location":"api/#src.geostat.kernel.TrendPrior","title":"<code>TrendPrior</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>TrendPrior class representing a kernel with a linear trend prior for Gaussian Processes (GPs).</p> <p>The <code>TrendPrior</code> class defines a kernel that incorporates a linear trend in the covariance structure using a provided featurizer function. This kernel is particularly useful when the underlying process is expected to exhibit a trend that can be captured by the specified features.</p> <p>Parameters:</p> Name Type Description Default <code>featurizer</code> <code>Callable</code> <p>A function that takes input locations and returns a feature matrix. This function defines the features used in the trend prior.</p> required <code>alpha</code> <code>float or Variable</code> <p>The scaling factor (weight) applied to the trend prior.</p> required <p>Examples:</p> <p>Defining a TrendPrior kernel with a custom featurizer:</p> <pre><code>import tensorflow as tf\nfrom geostat.kernel import TrendPrior\n\n# Define a simple featurizer function\ndef simple_featurizer(x):\n    return x, x**2\n\nalpha = 0.5\ntrend_prior_kernel = TrendPrior(featurizer=simple_featurizer, alpha=alpha)\n\nlocs1 = tf.constant([[1.0], [2.0], [3.0]])\nlocs2 = tf.constant([[1.5], [2.5], [3.5]])\ncovariance_matrix = trend_prior_kernel({'locs1': locs1, 'locs2': locs2, 'alpha': alpha})\n</code></pre> Notes <ul> <li>The <code>call</code> method computes the covariance matrix using the features generated by the featurizer     function and scales it by the <code>alpha</code> parameter.</li> <li>The <code>vars</code> method returns the parameter dictionary for <code>alpha</code> using the <code>ppp</code> function.</li> <li>The <code>TrendPrior</code> kernel is typically used when the GP model needs to account for linear or      polynomial trends in the data.</li> </ul> Source code in <code>src/geostat/kernel.py</code> <pre><code>class TrendPrior(Kernel):\n    \"\"\"\n    TrendPrior class representing a kernel with a linear trend prior for Gaussian Processes (GPs).\n\n    The `TrendPrior` class defines a kernel that incorporates a linear trend in the covariance structure\n    using a provided featurizer function. This kernel is particularly useful when the underlying process\n    is expected to exhibit a trend that can be captured by the specified features.\n\n    Parameters:\n        featurizer (Callable):\n            A function that takes input locations and returns a feature matrix. This function defines\n            the features used in the trend prior.\n        alpha (float or tf.Variable):\n            The scaling factor (weight) applied to the trend prior.\n\n    Examples:\n        Defining a TrendPrior kernel with a custom featurizer:\n\n        ```\n        import tensorflow as tf\n        from geostat.kernel import TrendPrior\n\n        # Define a simple featurizer function\n        def simple_featurizer(x):\n            return x, x**2\n\n        alpha = 0.5\n        trend_prior_kernel = TrendPrior(featurizer=simple_featurizer, alpha=alpha)\n\n        locs1 = tf.constant([[1.0], [2.0], [3.0]])\n        locs2 = tf.constant([[1.5], [2.5], [3.5]])\n        covariance_matrix = trend_prior_kernel({'locs1': locs1, 'locs2': locs2, 'alpha': alpha})\n        ```\n\n    Notes:\n        - The `call` method computes the covariance matrix using the features generated by the featurizer\n            function and scales it by the `alpha` parameter.\n        - The `vars` method returns the parameter dictionary for `alpha` using the `ppp` function.\n        - The `TrendPrior` kernel is typically used when the GP model needs to account for linear or \n            polynomial trends in the data.\n    \"\"\"\n\n    def __init__(self, featurizer, alpha):\n        fa = dict(alpha=alpha)\n        self.featurizer = featurizer\n        super().__init__(fa, dict(locs1='locs1', locs2='locs2'))\n\n    def vars(self):\n        return ppp(self.fa['alpha'])\n\n    def call(self, e):\n        F1 = tf.cast(self.featurizer(e['locs1']), tf.float32)\n        F2 = tf.cast(self.featurizer(e['locs2']), tf.float32)\n        return e['alpha'] * tf.einsum('ba,ca-&gt;bc', F1, F2)\n</code></pre>"},{"location":"api/#src.geostat.kernel.Wiener","title":"<code>Wiener</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>Wiener kernel class for Gaussian Processes (GPs).</p> <p>The <code>Wiener</code> class defines a kernel that represents a Wiener process (or Brownian motion) in one dimension. It models the covariance based on the minimum distance along a specified axis of integration, starting  from a given point.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>The axis along which the Wiener process operates (e.g., 0 for x-axis, 1 for y-axis).</p> required <code>start</code> <code>float</code> <p>The starting point of the Wiener process along the specified axis.</p> required <p>Examples:</p> <p>Creating and using a <code>Wiener</code> kernel:</p> <pre><code>from geostat.kernel import Wiener\n\n# Create a Wiener kernel operating along the x-axis starting from 0.0\nwiener_kernel = Wiener(axis=0, start=0.0)\n\nlocs1 = np.array([[0.0], [1.0], [2.0]])\nlocs2 = np.array([[0.0], [1.0], [2.0]])\ncovariance_matrix = wiener_kernel({'locs1': locs1, 'locs2': locs2})\n</code></pre> Notes <ul> <li>The <code>call</code> method computes the covariance matrix using the Wiener process formula, which is based      on the minimum distance along the specified <code>axis</code> from the <code>start</code> point.</li> <li>The <code>vars</code> method returns an empty dictionary since the Wiener kernel does not have tunable parameters.</li> <li>The <code>Wiener</code> kernel is suitable for modeling processes that evolve with time or any other      ordered dimension, representing a type of random walk or Brownian motion.</li> </ul> Source code in <code>src/geostat/kernel.py</code> <pre><code>class Wiener(Kernel):\n    \"\"\"\n    Wiener kernel class for Gaussian Processes (GPs).\n\n    The `Wiener` class defines a kernel that represents a Wiener process (or Brownian motion) in one dimension.\n    It models the covariance based on the minimum distance along a specified axis of integration, starting \n    from a given point.\n\n    Parameters:\n        axis (int):\n            The axis along which the Wiener process operates (e.g., 0 for x-axis, 1 for y-axis).\n        start (float):\n            The starting point of the Wiener process along the specified axis.\n\n    Examples:\n        Creating and using a `Wiener` kernel:\n\n        ```\n        from geostat.kernel import Wiener\n\n        # Create a Wiener kernel operating along the x-axis starting from 0.0\n        wiener_kernel = Wiener(axis=0, start=0.0)\n\n        locs1 = np.array([[0.0], [1.0], [2.0]])\n        locs2 = np.array([[0.0], [1.0], [2.0]])\n        covariance_matrix = wiener_kernel({'locs1': locs1, 'locs2': locs2})\n        ```\n\n    Notes:\n        - The `call` method computes the covariance matrix using the Wiener process formula, which is based \n            on the minimum distance along the specified `axis` from the `start` point.\n        - The `vars` method returns an empty dictionary since the Wiener kernel does not have tunable parameters.\n        - The `Wiener` kernel is suitable for modeling processes that evolve with time or any other \n            ordered dimension, representing a type of random walk or Brownian motion.\n    \"\"\"\n\n    def __init__(self, axis, start):\n\n        self.axis = axis\n        self.start = start\n\n        # Include the element of scale corresponding to the axis of\n        # integration as an explicit formal argument.\n        fa = dict()\n\n        super().__init__(fa, dict(locs1='locs1', locs2='locs2'))\n\n    def vars(self):\n        return {}\n\n    def call(self, e):\n        x1 = e['locs1'][..., self.axis]\n        x2 = e['locs2'][..., self.axis]\n        k = tf.maximum(0., tf.minimum(ed(x1, 1), ed(x2, 0)) - self.start)\n        return k\n</code></pre>"},{"location":"api/#src.geostat.model","title":"<code>src.geostat.model</code>","text":""},{"location":"api/#src.geostat.model.Featurizer","title":"<code>Featurizer</code>","text":"<p>Featurizer class for producing feature matrices (F matrix) from location data.</p> <p>The <code>Featurizer</code> applies a specified featurization function to the input location data  and generates the corresponding feature matrix. If no featurization function is provided,  it produces a matrix with appropriate dimensions containing only ones.</p> <p>Parameters:</p> Name Type Description Default <code>featurization</code> <code>Callable or None</code> <p>A function that takes in the individual components of location data and returns the features. If set to <code>None</code>, the featurizer will produce an empty feature matrix (i.e., only ones).</p> required <p>Examples:</p> <p>Creating a <code>Featurizer</code> using a custom featurization function:</p> <pre><code>import tensorflow as tf\nfrom geostat.model import Featurizer\n\n# Define a custom featurization function\ndef simple_featurizer(x, y):\n    return x, y, x * y\n\n# Initialize the Featurizer\nfeaturizer = Featurizer(simple_featurizer)\n</code></pre> <p>Using the <code>Featurizer</code> to transform location data:</p> <pre><code>locs = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\nF_matrix = featurizer(locs)\nprint(F_matrix) # F_matrix will contain the features: (x, y, x*y) for each location\n# tf.Tensor(\n# [[ 1.  2.  2.]\n#  [ 3.  4. 12.]\n#  [ 5.  6. 30.]], shape=(3, 3), dtype=float32)\n</code></pre> <p>Handling the case where no featurization is provided:</p> <pre><code>featurizer_no_feat = Featurizer(None)\nF_matrix = featurizer_no_feat(locs)\nprint(F_matrix) # Since no featurization function is provided, F_matrix will have shape (3, 0)\n# tf.Tensor([], shape=(3, 0), dtype=float32)\n</code></pre> Notes <ul> <li>The <code>__call__</code> method is used to apply the featurization to input location data.</li> <li>If <code>featurization</code> returns a tuple, it is assumed to represent multiple features,  which will be stacked to form the feature matrix.</li> </ul> Source code in <code>src/geostat/model.py</code> <pre><code>class Featurizer:\n    \"\"\"\n    Featurizer class for producing feature matrices (F matrix) from location data.\n\n    The `Featurizer` applies a specified featurization function to the input location data \n    and generates the corresponding feature matrix. If no featurization function is provided, \n    it produces a matrix with appropriate dimensions containing only ones.\n\n    Parameters:\n        featurization (Callable or None):\n            A function that takes in the individual components of location data and returns the features.\n            If set to `None`, the featurizer will produce an empty feature matrix (i.e., only ones).\n\n    Examples:\n        Creating a `Featurizer` using a custom featurization function:\n\n        ```\n        import tensorflow as tf\n        from geostat.model import Featurizer\n\n        # Define a custom featurization function\n        def simple_featurizer(x, y):\n            return x, y, x * y\n\n        # Initialize the Featurizer\n        featurizer = Featurizer(simple_featurizer)\n        ```\n\n        Using the `Featurizer` to transform location data:\n\n        ```\n        locs = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n        F_matrix = featurizer(locs)\n        print(F_matrix) # F_matrix will contain the features: (x, y, x*y) for each location\n        # tf.Tensor(\n        # [[ 1.  2.  2.]\n        #  [ 3.  4. 12.]\n        #  [ 5.  6. 30.]], shape=(3, 3), dtype=float32)\n        ```\n\n        Handling the case where no featurization is provided:\n\n        ```\n        featurizer_no_feat = Featurizer(None)\n        F_matrix = featurizer_no_feat(locs)\n        print(F_matrix) # Since no featurization function is provided, F_matrix will have shape (3, 0)\n        # tf.Tensor([], shape=(3, 0), dtype=float32)\n        ```\n\n    Notes:\n        - The `__call__` method is used to apply the featurization to input location data.\n        - If `featurization` returns a tuple, it is assumed to represent multiple features, \n        which will be stacked to form the feature matrix.\n    \"\"\"\n\n    def __init__(self, featurization):\n        self.featurization = featurization\n\n    def __call__(self, locs):\n        locs = tf.cast(locs, tf.float32)\n        if self.featurization is None: # No features.\n            return tf.ones([tf.shape(locs)[0], 0], dtype=tf.float32)\n\n        feats = self.featurization(*tf.unstack(locs, axis=1))\n        if isinstance(feats, tuple): # One or many features.\n            if len(feats) == 0:\n                return tf.ones([tf.shape(locs)[0], 0], dtype=tf.float32)\n            else:\n                feats = self.featurization(*tf.unstack(locs, axis=1))\n                feats = [tf.broadcast_to(tf.cast(f, tf.float32), [tf.shape(locs)[0]]) for f in feats]\n                return tf.stack(feats, axis=1)\n        else: # One feature.\n            return e(feats)\n</code></pre>"},{"location":"api/#src.geostat.model.GP","title":"<code>GP</code>  <code>dataclass</code>","text":"<p>Gaussian Process (GP) model class with a mean function and a kernel.</p> <p>This class represents a Gaussian Process with specified mean and kernel functions. If no mean is provided, a zero mean is used by default. The kernel must always be specified. The class supports addition to combine two GP models, and it allows gathering variables  from the mean and kernel.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Trend</code> <p>The mean function of the Gaussian Process. If not provided or set to 0,  a ZeroTrend is used as the default mean.</p> <code>None</code> <code>kernel</code> <code>Kernel</code> <p>The kernel function of the Gaussian Process. This parameter is required.</p> <code>None</code> <p>Examples:</p> <p>Creating a simple Gaussian Process with default mean and a Noise kernel:</p> <pre><code>import geostat.mean as mn\nimport geostat.kernel as krn\nfrom geostat import Parameters, GP\np = Parameters(nugget=1., sill=1., beta=[4., 3., 2., 1.])\nkernel = krn.Noise(p.nugget)\ngp = GP(kernel=kernel)\n</code></pre> <p>The mean defaults to ZeroTrend if not provided.</p> <pre><code>print(gp.mean)\n# ZeroTrend(fa={}, autoinputs={'locs1': 'locs1'})\n</code></pre> <p>Specifying both mean and kernel:</p> <pre><code>@geostat.featurizer()\ndef trend_featurizer(x, y): return 1., x, y, x*y\nmean_function = mn.Trend(trend_featurizer, beta=p.beta)\ngp = GP(mean=mean_function, kernel=kernel)\n</code></pre> <p>Adding two GP objects:</p> <pre><code>gp1 = GP(kernel=krn.Noise(p.nugget))\ngp2 = GP(mean=mean_function, kernel=krn.Delta(p.sill))\ncombined_gp = gp1 + gp2\nprint(\"Combined Mean: \", combined_gp.mean)  # &lt;Trend object&gt;\nprint(\"Combined Kernel: \", combined_gp.kernel)  # &lt;Stack object&gt;\n</code></pre> Notes <ul> <li>The <code>__tf_tracing_type__</code> method is used for TensorFlow tracing purposes and typically not called directly.</li> <li>Ensure that the kernel is always provided upon initialization.</li> </ul> Source code in <code>src/geostat/model.py</code> <pre><code>@dataclass\nclass GP:    \n    \"\"\"\n    Gaussian Process (GP) model class with a mean function and a kernel.\n\n    This class represents a Gaussian Process with specified mean and kernel functions.\n    If no mean is provided, a zero mean is used by default. The kernel must always be specified.\n    The class supports addition to combine two GP models, and it allows gathering variables \n    from the mean and kernel.\n\n    Parameters:\n        mean (mn.Trend, optional):\n            The mean function of the Gaussian Process. If not provided or set to 0, \n            a ZeroTrend is used as the default mean.\n\n        kernel (krn.Kernel):\n            The kernel function of the Gaussian Process. This parameter is required.\n\n    Examples:\n        Creating a simple Gaussian Process with default mean and a Noise kernel:\n\n        ```\n        import geostat.mean as mn\n        import geostat.kernel as krn\n        from geostat import Parameters, GP\n        p = Parameters(nugget=1., sill=1., beta=[4., 3., 2., 1.])\n        kernel = krn.Noise(p.nugget)\n        gp = GP(kernel=kernel)\n        ```\n\n        The mean defaults to ZeroTrend if not provided.\n\n        ```\n        print(gp.mean)\n        # ZeroTrend(fa={}, autoinputs={'locs1': 'locs1'})\n        ```\n\n        Specifying both mean and kernel:\n\n        ```\n        @geostat.featurizer()\n        def trend_featurizer(x, y): return 1., x, y, x*y\n        mean_function = mn.Trend(trend_featurizer, beta=p.beta)\n        gp = GP(mean=mean_function, kernel=kernel)\n        ```\n\n        Adding two GP objects:\n\n        ```\n        gp1 = GP(kernel=krn.Noise(p.nugget))\n        gp2 = GP(mean=mean_function, kernel=krn.Delta(p.sill))\n        combined_gp = gp1 + gp2\n        print(\"Combined Mean: \", combined_gp.mean)  # &lt;Trend object&gt;\n        print(\"Combined Kernel: \", combined_gp.kernel)  # &lt;Stack object&gt;\n        ```\n\n    Notes:\n        - The `__tf_tracing_type__` method is used for TensorFlow tracing purposes and typically not called directly.\n        - Ensure that the kernel is always provided upon initialization.\n    \"\"\"\n\n    mean: mn.Trend = None\n    kernel: krn.Kernel = None\n\n    def __post_init__(self):\n        if self.mean is None or self.mean == 0:\n            self.mean = mn.ZeroTrend()\n        assert self.kernel is not None\n\n    def __add__(self, other):\n        return GP(self.mean + other.mean, self.kernel + other.kernel)\n\n    def __tf_tracing_type__(self, context):\n        return SingletonTraceType(self)\n\n    def gather_vars(self):\n        return self.mean.gather_vars() | self.kernel.gather_vars()\n</code></pre>"},{"location":"api/#src.geostat.model.Model","title":"<code>Model</code>  <code>dataclass</code>","text":"<p>Model class for performing Gaussian Process (GP) training and prediction with optional warping.</p> <p>The <code>Model</code> class integrates a GP model with optional data warping, and supports data generation on given location, training on given location and observation data, and prediction on given location.</p> <p>Parameters:</p> Name Type Description Default <code>gp</code> <code>GP</code> <p>The Gaussian Process model to be used for training and prediction.</p> required <code>warp</code> <code>Warp</code> <p>An optional warping transformation applied to the data. If not specified, <code>NoWarp</code>  is used by default.</p> <code>None</code> <code>parameter_sample_size</code> <code>int</code> <p>The number of parameter samples to draw. Default is None.</p> <code>None</code> <code>locs</code> <code>ndarray</code> <p>A NumPy array containing location data.</p> <code>None</code> <code>vals</code> <code>ndarray</code> <p>A NumPy array containing observed values corresponding to <code>locs</code>.</p> <code>None</code> <code>cats</code> <code>ndarray</code> <p>A NumPy array containing categorical data.</p> <code>None</code> <code>report</code> <code>Callable</code> <p>A custom reporting function to display model parameters. If not provided, a default  reporting function is used.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print model parameters and status updates. Default is True.</p> <code>True</code> <p>Examples:</p> <p>Initializing a <code>Model</code> with a Gaussian Process:</p> <pre><code>from geostat import GP, Model, Parameters\nfrom geostat.kernel import Noise\nimport numpy as np\n\n# Create parameters.\np = Parameters(nugget=1.)\n\n# Define the Gaussian Process and the model\ngp = GP(kernel=Noise(nugget=p.nugget))\nlocs = np.array([[0.0, 1.0], [1.0, 2.0]])\nvals = np.array([1.0, 2.0])\nmodel = Model(gp=gp, locs=locs, vals=vals)\n</code></pre> Notes <ul> <li>The <code>__post_init__</code> method sets up default values, initializes the warping if not provided,  and sets up reporting and data preprocessing.</li> </ul> Source code in <code>src/geostat/model.py</code> <pre><code>@dataclass\nclass Model():\n    \"\"\"\n    Model class for performing Gaussian Process (GP) training and prediction with optional warping.\n\n    The `Model` class integrates a GP model with optional data warping, and supports data generation on given location,\n    training on given location and observation data, and prediction on given location.\n\n    Parameters:\n        gp (GP):\n            The Gaussian Process model to be used for training and prediction.\n        warp (Warp, optional):\n            An optional warping transformation applied to the data. If not specified, `NoWarp` \n            is used by default.\n        parameter_sample_size (int, optional):\n            The number of parameter samples to draw. Default is None.\n        locs (np.ndarray, optional):\n            A NumPy array containing location data.\n        vals (np.ndarray, optional):\n            A NumPy array containing observed values corresponding to `locs`.\n        cats (np.ndarray, optional):\n            A NumPy array containing categorical data.\n        report (Callable, optional):\n            A custom reporting function to display model parameters. If not provided, a default \n            reporting function is used.\n        verbose (bool, optional):\n            Whether to print model parameters and status updates. Default is True.\n\n    Examples:\n        Initializing a `Model` with a Gaussian Process:\n\n        ```\n        from geostat import GP, Model, Parameters\n        from geostat.kernel import Noise\n        import numpy as np\n\n        # Create parameters.\n        p = Parameters(nugget=1.)\n\n        # Define the Gaussian Process and the model\n        gp = GP(kernel=Noise(nugget=p.nugget))\n        locs = np.array([[0.0, 1.0], [1.0, 2.0]])\n        vals = np.array([1.0, 2.0])\n        model = Model(gp=gp, locs=locs, vals=vals)\n        ```\n\n    Notes:\n        - The `__post_init__` method sets up default values, initializes the warping if not provided, \n        and sets up reporting and data preprocessing.\n    \"\"\"\n\n    gp: GP\n    warp: Warp = None\n    parameter_sample_size: Optional[int] = None\n    locs: np.ndarray = None\n    vals: np.ndarray = None\n    cats: np.ndarray = None\n    report: Callable = None\n    verbose: bool = True\n\n    def __post_init__(self):\n        # '''\n        # Parameters:\n        #         x : Pandas DataFrame with columns for locations.\n\n        #         u : A Pandas Series containing observations.\n\n        #         featurization : function, optional\n        #             Should be a function that takes x1 (n-dim array of input data)\n        #             and returns the coordinates, i.e., x, y, x**2, y**2.\n        #             Example: def featurization(x1):\n        #                         return x1[:, 0], x1[:, 1], x1[:, 0]**2, x1[:, 1]**2.\n        #             Default is None.\n\n        #         latent : List[GP]\n        #              Name of the covariance function to use in the GP.\n        #              Should be 'squared-exp' or 'gamma-exp'.\n        #              Default is 'squared-exp'.\n\n        #         verbose : boolean, optional\n        #             Whether or not to print parameters.\n        #             Default is True.\n\n        # Performs Gaussian process training and prediction.\n        # '''\n\n        if self.warp is None: self.warp = NoWarp()\n\n        # Default reporting function.\n        def default_report(p, prefix=None):\n            if prefix: print(prefix, end=' ')\n\n            def fmt(x):\n                if isinstance(x, tf.Tensor):\n                    x = x.numpy()\n\n                if isinstance(x, (int, np.int32, np.int64)):\n                    return '{:5d}'.format(x)\n                if isinstance(x, (float, np.float32, np.float64)):\n                    return '{:5.2f}'.format(x)\n                else:\n                    with np.printoptions(precision=2, formatter={'floatkind': '{:5.2f}'.format}):\n                        return str(x)\n\n            print('[%s]' % (' '.join('%s %s' % (k, fmt(v)) for k, v in p.items())))\n\n        if self.report == None: self.report = default_report\n\n        if self.locs is not None: self.locs = np.array(self.locs)\n        if self.vals is not None: self.vals = np.array(self.vals)\n        if self.cats is not None: self.cats = np.array(self.cats)\n\n        # Collect parameters and create TF parameters.\n        for p in self.gather_vars().values():\n            p.create_tf_variable()\n\n    def gather_vars(self):\n        return self.gp.gather_vars() | self.warp.gather_vars()\n\n    def set(self, **values):\n        \"\"\"\n        Sets the values of the model's parameters based on the provided keyword arguments.\n        Each parameter specified must exist in the model; otherwise, a `ValueError` is raised.\n\n        Parameters:\n            values (keyword arguments):\n                A dictionary of parameter names and their corresponding values that should be \n                set in the model. Each key corresponds to a parameter name, and the value is \n                the value to be assigned to that parameter.\n\n        Returns:\n            self (Model):\n                The model instance with updated parameter values, allowing for method chaining.\n\n        Raises:\n            ValueError:\n                If a provided parameter name does not exist in the model's parameters.\n\n        Examples:\n            Update parameter value using `set`:\n\n            ```\n            from geostat import GP, Model, Parameters\n            from geostat.kernel import Noise\n\n            # Create parameters.\n            p = Parameters(nugget=1.)\n\n            # Create model\n            kernel = Noise(nugget=p.nugget)\n            model = Model(GP(0, kernel))\n\n            # Update parameters\n            model.set(nugget=0.5)\n            ```\n\n        Notes:\n            - The `set` method retrieves the current parameters using `gather_vars` and updates \n            their values. The associated TensorFlow variables are also recreated.\n            - This method is useful for dynamically updating the model's parameters after initialization.\n        \"\"\"\n\n        parameters = self.gather_vars()\n        for name, v in values.items():\n            if name in parameters:\n                parameters[name].value = v\n                parameters[name].create_tf_variable()\n            else:\n                raise ValueError(f\"{name} is not a parameter\")\n        return self\n\n    def fit(self, locs, vals, cats=None, step_size=0.01, iters=100, reg=None):\n        \"\"\"\n        Trains the model using the provided location and value data by optimizing the parameters of the Gaussian Process (GP)\n        using the Adam optimizer. Optionally performs regularization and can handle categorical data.\n\n        Parameters:        \n            locs (np.ndarray):\n                A NumPy array containing the input locations for training.\n            vals (np.ndarray):\n                A NumPy array containing observed values corresponding to the `locs`.\n            cats (np.ndarray, optional):\n                A NumPy array containing categorical data for each observation in `locs`. If provided,\n                the data is sorted according to `cats` to enable stratified training. Defaults to None.\n            step_size (float, optional):\n                The learning rate for the Adam optimizer.\n            iters (int, optional):\n                The total number of iterations to run for training.\n            reg (float or None, optional):\n                Regularization penalty parameter. If None, no regularization is applied.\n\n        Returns:\n            self (Model):\n                The model instance with updated parameters, allowing for method chaining.\n\n        Examples:\n            Fitting a model using training data:\n\n            ```\n            from geostat import GP, Model, Parameters\n            from geostat.kernel import Noise\n            import numpy as np\n\n            # Create parameters.\n            p = Parameters(nugget=1.)\n\n            # Create model\n            kernel = Noise(nugget=p.nugget)\n            model = Model(GP(0, kernel))\n\n            # Fit model\n            locs = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])\n            vals = np.array([10.0, 15.0, 20.0])\n            model.fit(locs, vals, step_size=0.05, iters=500)\n            # [iter    50 ll -63.71 time  2.72 reg  0.00 nugget  6.37]\n            # [iter   100 ll -32.94 time  0.25 reg  0.00 nugget 13.97]\n            # [iter   150 ll -23.56 time  0.25 reg  0.00 nugget 22.65]\n            # [iter   200 ll -19.26 time  0.25 reg  0.00 nugget 32.27]\n            # [iter   250 ll -16.92 time  0.25 reg  0.00 nugget 42.63]\n            # [iter   300 ll -15.52 time  0.24 reg  0.00 nugget 53.50]\n            # [iter   350 ll -14.63 time  0.24 reg  0.00 nugget 64.71]\n            # [iter   400 ll -14.03 time  0.24 reg  0.00 nugget 76.10]\n            # [iter   450 ll -13.61 time  0.25 reg  0.00 nugget 87.52]\n            # [iter   500 ll -13.32 time  0.24 reg  0.00 nugget 98.85]\n            ```\n\n            Using categorical data for training:\n\n            ```\n            cats = np.array([1, 1, 2])\n            model.fit(locs, vals, cats=cats, step_size=0.01, iters=300)\n            # [iter    30 ll -12.84 time  0.25 reg  0.00 nugget 131.53]\n            # [iter    60 ll -12.62 time  0.15 reg  0.00 nugget 164.41]\n            # [iter    90 ll -12.53 time  0.16 reg  0.00 nugget 191.70]\n            # [iter   120 ll -12.50 time  0.16 reg  0.00 nugget 211.74]\n            # [iter   150 ll -12.49 time  0.15 reg  0.00 nugget 225.07]\n            # [iter   180 ll -12.49 time  0.16 reg  0.00 nugget 233.15]\n            # [iter   210 ll -12.49 time  0.15 reg  0.00 nugget 237.64]\n            # [iter   240 ll -12.49 time  0.15 reg  0.00 nugget 239.92]\n            # [iter   270 ll -12.49 time  0.15 reg  0.00 nugget 240.98]\n            # [iter   300 ll -12.49 time  0.15 reg  0.00 nugget 241.42]\n            ```\n\n        Notes:\n            - The `fit` method uses the Adam optimizer to minimize the negative log-likelihood (`ll`) and any regularization \n            penalties specified by `reg`.\n            - During training, if `cats` are provided, data points are sorted according to `cats` to ensure grouped training.\n            - The `verbose` flag determines whether training progress is printed after each iteration.\n            - After training, parameter values are saved and can be accessed or updated using the model's attributes.\n        \"\"\"\n\n        # Collect parameters and create TF parameters.\n        parameters = self.gather_vars()\n\n        # Permute datapoints if cats is given.\n        if cats is not None:\n            cats = np.array(cats)\n            perm = np.argsort(cats)\n            locs, vals, cats = locs[perm], vals[perm], cats[perm]\n        else:\n            cats = np.zeros(locs.shape[:1], np.int32)\n            perm = None\n\n        # Data dict.\n        self.data = {\n            'warplocs': self.warp(locs),\n            'vals': tf.constant(vals, dtype=tf.float32),\n            'cats': tf.constant(cats, dtype=tf.int32)}\n\n        optimizer = tf.keras.optimizers.Adam(learning_rate=step_size)\n\n        j = 0 # Iteration count.\n        for i in range(10):\n            t0 = time.time()\n            while j &lt; (i + 1) * iters / 10:\n                ll, reg_penalty = gp_train_step(\n                    optimizer, self.data, parameters, self.gp, reg)\n                j += 1\n\n            time_elapsed = time.time() - t0\n            if self.verbose == True:\n                self.report(\n                  dict(iter=j, ll=ll, time=time_elapsed, reg=reg_penalty) |\n                  {p.name: p.surface() for p in parameters.values()})\n\n        # Save parameter values.\n        for p in parameters.values():\n            p.update_value()\n\n        # Restore order if things were permuted.\n        if perm is not None:\n            revperm = np.argsort(perm)\n            locs, vals, cats = locs[revperm], vals[revperm], cats[revperm]\n\n        self.locs = locs\n        self.vals = vals\n        self.cats = cats\n\n        return self\n\n    def mcmc(self, locs, vals, cats=None,\n            chains=4, step_size=0.1, move_prob=0.5,\n            samples=1000, burnin=500, report_interval=100):\n\n        assert samples % report_interval == 0, '`samples` must be a multiple of `report_interval`'\n        assert burnin % report_interval == 0, '`burnin` must be a multiple of `report_interval`'\n\n        # Permute datapoints if cats is given.\n        if cats is not None:\n            cats = np.array(cats)\n            perm = np.argsort(cats)\n            locs, vals, cats = locs[perm], vals[perm], cats[perm]\n\n        # Data dict.\n        self.data = {\n            'locs': tf.constant(locs, dtype=tf.float32),\n            'vals': tf.constant(vals, dtype=tf.float32),\n            'cats': None if cats is None else tf.constant(cats, dtype=tf.int32)}\n\n        # Initial MCMC state.\n        initial_up = self.parameter_space.get_underlying(self.parameters)\n\n        # Unnormalized log posterior distribution.\n        def g(up):\n            sp = self.parameter_space.get_surface(up)\n            return gp_log_likelihood(self.data, sp, self.gp)\n\n        def f(*up_flat):\n            up = tf.nest.pack_sequence_as(initial_up, up_flat)\n            ll = tf.map_fn(g, up, fn_output_signature=tf.float32)\n            # log_prior = -tf.reduce_sum(tf.math.log(1. + tf.square(up_flat)), axis=0)\n            return ll # + log_prior\n\n        # Run the chain for a burst.\n        @tf.function\n        def run_chain(current_state, final_results, kernel, iters):\n            samples, results, final_results = tfp.mcmc.sample_chain(\n                num_results=iters,\n                current_state=current_state,\n                kernel=kernel,\n                return_final_kernel_results=True,\n                trace_fn=lambda _, results: results)\n\n            return samples, results, final_results\n\n        def new_state_fn(scale, dtype):\n          direction_dist = tfd.Normal(loc=dtype(0), scale=dtype(1))\n          scale_dist = tfd.Exponential(rate=dtype(1/scale))\n          pick_dist = tfd.Bernoulli(probs=move_prob)\n\n          def _fn(state_parts, seed):\n            next_state_parts = []\n            part_seeds = tfp.random.split_seed(\n                seed, n=len(state_parts), salt='rwmcauchy')\n            for sp, ps in zip(state_parts, part_seeds):\n                pick = tf.cast(pick_dist.sample(sample_shape=sp.shape, seed=ps), tf.float32)\n                direction = direction_dist.sample(sample_shape=sp.shape, seed=ps)\n                scale_val = scale_dist.sample(seed=ps)\n                next_state_parts.append(sp + tf.einsum('a...,a-&gt;a...', pick * direction, scale_val))\n            return next_state_parts\n          return _fn\n\n        inv_temps = 0.5**np.arange(chains, dtype=np.float32)\n\n        def make_kernel_fn(target_log_prob_fn):\n            return tfp.mcmc.RandomWalkMetropolis(\n                target_log_prob_fn=target_log_prob_fn,\n                new_state_fn=new_state_fn(scale=step_size / np.sqrt(inv_temps), dtype=np.float32))\n\n        kernel = tfp.mcmc.ReplicaExchangeMC(\n            target_log_prob_fn=f,\n            inverse_temperatures=inv_temps,\n            make_kernel_fn=make_kernel_fn)\n\n        # Do bursts.\n        current_state = tf.nest.flatten(initial_up)\n        final_results = None\n        acc_states = []\n        num_bursts = (samples + burnin) // report_interval\n        burnin_bursts = burnin // report_interval\n        for i in range(num_bursts):\n            is_burnin = i &lt; burnin_bursts\n\n            if self.verbose and (i == 0 or i == burnin_bursts):\n                print('BURNIN\\n' if is_burnin else '\\nSAMPLING')\n\n            t0 = time.time()\n            states, results, final_results = run_chain(current_state, final_results, kernel, report_interval)\n\n            if self.verbose == True:\n                if not is_burnin: print()\n                accept_rates = results.post_swap_replica_results.is_accepted.numpy().mean(axis=0)\n                print('[iter {:4d}] [time {:.1f}] [accept rates {}]'.format(\n                    ((i if is_burnin else i - burnin_bursts) + 1) * report_interval,\n                    time.time() - t0,\n                    ' '.join([f'{x:.2f}' for x in accept_rates.tolist()])))\n\n            if not is_burnin:\n                acc_states.append(tf.nest.map_structure(lambda x: x.numpy(), states))\n                all_states = [np.concatenate(x, 0) for x in zip(*acc_states)]\n                up = tf.nest.pack_sequence_as(initial_up, all_states)\n                sp = self.parameter_space.get_surface(up, numpy=True) \n\n                # Reporting\n                if self.verbose == True:\n                    for p in [5, 50, 95]:\n                        x = tf.nest.map_structure(lambda x: np.percentile(x, p, axis=0), sp)\n                        self.report(x, prefix=f'{p:02d}%ile')\n\n            current_state = [s[-1] for s in states]\n\n        posterior = self.parameter_space.get_surface(up, numpy=True)\n\n        # Restore order if things were permuted.\n        if cats is not None:\n            revperm = np.argsort(perm)\n            locs, vals, cats = locs[revperm], vals[revperm], cats[revperm]\n\n        return replace(self, \n            parameters=posterior,\n            parameter_sample_size=samples,\n            locs=locs, vals=vals, cats=cats)\n\n    def generate(self, locs, cats=None):\n        \"\"\"\n        Generates synthetic data values from the Gaussian Process (GP) model based on the provided location data.\n        This method simulates values based on the GP's covariance structure, allowing for random sample generation.\n\n        Parameters:\n            locs (np.ndarray):\n                A NumPy array containing the input locations for which to generate synthetic values.\n            cats (np.ndarray, optional):\n                A NumPy array containing categorical data corresponding to `locs`. If provided, data points \n                are permuted according to `cats` for stratified generation. Defaults to None.\n\n        Returns:\n            self (Model):\n                The model instance with generated values stored in `self.vals` and corresponding locations stored \n                in `self.locs`. This enables method chaining.\n\n        Examples:\n            Generating synthetic values for a set of locations:\n\n            ```\n            from geostat import GP, Model, Parameters\n            from geostat.kernel import Noise\n            import numpy as np\n\n            # Create parameters.\n            p = Parameters(nugget=1.)\n\n            # Create model\n            kernel = Noise(nugget=p.nugget)\n            model = Model(GP(0, kernel))\n\n            # Generate values based on locs\n            locs = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n            model.generate(locs)\n            generated_vals = model.vals  # Access the generated values\n            print(generated_vals)\n            # [0.45151083 1.23276189 0.3822659 ] (Values are non-deterministic)\n            ```\n\n        Notes:\n            - Conditional generation is currently not supported, and this method will raise an assertion error if \n            `self.locs` and `self.vals` are already defined.\n            - Generation from a distribution is not yet supported, and an assertion error will be raised if \n            `self.parameter_sample_size` is not `None`.\n            - If `cats` are provided, the data is permuted according to `cats` for stratified generation, and \n            the original order is restored before returning.\n        \"\"\"\n\n        assert self.locs is None and self.vals is None, 'Conditional generation not yet supported'\n        assert self.parameter_sample_size is None, 'Generation from a distribution not yet supported'\n\n        locs = np.array(locs)\n\n        # Permute datapoints if cats is given.\n        if cats is not None:\n            cats = np.array(cats)\n            perm = np.argsort(cats)\n            locs, cats = locs[perm], cats[perm]\n        else:\n            cats = np.zeros(locs.shape[:1], np.int32)\n            perm = None\n\n        m, S = gp_covariance(\n            self.gp,\n            self.warp(locs).run({}),\n            None if cats is None else tf.constant(cats, dtype=tf.int32))\n\n        vals = MVN(m, tf.linalg.cholesky(S)).sample().numpy()\n\n        # Restore order if things were permuted.\n        if perm is not None:\n            revperm = np.argsort(perm)\n            locs, vals, cats = locs[revperm], vals[revperm], cats[revperm]\n\n        self.locs = locs\n        self.vals = vals\n        self.cats = cats\n\n        return self\n\n    def predict(self, locs2, cats2=None, *, subsample=None, reduce=None, tracker=None, pair=False):\n        \"\"\"\n        Performs Gaussian Process (GP) predictions of the mean and variance for the given location data.\n        Supports batch predictions for large datasets and can handle categorical data.\n\n        Parameters:\n            locs2 (np.ndarray):\n                A NumPy array containing the input locations for which predictions are to be made.\n            cats2 (np.ndarray, optional):\n                A NumPy array containing categorical data for the prediction locations (`locs2`). If provided,\n                the data points will be permuted according to `cats2`. Default is None.\n            subsample (int, optional):\n                Specifies the number of parameter samples to be used for prediction when `parameter_sample_size` is set.\n                Only valid if parameters are sampled. Default is None.\n            reduce (str, optional):\n                Specifies the reduction method ('mean' or 'median') to aggregate predictions from multiple parameter samples.\n                Only valid if parameters are sampled. Default is None.\n            tracker (Callable, optional):\n                A tracking function for monitoring progress when making predictions across multiple samples. Default is None.\n            pair (bool, optional):\n                If True, performs pairwise predictions of mean and variance for each pair of input points in `locs2`.\n\n        Returns:\n            m (np.ndarray):\n                The predicted mean values for the input locations.\n            v (np.ndarray):\n                The predicted variances for the input locations.\n\n        Examples:\n            Making predictions for a set of locations:\n\n            ```\n            from geostat import GP, Model, Parameters\n            from geostat.kernel import SquaredExponential\n            import numpy as np\n\n            # Create parameters.\n            p = Parameters(sill=1.0, range=2.0)\n\n            # Create model\n            kernel = SquaredExponential(sill=p.sill, range=p.range)\n            model = Model(GP(0, kernel))\n\n            # Fit model\n            locs = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])\n            vals = np.array([10.0, 15.0, 20.0])\n            model.fit(locs, vals, step_size=0.05, iters=500)\n            # [iter    50 ll -40.27 time  2.29 reg  0.00 sill  6.35 range  1.96]\n            # [iter   100 ll -21.79 time  0.40 reg  0.00 sill 13.84 range  2.18]\n            # [iter   150 ll -16.17 time  0.39 reg  0.00 sill 22.31 range  2.44]\n            # [iter   200 ll -13.55 time  0.39 reg  0.00 sill 31.75 range  2.76]\n            # [iter   250 ll -12.08 time  0.38 reg  0.00 sill 42.08 range  3.12]\n            # [iter   300 ll -11.14 time  0.38 reg  0.00 sill 53.29 range  3.48]\n            # [iter   350 ll -10.50 time  0.38 reg  0.00 sill 65.36 range  3.85]\n            # [iter   400 ll -10.05 time  0.39 reg  0.00 sill 78.29 range  4.22]\n            # [iter   450 ll -9.70 time  0.39 reg  0.00 sill 92.07 range  4.59]\n            # [iter   500 ll -9.43 time  0.39 reg  0.00 sill 106.70 range  4.95]\n\n            # Run predictions\n            locs2 = np.array([[1.5, 1.5], [2.5, 4.0]])\n            mean, variance = model.predict(locs2)\n            print(mean)\n            # [ 9.89839798 18.77077269]\n            print(variance)\n            # [2.1572128  0.54444738]\n            ```\n\n        Notes:\n            - If `subsample` is specified, it should be used only when `parameter_sample_size` is defined.\n            - The `reduce` parameter allows aggregation of predictions, but it's valid only with sampled parameters.\n            - The method supports pairwise predictions by setting `pair=True`, which is useful for predicting \n            the covariance between two sets of locations.\n            - The internal `interpolate_batch` and `interpolate_pair_batch` functions handle the prediction computations\n            in a batched manner to support large datasets efficiently.\n        \"\"\"\n\n        assert subsample is None or self.parameter_sample_size is not None, \\\n            '`subsample` is only valid with sampled parameters'\n\n        assert reduce is None or self.parameter_sample_size is not None, \\\n            '`reduce` is only valid with sampled parameters'\n\n        assert subsample is None or reduce is None, \\\n            '`subsample` and `reduce` cannot both be given'\n\n        if tracker is None: tracker = lambda x: x\n\n        assert self.locs.shape[-1] == locs2.shape[-1], 'Mismatch in location dimensions'\n        if cats2 is not None:\n            assert cats2.shape == locs2.shape[:1], 'Mismatched shapes in cats and locs'\n        else:\n            cats2 = np.zeros(locs2.shape[:1], np.int32)\n\n        def interpolate_batch(A11i, locs1, vals1diff, cats1, locs2, cats2):\n            \"\"\"\n            Inputs:\n              locs1.shape = [N1, K]\n              vals1diff.shape = [N1]\n              cats1.shape = [N1]\n              locs2.shape = [N2, K]\n              cats2.shape = [N2]\n\n            Outputs:\n              u2_mean.shape = [N2]\n              u2_var.shape = [N2]\n            \"\"\"\n\n            N1 = len(locs1) # Number of measurements.\n\n            # Permute datapoints if cats is given.\n            if cats2 is not None:\n                perm = np.argsort(cats2)\n                locs2, cats2 = locs2[perm], cats2[perm]\n                locs2 = self.warp(locs2).run({})\n\n            _, A12 = gp_covariance2(\n                self.gp,\n                tf.constant(locs1, dtype=tf.float32),\n                tf.constant(cats1, dtype=tf.int32),\n                tf.constant(locs2, dtype=tf.float32),\n                tf.constant(cats2, dtype=tf.int32),\n                N1)\n\n            m2, A22 = gp_covariance(\n                self.gp,\n                tf.constant(locs2, dtype=tf.float32),\n                tf.constant(cats2, dtype=tf.int32))\n\n            # Restore order if things were permuted.\n            if cats2 is not None:\n                revperm = np.argsort(perm)\n                m2 = tf.gather(m2, revperm)\n                A12 = tf.gather(A12, revperm, axis=-1)\n                A22 = tf.gather(tf.gather(A22, revperm), revperm, axis=-1)\n\n            u2_mean = m2 + tf.einsum('ab,a-&gt;b', A12, tf.einsum('ab,b-&gt;a', A11i, vals1diff))\n            u2_var = tf.linalg.diag_part(A22) -  tf.einsum('ab,ab-&gt;b', A12, tf.matmul(A11i, A12))\n\n            return u2_mean, u2_var\n\n        def interpolate_pair_batch(A11i, locs1, vals1diff, cats1, locs2, cats2):\n            \"\"\"\n            Inputs:\n              locs1.shape = [N1, K]\n              vals1diff.shape = [N1]\n              cats1.shape = [N1]\n              locs2.shape = [N2, 2, K]\n              cats2.shape = [N2]\n\n            Outputs:\n              u2_mean.shape = [N2, 2]\n              u2_var.shape = [N2, 2, 2]\n            \"\"\"\n\n            N1 = len(locs1) # Number of measurements.\n            N2 = len(locs2) # Number of prediction pairs.\n\n            # Permute datapoints if cats is given.\n            perm = np.argsort(cats2)\n            locs2, cats2 = locs2[perm], cats2[perm]\n\n            # Warp locs2.\n            locs2_shape = locs2.shape\n            locs2 = locs2.reshape([-1, locs2_shape[-1]])  # Shape into matrix.\n            locs2 = self.warp(locs2).run({})\n            locs2 = tf.reshape(locs2, locs2_shape)  # Revert shape.\n\n            _, A12 = gp_covariance2(\n                self.gp,\n                tf.constant(locs1, dtype=tf.float32),\n                tf.constant(cats1, dtype=tf.int32),\n                tf.constant(locs2[:, 0, :], dtype=tf.float32),\n                tf.constant(cats2, dtype=tf.int32),\n                N1)\n\n            _, A13 = gp_covariance2(\n                self.gp,\n                tf.constant(locs1, dtype=tf.float32),\n                tf.constant(cats1, dtype=tf.int32),\n                tf.constant(locs2[:, 1, :], dtype=tf.float32),\n                tf.constant(cats2, dtype=tf.int32),\n                N1)\n\n            m2, A22 = gp_covariance(\n                self.gp,\n                tf.constant(locs2[:, 0, :], dtype=tf.float32),\n                tf.constant(cats2, dtype=tf.int32))\n\n            m3, A33 = gp_covariance(\n                self.gp,\n                tf.constant(locs2[:, 1, :], dtype=tf.float32),\n                tf.constant(cats2, dtype=tf.int32))\n\n            _, A23 = gp_covariance2(\n                self.gp,\n                tf.constant(locs2[:, 0, :], dtype=tf.float32),\n                tf.constant(cats2, dtype=tf.int32),\n                tf.constant(locs2[:, 1, :], dtype=tf.float32),\n                tf.constant(cats2, dtype=tf.int32),\n                N2)\n\n            # Reassemble into more useful shapes.\n\n            A12 = tf.stack([A12, A13], axis=-1) # [N1, N2, 2]\n\n            m2 = tf.stack([m2, m3], axis=-1) # [N2, 2]\n\n            A22 = tf.linalg.diag_part(A22)\n            A33 = tf.linalg.diag_part(A33)\n            A23 = tf.linalg.diag_part(A23)\n            A22 = tf.stack([tf.stack([A22, A23], axis=-1), tf.stack([A23, A33], axis=-1)], axis=-2) # [N2, 2, 2]\n\n            # Restore order if things were permuted.\n            revperm = np.argsort(perm)\n            m2 = tf.gather(m2, revperm)\n            A12 = tf.gather(A12, revperm, axis=1)\n            A22 = tf.gather(A22, revperm)\n\n            u2_mean = m2 + tf.einsum('abc,a-&gt;bc', A12, tf.einsum('ab,b-&gt;a', A11i, vals1diff))\n            u2_var = A22 - tf.einsum('abc,abd-&gt;bcd', A12, tf.einsum('ae,ebd-&gt;abd', A11i, A12))\n\n            return u2_mean, u2_var\n\n        def interpolate(locs1, vals1, cats1, locs2, cats2, pair=False):\n            # Interpolate in batches.\n            batch_size = locs1.shape[0] // 2\n\n            for_gp = []\n\n            for start in np.arange(0, len(locs2), batch_size):\n                stop = start + batch_size\n                subset = locs2[start:stop], cats2[start:stop]\n                for_gp.append(subset)\n\n            # Permute datapoints if cats is given.\n            if cats1 is not None:\n                perm = np.argsort(cats1)\n                locs1, vals1, cats1 = locs1[perm], vals1[perm], cats1[perm]\n\n            locs1 = self.warp(locs1).run({})\n\n            m1, A11 = gp_covariance(\n                self.gp,\n                tf.constant(locs1, dtype=tf.float32),\n                tf.constant(cats1, dtype=tf.int32))\n\n            A11i = tf.linalg.inv(A11)\n\n            u2_mean_s = []\n            u2_var_s = []\n\n            f = interpolate_pair_batch if pair else interpolate_batch\n\n            for locs_subset, cats_subset in for_gp:\n                u2_mean, u2_var = f(A11i, locs1, vals1 - m1, cats1, locs_subset, cats_subset)\n                u2_mean = u2_mean.numpy()\n                u2_var = u2_var.numpy()\n                u2_mean_s.append(u2_mean)\n                u2_var_s.append(u2_var)\n\n            u2_mean = np.concatenate(u2_mean_s)\n            u2_var = np.concatenate(u2_var_s)\n\n            return u2_mean, u2_var\n\n        if self.parameter_sample_size is None:\n            m, v = interpolate(self.locs, self.vals, self.cats, locs2, cats2, pair)\n        elif reduce == 'median':\n            raise NotImplementedError\n            p = tf.nest.map_structure(lambda x: np.quantile(x, 0.5, axis=0).astype(np.float32), self.parameters)\n            m, v = interpolate(self.locs, self.vals, self.cats, locs2, cats2, p, pair)\n        elif reduce == 'mean':\n            raise NotImplementedError\n            p = tf.nest.map_structure(lambda x: x.mean(axis=0).astype(np.float32), self.parameters)\n            m, v = interpolate(self.locs, self.vals, self.cats, locs2, cats2, p, pair)\n        else:\n            raise NotImplementedError\n            samples = self.parameter_sample_size\n\n            if subsample is not None:\n                assert subsample &lt;= samples, '`subsample` may not exceed sample size'\n            else:\n                subsample = samples\n\n            # Thin by picking roughly equally-spaced samples.\n            a = np.arange(samples) * subsample / samples % 1\n            pick = np.concatenate([[True], a[1:] &gt;= a[:-1]])\n            parameters = tf.nest.map_structure(lambda x: x[pick], self.parameters)\n\n            # Make a prediction for each sample.\n            results = []\n            for i in tracker(range(subsample)):\n                p = tf.nest.map_structure(lambda x: x[i], parameters)\n                results.append(interpolate(self.locs, self.vals, self.cats, locs2, cats2, p, pair))\n\n            mm, vv = [np.stack(x) for x in zip(*results)]\n            m = mm.mean(axis=0)\n            v = (np.square(mm) + vv).mean(axis=0) - np.square(m)\n\n        return m, v\n</code></pre>"},{"location":"api/#src.geostat.model.Model.fit","title":"<code>fit(locs, vals, cats=None, step_size=0.01, iters=100, reg=None)</code>","text":"<p>Trains the model using the provided location and value data by optimizing the parameters of the Gaussian Process (GP) using the Adam optimizer. Optionally performs regularization and can handle categorical data.</p> <p>Parameters:</p> Name Type Description Default <code>locs</code> <code>ndarray</code> <p>A NumPy array containing the input locations for training.</p> required <code>vals</code> <code>ndarray</code> <p>A NumPy array containing observed values corresponding to the <code>locs</code>.</p> required <code>cats</code> <code>ndarray</code> <p>A NumPy array containing categorical data for each observation in <code>locs</code>. If provided, the data is sorted according to <code>cats</code> to enable stratified training. Defaults to None.</p> <code>None</code> <code>step_size</code> <code>float</code> <p>The learning rate for the Adam optimizer.</p> <code>0.01</code> <code>iters</code> <code>int</code> <p>The total number of iterations to run for training.</p> <code>100</code> <code>reg</code> <code>float or None</code> <p>Regularization penalty parameter. If None, no regularization is applied.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>Model</code> <p>The model instance with updated parameters, allowing for method chaining.</p> <p>Examples:</p> <p>Fitting a model using training data:</p> <pre><code>from geostat import GP, Model, Parameters\nfrom geostat.kernel import Noise\nimport numpy as np\n\n# Create parameters.\np = Parameters(nugget=1.)\n\n# Create model\nkernel = Noise(nugget=p.nugget)\nmodel = Model(GP(0, kernel))\n\n# Fit model\nlocs = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])\nvals = np.array([10.0, 15.0, 20.0])\nmodel.fit(locs, vals, step_size=0.05, iters=500)\n# [iter    50 ll -63.71 time  2.72 reg  0.00 nugget  6.37]\n# [iter   100 ll -32.94 time  0.25 reg  0.00 nugget 13.97]\n# [iter   150 ll -23.56 time  0.25 reg  0.00 nugget 22.65]\n# [iter   200 ll -19.26 time  0.25 reg  0.00 nugget 32.27]\n# [iter   250 ll -16.92 time  0.25 reg  0.00 nugget 42.63]\n# [iter   300 ll -15.52 time  0.24 reg  0.00 nugget 53.50]\n# [iter   350 ll -14.63 time  0.24 reg  0.00 nugget 64.71]\n# [iter   400 ll -14.03 time  0.24 reg  0.00 nugget 76.10]\n# [iter   450 ll -13.61 time  0.25 reg  0.00 nugget 87.52]\n# [iter   500 ll -13.32 time  0.24 reg  0.00 nugget 98.85]\n</code></pre> <p>Using categorical data for training:</p> <pre><code>cats = np.array([1, 1, 2])\nmodel.fit(locs, vals, cats=cats, step_size=0.01, iters=300)\n# [iter    30 ll -12.84 time  0.25 reg  0.00 nugget 131.53]\n# [iter    60 ll -12.62 time  0.15 reg  0.00 nugget 164.41]\n# [iter    90 ll -12.53 time  0.16 reg  0.00 nugget 191.70]\n# [iter   120 ll -12.50 time  0.16 reg  0.00 nugget 211.74]\n# [iter   150 ll -12.49 time  0.15 reg  0.00 nugget 225.07]\n# [iter   180 ll -12.49 time  0.16 reg  0.00 nugget 233.15]\n# [iter   210 ll -12.49 time  0.15 reg  0.00 nugget 237.64]\n# [iter   240 ll -12.49 time  0.15 reg  0.00 nugget 239.92]\n# [iter   270 ll -12.49 time  0.15 reg  0.00 nugget 240.98]\n# [iter   300 ll -12.49 time  0.15 reg  0.00 nugget 241.42]\n</code></pre> Notes <ul> <li>The <code>fit</code> method uses the Adam optimizer to minimize the negative log-likelihood (<code>ll</code>) and any regularization  penalties specified by <code>reg</code>.</li> <li>During training, if <code>cats</code> are provided, data points are sorted according to <code>cats</code> to ensure grouped training.</li> <li>The <code>verbose</code> flag determines whether training progress is printed after each iteration.</li> <li>After training, parameter values are saved and can be accessed or updated using the model's attributes.</li> </ul> Source code in <code>src/geostat/model.py</code> <pre><code>def fit(self, locs, vals, cats=None, step_size=0.01, iters=100, reg=None):\n    \"\"\"\n    Trains the model using the provided location and value data by optimizing the parameters of the Gaussian Process (GP)\n    using the Adam optimizer. Optionally performs regularization and can handle categorical data.\n\n    Parameters:        \n        locs (np.ndarray):\n            A NumPy array containing the input locations for training.\n        vals (np.ndarray):\n            A NumPy array containing observed values corresponding to the `locs`.\n        cats (np.ndarray, optional):\n            A NumPy array containing categorical data for each observation in `locs`. If provided,\n            the data is sorted according to `cats` to enable stratified training. Defaults to None.\n        step_size (float, optional):\n            The learning rate for the Adam optimizer.\n        iters (int, optional):\n            The total number of iterations to run for training.\n        reg (float or None, optional):\n            Regularization penalty parameter. If None, no regularization is applied.\n\n    Returns:\n        self (Model):\n            The model instance with updated parameters, allowing for method chaining.\n\n    Examples:\n        Fitting a model using training data:\n\n        ```\n        from geostat import GP, Model, Parameters\n        from geostat.kernel import Noise\n        import numpy as np\n\n        # Create parameters.\n        p = Parameters(nugget=1.)\n\n        # Create model\n        kernel = Noise(nugget=p.nugget)\n        model = Model(GP(0, kernel))\n\n        # Fit model\n        locs = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])\n        vals = np.array([10.0, 15.0, 20.0])\n        model.fit(locs, vals, step_size=0.05, iters=500)\n        # [iter    50 ll -63.71 time  2.72 reg  0.00 nugget  6.37]\n        # [iter   100 ll -32.94 time  0.25 reg  0.00 nugget 13.97]\n        # [iter   150 ll -23.56 time  0.25 reg  0.00 nugget 22.65]\n        # [iter   200 ll -19.26 time  0.25 reg  0.00 nugget 32.27]\n        # [iter   250 ll -16.92 time  0.25 reg  0.00 nugget 42.63]\n        # [iter   300 ll -15.52 time  0.24 reg  0.00 nugget 53.50]\n        # [iter   350 ll -14.63 time  0.24 reg  0.00 nugget 64.71]\n        # [iter   400 ll -14.03 time  0.24 reg  0.00 nugget 76.10]\n        # [iter   450 ll -13.61 time  0.25 reg  0.00 nugget 87.52]\n        # [iter   500 ll -13.32 time  0.24 reg  0.00 nugget 98.85]\n        ```\n\n        Using categorical data for training:\n\n        ```\n        cats = np.array([1, 1, 2])\n        model.fit(locs, vals, cats=cats, step_size=0.01, iters=300)\n        # [iter    30 ll -12.84 time  0.25 reg  0.00 nugget 131.53]\n        # [iter    60 ll -12.62 time  0.15 reg  0.00 nugget 164.41]\n        # [iter    90 ll -12.53 time  0.16 reg  0.00 nugget 191.70]\n        # [iter   120 ll -12.50 time  0.16 reg  0.00 nugget 211.74]\n        # [iter   150 ll -12.49 time  0.15 reg  0.00 nugget 225.07]\n        # [iter   180 ll -12.49 time  0.16 reg  0.00 nugget 233.15]\n        # [iter   210 ll -12.49 time  0.15 reg  0.00 nugget 237.64]\n        # [iter   240 ll -12.49 time  0.15 reg  0.00 nugget 239.92]\n        # [iter   270 ll -12.49 time  0.15 reg  0.00 nugget 240.98]\n        # [iter   300 ll -12.49 time  0.15 reg  0.00 nugget 241.42]\n        ```\n\n    Notes:\n        - The `fit` method uses the Adam optimizer to minimize the negative log-likelihood (`ll`) and any regularization \n        penalties specified by `reg`.\n        - During training, if `cats` are provided, data points are sorted according to `cats` to ensure grouped training.\n        - The `verbose` flag determines whether training progress is printed after each iteration.\n        - After training, parameter values are saved and can be accessed or updated using the model's attributes.\n    \"\"\"\n\n    # Collect parameters and create TF parameters.\n    parameters = self.gather_vars()\n\n    # Permute datapoints if cats is given.\n    if cats is not None:\n        cats = np.array(cats)\n        perm = np.argsort(cats)\n        locs, vals, cats = locs[perm], vals[perm], cats[perm]\n    else:\n        cats = np.zeros(locs.shape[:1], np.int32)\n        perm = None\n\n    # Data dict.\n    self.data = {\n        'warplocs': self.warp(locs),\n        'vals': tf.constant(vals, dtype=tf.float32),\n        'cats': tf.constant(cats, dtype=tf.int32)}\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=step_size)\n\n    j = 0 # Iteration count.\n    for i in range(10):\n        t0 = time.time()\n        while j &lt; (i + 1) * iters / 10:\n            ll, reg_penalty = gp_train_step(\n                optimizer, self.data, parameters, self.gp, reg)\n            j += 1\n\n        time_elapsed = time.time() - t0\n        if self.verbose == True:\n            self.report(\n              dict(iter=j, ll=ll, time=time_elapsed, reg=reg_penalty) |\n              {p.name: p.surface() for p in parameters.values()})\n\n    # Save parameter values.\n    for p in parameters.values():\n        p.update_value()\n\n    # Restore order if things were permuted.\n    if perm is not None:\n        revperm = np.argsort(perm)\n        locs, vals, cats = locs[revperm], vals[revperm], cats[revperm]\n\n    self.locs = locs\n    self.vals = vals\n    self.cats = cats\n\n    return self\n</code></pre>"},{"location":"api/#src.geostat.model.Model.generate","title":"<code>generate(locs, cats=None)</code>","text":"<p>Generates synthetic data values from the Gaussian Process (GP) model based on the provided location data. This method simulates values based on the GP's covariance structure, allowing for random sample generation.</p> <p>Parameters:</p> Name Type Description Default <code>locs</code> <code>ndarray</code> <p>A NumPy array containing the input locations for which to generate synthetic values.</p> required <code>cats</code> <code>ndarray</code> <p>A NumPy array containing categorical data corresponding to <code>locs</code>. If provided, data points  are permuted according to <code>cats</code> for stratified generation. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>Model</code> <p>The model instance with generated values stored in <code>self.vals</code> and corresponding locations stored  in <code>self.locs</code>. This enables method chaining.</p> <p>Examples:</p> <p>Generating synthetic values for a set of locations:</p> <pre><code>from geostat import GP, Model, Parameters\nfrom geostat.kernel import Noise\nimport numpy as np\n\n# Create parameters.\np = Parameters(nugget=1.)\n\n# Create model\nkernel = Noise(nugget=p.nugget)\nmodel = Model(GP(0, kernel))\n\n# Generate values based on locs\nlocs = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\nmodel.generate(locs)\ngenerated_vals = model.vals  # Access the generated values\nprint(generated_vals)\n# [0.45151083 1.23276189 0.3822659 ] (Values are non-deterministic)\n</code></pre> Notes <ul> <li>Conditional generation is currently not supported, and this method will raise an assertion error if  <code>self.locs</code> and <code>self.vals</code> are already defined.</li> <li>Generation from a distribution is not yet supported, and an assertion error will be raised if  <code>self.parameter_sample_size</code> is not <code>None</code>.</li> <li>If <code>cats</code> are provided, the data is permuted according to <code>cats</code> for stratified generation, and  the original order is restored before returning.</li> </ul> Source code in <code>src/geostat/model.py</code> <pre><code>def generate(self, locs, cats=None):\n    \"\"\"\n    Generates synthetic data values from the Gaussian Process (GP) model based on the provided location data.\n    This method simulates values based on the GP's covariance structure, allowing for random sample generation.\n\n    Parameters:\n        locs (np.ndarray):\n            A NumPy array containing the input locations for which to generate synthetic values.\n        cats (np.ndarray, optional):\n            A NumPy array containing categorical data corresponding to `locs`. If provided, data points \n            are permuted according to `cats` for stratified generation. Defaults to None.\n\n    Returns:\n        self (Model):\n            The model instance with generated values stored in `self.vals` and corresponding locations stored \n            in `self.locs`. This enables method chaining.\n\n    Examples:\n        Generating synthetic values for a set of locations:\n\n        ```\n        from geostat import GP, Model, Parameters\n        from geostat.kernel import Noise\n        import numpy as np\n\n        # Create parameters.\n        p = Parameters(nugget=1.)\n\n        # Create model\n        kernel = Noise(nugget=p.nugget)\n        model = Model(GP(0, kernel))\n\n        # Generate values based on locs\n        locs = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n        model.generate(locs)\n        generated_vals = model.vals  # Access the generated values\n        print(generated_vals)\n        # [0.45151083 1.23276189 0.3822659 ] (Values are non-deterministic)\n        ```\n\n    Notes:\n        - Conditional generation is currently not supported, and this method will raise an assertion error if \n        `self.locs` and `self.vals` are already defined.\n        - Generation from a distribution is not yet supported, and an assertion error will be raised if \n        `self.parameter_sample_size` is not `None`.\n        - If `cats` are provided, the data is permuted according to `cats` for stratified generation, and \n        the original order is restored before returning.\n    \"\"\"\n\n    assert self.locs is None and self.vals is None, 'Conditional generation not yet supported'\n    assert self.parameter_sample_size is None, 'Generation from a distribution not yet supported'\n\n    locs = np.array(locs)\n\n    # Permute datapoints if cats is given.\n    if cats is not None:\n        cats = np.array(cats)\n        perm = np.argsort(cats)\n        locs, cats = locs[perm], cats[perm]\n    else:\n        cats = np.zeros(locs.shape[:1], np.int32)\n        perm = None\n\n    m, S = gp_covariance(\n        self.gp,\n        self.warp(locs).run({}),\n        None if cats is None else tf.constant(cats, dtype=tf.int32))\n\n    vals = MVN(m, tf.linalg.cholesky(S)).sample().numpy()\n\n    # Restore order if things were permuted.\n    if perm is not None:\n        revperm = np.argsort(perm)\n        locs, vals, cats = locs[revperm], vals[revperm], cats[revperm]\n\n    self.locs = locs\n    self.vals = vals\n    self.cats = cats\n\n    return self\n</code></pre>"},{"location":"api/#src.geostat.model.Model.predict","title":"<code>predict(locs2, cats2=None, *, subsample=None, reduce=None, tracker=None, pair=False)</code>","text":"<p>Performs Gaussian Process (GP) predictions of the mean and variance for the given location data. Supports batch predictions for large datasets and can handle categorical data.</p> <p>Parameters:</p> Name Type Description Default <code>locs2</code> <code>ndarray</code> <p>A NumPy array containing the input locations for which predictions are to be made.</p> required <code>cats2</code> <code>ndarray</code> <p>A NumPy array containing categorical data for the prediction locations (<code>locs2</code>). If provided, the data points will be permuted according to <code>cats2</code>. Default is None.</p> <code>None</code> <code>subsample</code> <code>int</code> <p>Specifies the number of parameter samples to be used for prediction when <code>parameter_sample_size</code> is set. Only valid if parameters are sampled. Default is None.</p> <code>None</code> <code>reduce</code> <code>str</code> <p>Specifies the reduction method ('mean' or 'median') to aggregate predictions from multiple parameter samples. Only valid if parameters are sampled. Default is None.</p> <code>None</code> <code>tracker</code> <code>Callable</code> <p>A tracking function for monitoring progress when making predictions across multiple samples. Default is None.</p> <code>None</code> <code>pair</code> <code>bool</code> <p>If True, performs pairwise predictions of mean and variance for each pair of input points in <code>locs2</code>.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>m</code> <code>ndarray</code> <p>The predicted mean values for the input locations.</p> <code>v</code> <code>ndarray</code> <p>The predicted variances for the input locations.</p> <p>Examples:</p> <p>Making predictions for a set of locations:</p> <pre><code>from geostat import GP, Model, Parameters\nfrom geostat.kernel import SquaredExponential\nimport numpy as np\n\n# Create parameters.\np = Parameters(sill=1.0, range=2.0)\n\n# Create model\nkernel = SquaredExponential(sill=p.sill, range=p.range)\nmodel = Model(GP(0, kernel))\n\n# Fit model\nlocs = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])\nvals = np.array([10.0, 15.0, 20.0])\nmodel.fit(locs, vals, step_size=0.05, iters=500)\n# [iter    50 ll -40.27 time  2.29 reg  0.00 sill  6.35 range  1.96]\n# [iter   100 ll -21.79 time  0.40 reg  0.00 sill 13.84 range  2.18]\n# [iter   150 ll -16.17 time  0.39 reg  0.00 sill 22.31 range  2.44]\n# [iter   200 ll -13.55 time  0.39 reg  0.00 sill 31.75 range  2.76]\n# [iter   250 ll -12.08 time  0.38 reg  0.00 sill 42.08 range  3.12]\n# [iter   300 ll -11.14 time  0.38 reg  0.00 sill 53.29 range  3.48]\n# [iter   350 ll -10.50 time  0.38 reg  0.00 sill 65.36 range  3.85]\n# [iter   400 ll -10.05 time  0.39 reg  0.00 sill 78.29 range  4.22]\n# [iter   450 ll -9.70 time  0.39 reg  0.00 sill 92.07 range  4.59]\n# [iter   500 ll -9.43 time  0.39 reg  0.00 sill 106.70 range  4.95]\n\n# Run predictions\nlocs2 = np.array([[1.5, 1.5], [2.5, 4.0]])\nmean, variance = model.predict(locs2)\nprint(mean)\n# [ 9.89839798 18.77077269]\nprint(variance)\n# [2.1572128  0.54444738]\n</code></pre> Notes <ul> <li>If <code>subsample</code> is specified, it should be used only when <code>parameter_sample_size</code> is defined.</li> <li>The <code>reduce</code> parameter allows aggregation of predictions, but it's valid only with sampled parameters.</li> <li>The method supports pairwise predictions by setting <code>pair=True</code>, which is useful for predicting  the covariance between two sets of locations.</li> <li>The internal <code>interpolate_batch</code> and <code>interpolate_pair_batch</code> functions handle the prediction computations in a batched manner to support large datasets efficiently.</li> </ul> Source code in <code>src/geostat/model.py</code> <pre><code>def predict(self, locs2, cats2=None, *, subsample=None, reduce=None, tracker=None, pair=False):\n    \"\"\"\n    Performs Gaussian Process (GP) predictions of the mean and variance for the given location data.\n    Supports batch predictions for large datasets and can handle categorical data.\n\n    Parameters:\n        locs2 (np.ndarray):\n            A NumPy array containing the input locations for which predictions are to be made.\n        cats2 (np.ndarray, optional):\n            A NumPy array containing categorical data for the prediction locations (`locs2`). If provided,\n            the data points will be permuted according to `cats2`. Default is None.\n        subsample (int, optional):\n            Specifies the number of parameter samples to be used for prediction when `parameter_sample_size` is set.\n            Only valid if parameters are sampled. Default is None.\n        reduce (str, optional):\n            Specifies the reduction method ('mean' or 'median') to aggregate predictions from multiple parameter samples.\n            Only valid if parameters are sampled. Default is None.\n        tracker (Callable, optional):\n            A tracking function for monitoring progress when making predictions across multiple samples. Default is None.\n        pair (bool, optional):\n            If True, performs pairwise predictions of mean and variance for each pair of input points in `locs2`.\n\n    Returns:\n        m (np.ndarray):\n            The predicted mean values for the input locations.\n        v (np.ndarray):\n            The predicted variances for the input locations.\n\n    Examples:\n        Making predictions for a set of locations:\n\n        ```\n        from geostat import GP, Model, Parameters\n        from geostat.kernel import SquaredExponential\n        import numpy as np\n\n        # Create parameters.\n        p = Parameters(sill=1.0, range=2.0)\n\n        # Create model\n        kernel = SquaredExponential(sill=p.sill, range=p.range)\n        model = Model(GP(0, kernel))\n\n        # Fit model\n        locs = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])\n        vals = np.array([10.0, 15.0, 20.0])\n        model.fit(locs, vals, step_size=0.05, iters=500)\n        # [iter    50 ll -40.27 time  2.29 reg  0.00 sill  6.35 range  1.96]\n        # [iter   100 ll -21.79 time  0.40 reg  0.00 sill 13.84 range  2.18]\n        # [iter   150 ll -16.17 time  0.39 reg  0.00 sill 22.31 range  2.44]\n        # [iter   200 ll -13.55 time  0.39 reg  0.00 sill 31.75 range  2.76]\n        # [iter   250 ll -12.08 time  0.38 reg  0.00 sill 42.08 range  3.12]\n        # [iter   300 ll -11.14 time  0.38 reg  0.00 sill 53.29 range  3.48]\n        # [iter   350 ll -10.50 time  0.38 reg  0.00 sill 65.36 range  3.85]\n        # [iter   400 ll -10.05 time  0.39 reg  0.00 sill 78.29 range  4.22]\n        # [iter   450 ll -9.70 time  0.39 reg  0.00 sill 92.07 range  4.59]\n        # [iter   500 ll -9.43 time  0.39 reg  0.00 sill 106.70 range  4.95]\n\n        # Run predictions\n        locs2 = np.array([[1.5, 1.5], [2.5, 4.0]])\n        mean, variance = model.predict(locs2)\n        print(mean)\n        # [ 9.89839798 18.77077269]\n        print(variance)\n        # [2.1572128  0.54444738]\n        ```\n\n    Notes:\n        - If `subsample` is specified, it should be used only when `parameter_sample_size` is defined.\n        - The `reduce` parameter allows aggregation of predictions, but it's valid only with sampled parameters.\n        - The method supports pairwise predictions by setting `pair=True`, which is useful for predicting \n        the covariance between two sets of locations.\n        - The internal `interpolate_batch` and `interpolate_pair_batch` functions handle the prediction computations\n        in a batched manner to support large datasets efficiently.\n    \"\"\"\n\n    assert subsample is None or self.parameter_sample_size is not None, \\\n        '`subsample` is only valid with sampled parameters'\n\n    assert reduce is None or self.parameter_sample_size is not None, \\\n        '`reduce` is only valid with sampled parameters'\n\n    assert subsample is None or reduce is None, \\\n        '`subsample` and `reduce` cannot both be given'\n\n    if tracker is None: tracker = lambda x: x\n\n    assert self.locs.shape[-1] == locs2.shape[-1], 'Mismatch in location dimensions'\n    if cats2 is not None:\n        assert cats2.shape == locs2.shape[:1], 'Mismatched shapes in cats and locs'\n    else:\n        cats2 = np.zeros(locs2.shape[:1], np.int32)\n\n    def interpolate_batch(A11i, locs1, vals1diff, cats1, locs2, cats2):\n        \"\"\"\n        Inputs:\n          locs1.shape = [N1, K]\n          vals1diff.shape = [N1]\n          cats1.shape = [N1]\n          locs2.shape = [N2, K]\n          cats2.shape = [N2]\n\n        Outputs:\n          u2_mean.shape = [N2]\n          u2_var.shape = [N2]\n        \"\"\"\n\n        N1 = len(locs1) # Number of measurements.\n\n        # Permute datapoints if cats is given.\n        if cats2 is not None:\n            perm = np.argsort(cats2)\n            locs2, cats2 = locs2[perm], cats2[perm]\n            locs2 = self.warp(locs2).run({})\n\n        _, A12 = gp_covariance2(\n            self.gp,\n            tf.constant(locs1, dtype=tf.float32),\n            tf.constant(cats1, dtype=tf.int32),\n            tf.constant(locs2, dtype=tf.float32),\n            tf.constant(cats2, dtype=tf.int32),\n            N1)\n\n        m2, A22 = gp_covariance(\n            self.gp,\n            tf.constant(locs2, dtype=tf.float32),\n            tf.constant(cats2, dtype=tf.int32))\n\n        # Restore order if things were permuted.\n        if cats2 is not None:\n            revperm = np.argsort(perm)\n            m2 = tf.gather(m2, revperm)\n            A12 = tf.gather(A12, revperm, axis=-1)\n            A22 = tf.gather(tf.gather(A22, revperm), revperm, axis=-1)\n\n        u2_mean = m2 + tf.einsum('ab,a-&gt;b', A12, tf.einsum('ab,b-&gt;a', A11i, vals1diff))\n        u2_var = tf.linalg.diag_part(A22) -  tf.einsum('ab,ab-&gt;b', A12, tf.matmul(A11i, A12))\n\n        return u2_mean, u2_var\n\n    def interpolate_pair_batch(A11i, locs1, vals1diff, cats1, locs2, cats2):\n        \"\"\"\n        Inputs:\n          locs1.shape = [N1, K]\n          vals1diff.shape = [N1]\n          cats1.shape = [N1]\n          locs2.shape = [N2, 2, K]\n          cats2.shape = [N2]\n\n        Outputs:\n          u2_mean.shape = [N2, 2]\n          u2_var.shape = [N2, 2, 2]\n        \"\"\"\n\n        N1 = len(locs1) # Number of measurements.\n        N2 = len(locs2) # Number of prediction pairs.\n\n        # Permute datapoints if cats is given.\n        perm = np.argsort(cats2)\n        locs2, cats2 = locs2[perm], cats2[perm]\n\n        # Warp locs2.\n        locs2_shape = locs2.shape\n        locs2 = locs2.reshape([-1, locs2_shape[-1]])  # Shape into matrix.\n        locs2 = self.warp(locs2).run({})\n        locs2 = tf.reshape(locs2, locs2_shape)  # Revert shape.\n\n        _, A12 = gp_covariance2(\n            self.gp,\n            tf.constant(locs1, dtype=tf.float32),\n            tf.constant(cats1, dtype=tf.int32),\n            tf.constant(locs2[:, 0, :], dtype=tf.float32),\n            tf.constant(cats2, dtype=tf.int32),\n            N1)\n\n        _, A13 = gp_covariance2(\n            self.gp,\n            tf.constant(locs1, dtype=tf.float32),\n            tf.constant(cats1, dtype=tf.int32),\n            tf.constant(locs2[:, 1, :], dtype=tf.float32),\n            tf.constant(cats2, dtype=tf.int32),\n            N1)\n\n        m2, A22 = gp_covariance(\n            self.gp,\n            tf.constant(locs2[:, 0, :], dtype=tf.float32),\n            tf.constant(cats2, dtype=tf.int32))\n\n        m3, A33 = gp_covariance(\n            self.gp,\n            tf.constant(locs2[:, 1, :], dtype=tf.float32),\n            tf.constant(cats2, dtype=tf.int32))\n\n        _, A23 = gp_covariance2(\n            self.gp,\n            tf.constant(locs2[:, 0, :], dtype=tf.float32),\n            tf.constant(cats2, dtype=tf.int32),\n            tf.constant(locs2[:, 1, :], dtype=tf.float32),\n            tf.constant(cats2, dtype=tf.int32),\n            N2)\n\n        # Reassemble into more useful shapes.\n\n        A12 = tf.stack([A12, A13], axis=-1) # [N1, N2, 2]\n\n        m2 = tf.stack([m2, m3], axis=-1) # [N2, 2]\n\n        A22 = tf.linalg.diag_part(A22)\n        A33 = tf.linalg.diag_part(A33)\n        A23 = tf.linalg.diag_part(A23)\n        A22 = tf.stack([tf.stack([A22, A23], axis=-1), tf.stack([A23, A33], axis=-1)], axis=-2) # [N2, 2, 2]\n\n        # Restore order if things were permuted.\n        revperm = np.argsort(perm)\n        m2 = tf.gather(m2, revperm)\n        A12 = tf.gather(A12, revperm, axis=1)\n        A22 = tf.gather(A22, revperm)\n\n        u2_mean = m2 + tf.einsum('abc,a-&gt;bc', A12, tf.einsum('ab,b-&gt;a', A11i, vals1diff))\n        u2_var = A22 - tf.einsum('abc,abd-&gt;bcd', A12, tf.einsum('ae,ebd-&gt;abd', A11i, A12))\n\n        return u2_mean, u2_var\n\n    def interpolate(locs1, vals1, cats1, locs2, cats2, pair=False):\n        # Interpolate in batches.\n        batch_size = locs1.shape[0] // 2\n\n        for_gp = []\n\n        for start in np.arange(0, len(locs2), batch_size):\n            stop = start + batch_size\n            subset = locs2[start:stop], cats2[start:stop]\n            for_gp.append(subset)\n\n        # Permute datapoints if cats is given.\n        if cats1 is not None:\n            perm = np.argsort(cats1)\n            locs1, vals1, cats1 = locs1[perm], vals1[perm], cats1[perm]\n\n        locs1 = self.warp(locs1).run({})\n\n        m1, A11 = gp_covariance(\n            self.gp,\n            tf.constant(locs1, dtype=tf.float32),\n            tf.constant(cats1, dtype=tf.int32))\n\n        A11i = tf.linalg.inv(A11)\n\n        u2_mean_s = []\n        u2_var_s = []\n\n        f = interpolate_pair_batch if pair else interpolate_batch\n\n        for locs_subset, cats_subset in for_gp:\n            u2_mean, u2_var = f(A11i, locs1, vals1 - m1, cats1, locs_subset, cats_subset)\n            u2_mean = u2_mean.numpy()\n            u2_var = u2_var.numpy()\n            u2_mean_s.append(u2_mean)\n            u2_var_s.append(u2_var)\n\n        u2_mean = np.concatenate(u2_mean_s)\n        u2_var = np.concatenate(u2_var_s)\n\n        return u2_mean, u2_var\n\n    if self.parameter_sample_size is None:\n        m, v = interpolate(self.locs, self.vals, self.cats, locs2, cats2, pair)\n    elif reduce == 'median':\n        raise NotImplementedError\n        p = tf.nest.map_structure(lambda x: np.quantile(x, 0.5, axis=0).astype(np.float32), self.parameters)\n        m, v = interpolate(self.locs, self.vals, self.cats, locs2, cats2, p, pair)\n    elif reduce == 'mean':\n        raise NotImplementedError\n        p = tf.nest.map_structure(lambda x: x.mean(axis=0).astype(np.float32), self.parameters)\n        m, v = interpolate(self.locs, self.vals, self.cats, locs2, cats2, p, pair)\n    else:\n        raise NotImplementedError\n        samples = self.parameter_sample_size\n\n        if subsample is not None:\n            assert subsample &lt;= samples, '`subsample` may not exceed sample size'\n        else:\n            subsample = samples\n\n        # Thin by picking roughly equally-spaced samples.\n        a = np.arange(samples) * subsample / samples % 1\n        pick = np.concatenate([[True], a[1:] &gt;= a[:-1]])\n        parameters = tf.nest.map_structure(lambda x: x[pick], self.parameters)\n\n        # Make a prediction for each sample.\n        results = []\n        for i in tracker(range(subsample)):\n            p = tf.nest.map_structure(lambda x: x[i], parameters)\n            results.append(interpolate(self.locs, self.vals, self.cats, locs2, cats2, p, pair))\n\n        mm, vv = [np.stack(x) for x in zip(*results)]\n        m = mm.mean(axis=0)\n        v = (np.square(mm) + vv).mean(axis=0) - np.square(m)\n\n    return m, v\n</code></pre>"},{"location":"api/#src.geostat.model.Model.set","title":"<code>set(**values)</code>","text":"<p>Sets the values of the model's parameters based on the provided keyword arguments. Each parameter specified must exist in the model; otherwise, a <code>ValueError</code> is raised.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>keyword arguments</code> <p>A dictionary of parameter names and their corresponding values that should be  set in the model. Each key corresponds to a parameter name, and the value is  the value to be assigned to that parameter.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>self</code> <code>Model</code> <p>The model instance with updated parameter values, allowing for method chaining.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a provided parameter name does not exist in the model's parameters.</p> <p>Examples:</p> <p>Update parameter value using <code>set</code>:</p> <pre><code>from geostat import GP, Model, Parameters\nfrom geostat.kernel import Noise\n\n# Create parameters.\np = Parameters(nugget=1.)\n\n# Create model\nkernel = Noise(nugget=p.nugget)\nmodel = Model(GP(0, kernel))\n\n# Update parameters\nmodel.set(nugget=0.5)\n</code></pre> Notes <ul> <li>The <code>set</code> method retrieves the current parameters using <code>gather_vars</code> and updates  their values. The associated TensorFlow variables are also recreated.</li> <li>This method is useful for dynamically updating the model's parameters after initialization.</li> </ul> Source code in <code>src/geostat/model.py</code> <pre><code>def set(self, **values):\n    \"\"\"\n    Sets the values of the model's parameters based on the provided keyword arguments.\n    Each parameter specified must exist in the model; otherwise, a `ValueError` is raised.\n\n    Parameters:\n        values (keyword arguments):\n            A dictionary of parameter names and their corresponding values that should be \n            set in the model. Each key corresponds to a parameter name, and the value is \n            the value to be assigned to that parameter.\n\n    Returns:\n        self (Model):\n            The model instance with updated parameter values, allowing for method chaining.\n\n    Raises:\n        ValueError:\n            If a provided parameter name does not exist in the model's parameters.\n\n    Examples:\n        Update parameter value using `set`:\n\n        ```\n        from geostat import GP, Model, Parameters\n        from geostat.kernel import Noise\n\n        # Create parameters.\n        p = Parameters(nugget=1.)\n\n        # Create model\n        kernel = Noise(nugget=p.nugget)\n        model = Model(GP(0, kernel))\n\n        # Update parameters\n        model.set(nugget=0.5)\n        ```\n\n    Notes:\n        - The `set` method retrieves the current parameters using `gather_vars` and updates \n        their values. The associated TensorFlow variables are also recreated.\n        - This method is useful for dynamically updating the model's parameters after initialization.\n    \"\"\"\n\n    parameters = self.gather_vars()\n    for name, v in values.items():\n        if name in parameters:\n            parameters[name].value = v\n            parameters[name].create_tf_variable()\n        else:\n            raise ValueError(f\"{name} is not a parameter\")\n    return self\n</code></pre>"},{"location":"api/#src.geostat.model.NormalizingFeaturizer","title":"<code>NormalizingFeaturizer</code>","text":"<p>NormalizingFeaturizer class for producing normalized feature matrices (F matrix) with an intercept.</p> <p>The <code>NormalizingFeaturizer</code> takes raw location data and applies a specified featurization function. It normalizes the resulting features and remembers normalization parameters using the mean and standard deviation calculated from the  original data and adds an intercept feature (a column of ones) to the matrix.</p> <p>Parameters:</p> Name Type Description Default <code>featurization</code> <code>Callable</code> <p>A function or callable that defines how the input location data should be featurized.</p> required <code>locs</code> <code>array - like or Tensor</code> <p>The input location data used for calculating normalization parameters (mean and standard  deviation) and featurizing new data.</p> required <p>Examples:</p> <p>Creating a <code>NormalizingFeaturizer</code> using a custom featurization function and location data:</p> <pre><code>import tensorflow as tf\nfrom geostat.model import NormalizingFeaturizer\n\n# Define a simple featurization function\ndef custom_featurizer(x, y):\n    return x, y, x * y\n\n# Sample location data\nlocs = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n\n# Create the NormalizingFeaturizer\nnorm_featurizer = NormalizingFeaturizer(custom_featurizer, locs)\n</code></pre> <p>Using the <code>NormalizingFeaturizer</code> to featurize new location data:</p> <pre><code>new_locs = tf.constant([[7.0, 8.0], [9.0, 10.0]])\nF_matrix = norm_featurizer(new_locs)\nprint(F_matrix) # F_matrix will contain normalized features with an additional intercept column\n# tf.Tensor(\n# [[1.        2.4494898 2.4494898 3.5676992]\n#  [1.        3.6742349 3.6742349 6.50242  ]], shape=(2, 4), dtype=float32)\n</code></pre> Notes <ul> <li>The normalization parameters (<code>unnorm_mean</code> and <code>unnorm_std</code>) are calculated based on the  initial <code>locs</code> data provided during initialization.</li> <li>The <code>__call__</code> method applies the normalization and adds an intercept feature when used  to featurize new location data.</li> </ul> Source code in <code>src/geostat/model.py</code> <pre><code>class NormalizingFeaturizer:\n    \"\"\"\n    NormalizingFeaturizer class for producing normalized feature matrices (F matrix) with an intercept.\n\n    The `NormalizingFeaturizer` takes raw location data and applies a specified featurization function.\n    It normalizes the resulting features and remembers normalization parameters using the mean and standard deviation calculated from the \n    original data and adds an intercept feature (a column of ones) to the matrix.\n\n    Parameters:\n        featurization (Callable):\n            A function or callable that defines how the input location data should be featurized.\n        locs (array-like or Tensor):\n            The input location data used for calculating normalization parameters (mean and standard \n            deviation) and featurizing new data.\n\n    Examples:\n        Creating a `NormalizingFeaturizer` using a custom featurization function and location data:\n\n        ```\n        import tensorflow as tf\n        from geostat.model import NormalizingFeaturizer\n\n        # Define a simple featurization function\n        def custom_featurizer(x, y):\n            return x, y, x * y\n\n        # Sample location data\n        locs = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n\n        # Create the NormalizingFeaturizer\n        norm_featurizer = NormalizingFeaturizer(custom_featurizer, locs)\n        ```\n\n        Using the `NormalizingFeaturizer` to featurize new location data:\n\n        ```\n        new_locs = tf.constant([[7.0, 8.0], [9.0, 10.0]])\n        F_matrix = norm_featurizer(new_locs)\n        print(F_matrix) # F_matrix will contain normalized features with an additional intercept column\n        # tf.Tensor(\n        # [[1.        2.4494898 2.4494898 3.5676992]\n        #  [1.        3.6742349 3.6742349 6.50242  ]], shape=(2, 4), dtype=float32)\n        ```\n\n    Notes:\n        - The normalization parameters (`unnorm_mean` and `unnorm_std`) are calculated based on the \n        initial `locs` data provided during initialization.\n        - The `__call__` method applies the normalization and adds an intercept feature when used \n        to featurize new location data.\n    \"\"\"\n\n    def __init__(self, featurization, locs):\n        self.unnorm_featurizer = Featurizer(featurization)\n        F_unnorm = self.unnorm_featurizer(locs)\n        self.unnorm_mean = tf.reduce_mean(F_unnorm, axis=0)\n        self.unnorm_std = tf.math.reduce_std(F_unnorm, axis=0)\n\n    def __call__(self, locs):\n        ones = tf.ones([tf.shape(locs)[0], 1], dtype=tf.float32)\n        F_unnorm = self.unnorm_featurizer(locs)\n        F_norm = (F_unnorm - self.unnorm_mean) / self.unnorm_std\n        return tf.concat([ones, F_norm], axis=1)\n</code></pre>"},{"location":"api/#src.geostat.model.Mix","title":"<code>Mix(inputs, weights=None)</code>","text":"<p>Linearly combines multiple Gaussian Processes (GPs) into a single GP using specified weights.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>list of GPs</code> <p>A list of GP objects to be combined.</p> required <code>weights</code> <code>matrix</code> <p>A matrix specifying how the inputs are to be combined. If not provided,  an identity matrix is assumed, meaning the GPs are combined without weighting.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GP</code> <code>GP</code> <p>A new GP object representing the linear combination of the input GPs with the specified weights.</p> <p>Examples:</p> <p>Combining two GPs into a new multi-output GP:</p> <p>Suppose you have two GPs:  \\(f_1(x) \\sim \\mathrm{GP}(\\mu_1, K_1)\\) and \\(f_2(x) \\sim \\mathrm{GP}(\\mu_2, K_2)\\),  and you want to create a new multi-output GP \\(\\mathbf{g}(x)\\) defined as:</p> \\[ \\mathbf{g}(x) = \\begin{pmatrix} g_1(x) \\\\ g_2(x) \\\\ g_3(x) \\end{pmatrix} = A \\begin{pmatrix} f_1(x) \\\\ f_2(x) \\end{pmatrix}, \\] <p>where \\(A\\) is the weights matrix. This can be implemented as:</p> <pre><code>g = Mix([f1, f2], [[a11, a12], [a21, a22], [a31, a32]])\n</code></pre> <p>The resulting GP \\(\\mathbf{g}(x)\\) can then be used for fitting, generating, or predicting  with methods such as <code>g.fit()</code>, <code>g.generate()</code>, or <code>g.predict()</code> and its components are specified using the <code>cats</code> parameter.</p> Notes <ul> <li>The <code>weights</code> parameter defines how the input GPs are linearly combined. If omitted,  each GP is assumed to be independent, and the identity matrix is used.</li> <li>The resulting GP supports all standard operations (e.g., <code>fit</code>, <code>generate</code>, <code>predict</code>).</li> </ul> Source code in <code>src/geostat/model.py</code> <pre><code>def Mix(inputs, weights=None):\n    \"\"\"\n    Linearly combines multiple Gaussian Processes (GPs) into a single GP using specified weights.\n\n    Parameters:\n        inputs (list of GPs):  \n            A list of GP objects to be combined.\n        weights (matrix, optional):  \n            A matrix specifying how the inputs are to be combined. If not provided, \n            an identity matrix is assumed, meaning the GPs are combined without weighting.\n\n    Returns:\n        GP (GP):\n            A new GP object representing the linear combination of the input GPs with the specified weights.\n\n\n\n    Examples:\n        Combining two GPs into a new multi-output GP:\n\n        Suppose you have two GPs: \n        \\\\(f_1(x) \\sim \\mathrm{GP}(\\mu_1, K_1)\\\\) and \\\\(f_2(x) \\sim \\mathrm{GP}(\\mu_2, K_2)\\\\), \n        and you want to create a new multi-output GP \\\\(\\mathbf{g}(x)\\\\) defined as:\n\n        $$\n        \\mathbf{g}(x) = \\\\begin{pmatrix}\n        g_1(x) \\\\\\\\\n        g_2(x) \\\\\\\\\n        g_3(x)\n        \\end{pmatrix} = A \\\\begin{pmatrix}\n        f_1(x) \\\\\\\\\n        f_2(x)\n        \\end{pmatrix},\n        $$\n\n        where \\\\(A\\\\) is the weights matrix. This can be implemented as:\n\n        ```\n        g = Mix([f1, f2], [[a11, a12], [a21, a22], [a31, a32]])\n        ```\n\n        The resulting GP \\\\(\\mathbf{g}(x)\\\\) can then be used for fitting, generating, or predicting \n        with methods such as `g.fit()`, `g.generate()`, or `g.predict()` and its components are specified using the `cats` parameter.\n\n    Notes:\n        - The `weights` parameter defines how the input GPs are linearly combined. If omitted, \n        each GP is assumed to be independent, and the identity matrix is used.\n        - The resulting GP supports all standard operations (e.g., `fit`, `generate`, `predict`).\n    \"\"\"\n\n    return GP(\n        mn.Mix([i.mean for i in inputs], weights), \n        krn.Mix([i.kernel for i in inputs], weights))\n</code></pre>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#an-introduction-to-geostat","title":"An introduction to Geostat","text":"<p>In Geostat, we create one model that is used to create synthetic data according to provided parameters, and we create a second model that does the inverse: it takes the data and infers the parameters.</p>"},{"location":"examples/#structured-covariance-functions","title":"Structured covariance functions","text":"<p>Here we show how a progressively more complex covariance function fits data better than simpler ones.</p>"},{"location":"examples/#making-predictions-in-a-shape","title":"Making predictions in a shape","text":"<p>Geostat has utility functions to make it easier to work with shapes.</p>"},{"location":"examples/#gaussian-processes-in-tensorflow","title":"Gaussian processes in Tensorflow","text":"<p>A tutorial on how to implement Gaussian processes in Tensorflow.</p>"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/","title":"3d gaussian processes","text":"<pre><code>from geostat import GP, Model, Mesh, Parameters\nimport geostat\nimport geostat.kernel as krn\nimport matplotlib.pyplot as pp\nimport numpy as np\nimport tensorflow as tf\n</code></pre>"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/#overview","title":"Overview","text":"<p>In this notebook we will:   * Use a Gaussian process with a complex stacked covariance function to generate synthetic data.   * Use Gaussian processes with covariance functions of increasing complexity to infer the geospatial parameters from the synthetic data. Each time, log likelihood improves and the nugget decreases. A smaller nugget means that predictions are more confident.</p>"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/#synthesizing-data","title":"Synthesizing data","text":"<p>We will synthesize data at random locations in 3D space near the origin.</p> <pre><code>np.random.seed(111)\n\nlocs = np.random.normal(size=[500, 3]) * [1., 1., 0.333]\n</code></pre> <p>There will be a cubic depth trend, but no horizontal trends. The decorator converts the function <code>trend_featurizer</code> into a featurizer that Geostat can use. The <code>normalize</code> argument causes each feature to have zero mean and unit variance after being applied to the given locations. It also adds a constant one feature.</p> <pre><code>@geostat.featurizer(normalize=locs.reshape([-1, 3]))\ndef trend_featurizer(x, y, z): return z, z*z, z*z*z\n</code></pre> <p>Model parameters are specified here, along with their values. The return value <code>p</code> is a namespace.</p> <pre><code>p = Parameters(alpha=0.1, zs=10., r=0.33, s1=1., s2=0.5, g1=1., g2=0.5, nugget=0.25)\n</code></pre> <p>The covariance function will include a trend based on the featurizer, and will combine two gamma-exponentials: one that respects depth with z-anisotropy, and one that ignores depth altogether. We will set the <code>range</code> for both to be the same parameter to show that it is possible to tie parameters together. In <code>TrendPrior</code>, <code>alpha</code> parameterizes the normal distribution prior for trend coefficients. </p> <pre><code>kernel = \\\n    krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\\n    krn.GammaExponential(range=p.r, sill=p.s1, gamma=p.g1, scale=[1., 1., p.zs]) + \\\n    krn.GammaExponential(range=p.r, sill=p.s2, gamma=p.g2, scale=[1., 1., 0.]) + \\\n    krn.Noise(p.nugget)\n</code></pre> <p>Define a Gaussian process with zero mean and a covariance function given by <code>kernel</code>.</p> <pre><code>gp = GP(0, kernel)\n</code></pre> <p>Instantiate a <code>Model</code> and immediately call <code>generate</code> to generate synthetic observations.</p> <pre><code>tf.random.set_seed(113)\n\nobs = Model(gp).generate(locs).vals\n</code></pre> <p>When the data is plotted, you can see an overall trend with some localized variations.</p> <pre><code>fig, axs = pp.subplots(3, figsize=(7, 6), dpi=120, sharex=True, sharey=True)\nvmin, vmax = obs.min(), obs.max()\npane = np.round((locs[:, 1] + 2) / 2).astype(int)\nfor i, ymid in enumerate(np.linspace(-2, 2, 3)):\n    ymin, ymax = ymid - 1, ymid + 1\n    c = axs[i].scatter(locs[pane == i, 0], locs[pane == i, 2], c=obs[pane == i], vmin=vmin, vmax=vmax)\n    axs[i].set_title('y = %0.1f' % ymid)\n    axs[i].set_aspect(0.9)\naxs[2].set_xlabel('x-axis')\naxs[1].set_ylabel('z-axis')\n\nfig.subplots_adjust(right=0.9)\ncbar_ax = fig.add_axes([0.88, 0.1, 0.02, 0.8])\nfig.colorbar(c, cax=cbar_ax)\n\nfig.suptitle('Synthetic data, projected to nearest cross section')\npp.show()\n</code></pre> <p></p> <p>Before we continue, let's define a function that takes a model and plots predictions for the three slices shown above.</p> <pre><code>def plot(model):\n    fig, axs = pp.subplots(3, figsize=(7, 6), dpi=120, sharex=True, sharey=True)\n    for i, ymid in enumerate(np.linspace(-2, 2, 3)):\n\n        mesh = Mesh.from_bounds([-3, -1, 3, 1], nx=200)\n        mesh_locs = mesh.locations(proj=[[1, 0, 0], [0, 0, 1], [0, ymid, 0]]) # [x, z, 1] -&gt; [x, y, z].\n        mean, var = model.predict(mesh_locs)\n        meshx, meshy, out = mesh.slice(mean)\n        c = axs[i].pcolormesh(meshx, meshy, out, vmin=vmin, vmax=vmax)\n\n        axs[i].set_title('y = %0.1f' % ymid)\n        axs[i].set_aspect(0.9)\n    axs[2].set_xlabel('x-axis')\n    axs[1].set_ylabel('z-axis')\n\n    fig.subplots_adjust(right=0.9)\n    cbar_ax = fig.add_axes([0.88, 0.1, 0.02, 0.8])\n    fig.colorbar(c, cax=cbar_ax)\n\n    fig.suptitle('Predictions for 3 cross sections')\n    pp.show()\n</code></pre>"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/#model-1-bayesian-regression","title":"Model 1: Bayesian regression","text":"<p>First let's try modeling the data with a Bayesian regression, which is a model with just trends and uncorrelated noise.</p> <pre><code>p = Parameters(alpha=1.0, nugget=0.5)\n\nkernel = krn.TrendPrior(trend_featurizer, alpha=p.alpha) + krn.Noise(nugget=p.nugget)\n\nmodel1 = Model(GP(0, kernel)).fit(locs, obs, iters=500)\n</code></pre> <pre><code>[iter    50 ll -910.60 time  1.73 reg  0.00 alpha  0.61 nugget  0.79]\n[iter   100 ll -853.59 time  0.69 reg  0.00 alpha  0.37 nugget  1.06]\n[iter   150 ll -836.20 time  0.69 reg  0.00 alpha  0.23 nugget  1.28]\n[iter   200 ll -830.86 time  0.68 reg  0.00 alpha  0.15 nugget  1.42]\n[iter   250 ll -829.16 time  0.69 reg  0.00 alpha  0.10 nugget  1.50]\n[iter   300 ll -828.57 time  0.68 reg  0.00 alpha  0.07 nugget  1.54]\n[iter   350 ll -828.35 time  0.69 reg  0.00 alpha  0.06 nugget  1.56]\n[iter   400 ll -828.27 time  0.68 reg  0.00 alpha  0.05 nugget  1.57]\n[iter   450 ll -828.25 time  0.69 reg  0.00 alpha  0.04 nugget  1.58]\n[iter   500 ll -828.24 time  0.68 reg  0.00 alpha  0.04 nugget  1.58]\n</code></pre> <p>And here are predictions for the three slices shown above.</p> <pre><code>plot(model1)\n</code></pre> <p></p>"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/#model-2-gp-with-isotropic-sq-exp-covariance-function","title":"Model 2: GP with isotropic sq-exp covariance function","text":"<p>Now let's layer on an isotropic squared exponential covariance function to the above model.</p> <pre><code>p = Parameters(alpha=1.0, range=1.0, sill=0.5, nugget=0.5)\n\nkernel = \\\n    krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\\n    krn.SquaredExponential(range=p.range, sill=p.sill) + \\\n    krn.Noise(nugget=p.nugget)\n\nmodel2 = Model(GP(0, kernel)).fit(locs, obs, iters=500)\n</code></pre> <pre><code>[iter    50 ll -864.77 time  1.41 reg  0.00 alpha  0.61 sill  0.75 range  0.62 nugget  0.77]\n[iter   100 ll -829.58 time  0.80 reg  0.00 alpha  0.37 sill  0.53 range  0.46 nugget  0.99]\n[iter   150 ll -820.58 time  0.80 reg  0.00 alpha  0.23 sill  0.38 range  0.40 nugget  1.13]\n[iter   200 ll -817.97 time  0.80 reg  0.00 alpha  0.15 sill  0.33 range  0.38 nugget  1.22]\n[iter   250 ll -816.98 time  0.80 reg  0.00 alpha  0.10 sill  0.31 range  0.38 nugget  1.27]\n[iter   300 ll -816.48 time  0.80 reg  0.00 alpha  0.07 sill  0.30 range  0.38 nugget  1.29]\n[iter   350 ll -816.18 time  0.80 reg  0.00 alpha  0.05 sill  0.29 range  0.39 nugget  1.31]\n[iter   400 ll -816.00 time  0.80 reg  0.00 alpha  0.04 sill  0.28 range  0.40 nugget  1.32]\n[iter   450 ll -815.90 time  0.80 reg  0.00 alpha  0.04 sill  0.27 range  0.42 nugget  1.33]\n[iter   500 ll -815.84 time  0.80 reg  0.00 alpha  0.03 sill  0.27 range  0.43 nugget  1.33]\n</code></pre> <p>The log-likelihood is improved as a result of a more complex model. Nugget is much lower. Predictions:</p> <pre><code>plot(model2)\n</code></pre> <p></p>"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/#model-3-gp-with-anisotropic-sq-exp-covariance-function","title":"Model 3: GP with anisotropic sq-exp covariance function","text":"<p>Now we switch from isotropic to anisotropic for the covariance function.</p> <pre><code>p = Parameters(alpha=1.0, range=1.0, sill=0.5, nugget=0.5, zs=5.0)\n\nkernel = \\\n    krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\\n    krn.SquaredExponential(range=p.range, sill=p.sill, scale=[1., 1., p.zs]) + \\\n    krn.Noise(nugget=p.nugget)\n\nmodel3 = Model(GP(0, kernel)).fit(locs, obs, iters=500)\n</code></pre> <pre><code>[iter    50 ll -841.04 time  1.50 reg  0.00 alpha  0.61 zs  7.43 sill  0.75 range  0.65 nugget  0.76]\n[iter   100 ll -821.03 time  0.86 reg  0.00 alpha  0.37 zs  8.75 sill  0.74 range  0.50 nugget  0.90]\n[iter   150 ll -817.17 time  0.86 reg  0.00 alpha  0.23 zs  8.73 sill  0.66 range  0.43 nugget  0.95]\n[iter   200 ll -815.63 time  0.86 reg  0.00 alpha  0.15 zs  8.13 sill  0.63 range  0.39 nugget  0.97]\n[iter   250 ll -814.62 time  0.86 reg  0.00 alpha  0.10 zs  7.37 sill  0.63 range  0.37 nugget  0.98]\n[iter   300 ll -813.89 time  0.87 reg  0.00 alpha  0.07 zs  6.66 sill  0.63 range  0.36 nugget  0.97]\n[iter   350 ll -813.41 time  0.87 reg  0.00 alpha  0.05 zs  6.08 sill  0.64 range  0.35 nugget  0.97]\n[iter   400 ll -813.12 time  0.87 reg  0.00 alpha  0.04 zs  5.64 sill  0.65 range  0.34 nugget  0.96]\n[iter   450 ll -812.97 time  0.87 reg  0.00 alpha  0.04 zs  5.32 sill  0.66 range  0.33 nugget  0.95]\n[iter   500 ll -812.89 time  0.87 reg  0.00 alpha  0.04 zs  5.11 sill  0.67 range  0.32 nugget  0.94]\n</code></pre> <p>Log-likelihood and nugget both improve further. Predictions:</p> <pre><code>plot(model3)\n</code></pre> <p></p>"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/#model-4-gp-with-anisotropic-gamma-exp-covariance-function","title":"Model 4: GP with anisotropic gamma-exp covariance function","text":"<p>Now we switch from a squared-exponential to a gamma-exponential covariance function, which has an extra shape parameter.</p> <pre><code>p = Parameters(alpha=1.0, range=1.0, sill=0.5, nugget=0.5, zs=5.0, gamma=1.0)\n\nkernel = \\\n    krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\\n    krn.GammaExponential(range=p.range, sill=p.sill, gamma=p.gamma, scale=[1., 1., p.zs]) + \\\n    krn.Noise(nugget=p.nugget)\n\nmodel4 = Model(GP(0, kernel)).fit(locs, obs, iters=500)\n</code></pre> <pre><code>[iter    50 ll -819.50 time  1.83 reg  0.00 alpha  0.61 zs  7.16 sill  0.76 range  0.67 gamma  0.84 nugget  0.74]\n[iter   100 ll -815.43 time  0.94 reg  0.00 alpha  0.37 zs  6.94 sill  0.86 range  0.58 gamma  0.81 nugget  0.82]\n[iter   150 ll -814.43 time  0.94 reg  0.00 alpha  0.23 zs  6.19 sill  0.87 range  0.56 gamma  0.81 nugget  0.82]\n[iter   200 ll -813.66 time  0.94 reg  0.00 alpha  0.15 zs  5.77 sill  0.87 range  0.53 gamma  0.80 nugget  0.82]\n[iter   250 ll -813.07 time  0.94 reg  0.00 alpha  0.10 zs  5.56 sill  0.86 range  0.50 gamma  0.80 nugget  0.82]\n[iter   300 ll -812.64 time  0.94 reg  0.00 alpha  0.07 zs  5.45 sill  0.86 range  0.48 gamma  0.81 nugget  0.81]\n[iter   350 ll -812.36 time  0.95 reg  0.00 alpha  0.05 zs  5.39 sill  0.86 range  0.46 gamma  0.82 nugget  0.80]\n[iter   400 ll -812.19 time  0.95 reg  0.00 alpha  0.04 zs  5.36 sill  0.86 range  0.45 gamma  0.83 nugget  0.79]\n[iter   450 ll -812.10 time  0.95 reg  0.00 alpha  0.04 zs  5.33 sill  0.86 range  0.43 gamma  0.84 nugget  0.78]\n[iter   500 ll -812.06 time  0.95 reg  0.00 alpha  0.03 zs  5.31 sill  0.87 range  0.43 gamma  0.85 nugget  0.77]\n</code></pre> <p>The log-likelihood has improved very slightly but nugget improves more significantly, since the pointy peak in the gamma-exponential does some of the work of a nugget. Predictions:</p> <pre><code>plot(model4)\n</code></pre> <p></p>"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/#model-5-gp-with-stacked-covariance-functions","title":"Model 5: GP with stacked covariance functions","text":"<p>Finally we switch to using the same covariance function used to generate the synthetic data.</p> <pre><code>p = Parameters(alpha=1., zs=5., r=0.5, s1=2., s2=1., g1=1., g2=1., nugget=1.)\n\nkernel = \\\n    krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\\n    krn.GammaExponential(range=p.r, sill=p.s1, gamma=p.g1, scale=[1., 1., p.zs]) + \\\n    krn.GammaExponential(range=p.r, sill=p.s2, gamma=p.g2, scale=[1., 1., 0.]) + \\\n    krn.Noise(nugget=p.nugget)\n\nmodel5 = Model(GP(0, kernel)).fit(locs, obs, iters=500)\n</code></pre> <pre><code>[iter    50 ll -824.02 time  2.05 reg  0.00 alpha  0.61 zs  4.04 s1  1.32 r  0.75 g1  0.85 s2  0.65 g2  1.16 nugget  0.67]\n[iter   100 ll -816.07 time  1.10 reg  0.00 alpha  0.38 zs  6.56 s1  1.11 r  0.81 g1  0.57 s2  0.50 g2  1.00 nugget  0.62]\n[iter   150 ll -813.38 time  1.10 reg  0.00 alpha  0.24 zs  8.73 s1  1.02 r  0.79 g1  0.45 s2  0.43 g2  0.79 nugget  0.60]\n[iter   200 ll -812.33 time  1.10 reg  0.00 alpha  0.16 zs  9.27 s1  0.97 r  0.74 g1  0.41 s2  0.39 g2  0.63 nugget  0.59]\n[iter   250 ll -811.74 time  1.11 reg  0.00 alpha  0.11 zs  9.15 s1  0.94 r  0.67 g1  0.42 s2  0.38 g2  0.57 nugget  0.59]\n[iter   300 ll -811.27 time  1.11 reg  0.00 alpha  0.08 zs  9.08 s1  0.91 r  0.60 g1  0.45 s2  0.38 g2  0.54 nugget  0.58]\n[iter   350 ll -810.74 time  1.11 reg  0.00 alpha  0.06 zs  9.15 s1  0.88 r  0.52 g1  0.52 s2  0.37 g2  0.54 nugget  0.57]\n[iter   400 ll -809.82 time  1.10 reg  0.00 alpha  0.05 zs  9.39 s1  0.82 r  0.43 g1  0.66 s2  0.37 g2  0.54 nugget  0.56]\n[iter   450 ll -808.27 time  1.11 reg  0.00 alpha  0.04 zs  9.57 s1  0.76 r  0.36 g1  0.93 s2  0.36 g2  0.52 nugget  0.54]\n[iter   500 ll -807.77 time  1.11 reg  0.00 alpha  0.04 zs  8.73 s1  0.73 r  0.35 g1  1.11 s2  0.37 g2  0.49 nugget  0.53]\n</code></pre> <p>Not surprisingly, log likelihood and nugget both improve further. You can see faint vertical stripes that correspond the the \"depth-invariant\" component of the covariance function.</p> <pre><code>plot(model5)\n</code></pre> <p></p>"},{"location":"notebooks/gaussian-processes-in-geostat/gaussian-processes-in-geostat/","title":"Gaussian processes in geostat","text":"<pre><code>from geostat import GP, Model, Mesh, Parameters\nimport geostat\nimport geostat.kernel as krn\nimport matplotlib.pyplot as pp\nimport numpy as np\n</code></pre>"},{"location":"notebooks/gaussian-processes-in-geostat/gaussian-processes-in-geostat/#overview","title":"Overview","text":"<p>In this notebook we will:   * Use a Gaussian process to generate synthetic data with known geospatial parameters.   * Use a second Gaussian process to infer the geospatial parameters from the synthetic data.   * Use the fitted Gaussian process to interpolate locations on a mesh.</p>"},{"location":"notebooks/gaussian-processes-in-geostat/gaussian-processes-in-geostat/#synthesizing-data","title":"Synthesizing data","text":"<p>We will synthesize data at mesh locations in a square centered on the origin.</p> <p>First define mesh locations using a <code>Mesh</code> object. The <code>nx</code> argument specifies 80 mesh coordinates in the x direction, and keeps the pitch the same in the y direction (which results in 80 mesh coordinates in that direction as well).</p> <pre><code>mesh = Mesh.from_bounds([-1, -1, 1, 1], nx=80)\n</code></pre> <p>Declare the terms of the spatial trend. The decorator converts the function <code>trend_featurizer</code> into a featurizer that Geostat can use. The <code>normalize</code> argument causes each feature to have zero mean and unit variance after being applied to mesh locations. It also adds a constant one feature. The method <code>mesh.locations()</code> returns an array of shape <code>[N, 2]</code>, where <code>N</code> is the number of locations. </p> <pre><code>@geostat.featurizer(normalize=mesh.locations())\ndef trend_featurizer(x, y): return x, y, x*x, x*y, y*y\n</code></pre> <p>Model parameters are specified here, along with their values. The return value <code>p</code> is a namespace.</p> <pre><code>p = Parameters(alpha=0.25, range=0.33, sill=1., nugget=0.25)\n</code></pre> <p>The covariance function has three terms:</p> <ul> <li><code>TrendPrior</code> specifies a trend based on <code>trend_featurizer</code>. In <code>TrendPrior</code>, <code>alpha</code> parameterizes the normal distribution prior for trend coefficients. </li> <li><code>SquaredExponential</code>, a stationary covariance function.</li> <li><code>Noise</code>, uncorrelated noise.</li> </ul> <pre><code>kernel = krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\\n         krn.SquaredExponential(range=p.range, sill=p.sill) + \\\n         krn.Noise(nugget=p.nugget)\n</code></pre> <p>Define a Gaussian process with zero mean and a covariance function given by <code>kernel</code>.</p> <pre><code>gp = GP(0, kernel)\n</code></pre> <p>Instantiate a <code>Model</code> and then call <code>generate</code> to generate synthetic observations. The result <code>mesh_obs</code> has shape <code>[N]</code>.</p> <pre><code>model = Model(gp)\nmesh_obs = model.generate(mesh.locations()).vals\n</code></pre> <p>When the data is plotted, you can see an overall trend with some localized variations. The method <code>mesh.slice()</code> forms the observations into a 2d array suitable for use with <code>pcolormesh</code>.</p> <pre><code>vmin, vmax = mesh_obs.min(), mesh_obs.max()\nmeshx, meshy, mesh_obs_2d = mesh.slice(mesh_obs) # Each return value is a 2d array.\nc = pp.pcolormesh(meshx, meshy, mesh_obs_2d, vmin=vmin, vmax=vmax)\npp.colorbar(c)\npp.title('Synthetic data')\npp.show()\n</code></pre> <p></p> <p>Of these synthetic datapoints we'll sample just 200, with which we'll try to reconstruct the rest of the data.</p> <pre><code>sample_indices = np.random.choice(len(mesh_obs), [200], replace=False)\nlocs = mesh.locations()[sample_indices, :]\nobs = mesh_obs[sample_indices]\n\nc = pp.scatter(locs[:, 0], locs[:, 1], c=obs, vmin=vmin, vmax=vmax)\npp.colorbar(c)\npp.title('Synthetic observations')\npp.show()\n</code></pre> <p></p>"},{"location":"notebooks/gaussian-processes-in-geostat/gaussian-processes-in-geostat/#inferring-parameters","title":"Inferring parameters","text":"<p>Now we <code>set</code> the parameters in model to something arbitrary and see if the model can infer the correct parameters from the data, which consists of <code>locs</code> and <code>obs</code>. We don't expect <code>alpha</code> to converge to what it used to be, since <code>TrendPrior</code> generates only a small number of trend coefficients using <code>alpha</code>. However, <code>sill</code>, <code>range</code>, and <code>nugget</code> should all converge to something close.</p> <p>(The <code>None</code> at the end suppresses extraneous output.)</p> <pre><code>model.set(alpha=1.0, range=1.0, sill=0.5, nugget=0.5)\n\nmodel.fit(locs, obs, iters=500)\n\nNone\n</code></pre> <pre><code>[iter    50 ll -237.51 time  1.86 reg  0.00 alpha  0.61 sill  0.84 range  0.60 nugget  0.56]\n[iter   100 ll -200.69 time  0.59 reg  0.00 alpha  0.37 sill  1.24 range  0.42 nugget  0.34]\n[iter   150 ll -194.76 time  0.59 reg  0.00 alpha  0.23 sill  1.44 range  0.37 nugget  0.26]\n[iter   200 ll -194.28 time  0.58 reg  0.00 alpha  0.17 sill  1.48 range  0.36 nugget  0.25]\n[iter   250 ll -194.12 time  0.58 reg  0.00 alpha  0.13 sill  1.47 range  0.36 nugget  0.25]\n[iter   300 ll -194.05 time  0.58 reg  0.00 alpha  0.11 sill  1.46 range  0.35 nugget  0.25]\n[iter   350 ll -194.01 time  0.58 reg  0.00 alpha  0.09 sill  1.46 range  0.35 nugget  0.25]\n[iter   400 ll -194.00 time  0.58 reg  0.00 alpha  0.09 sill  1.46 range  0.35 nugget  0.25]\n[iter   450 ll -193.99 time  0.58 reg  0.00 alpha  0.08 sill  1.47 range  0.35 nugget  0.25]\n[iter   500 ll -193.99 time  0.58 reg  0.00 alpha  0.08 sill  1.48 range  0.36 nugget  0.25]\n</code></pre>"},{"location":"notebooks/gaussian-processes-in-geostat/gaussian-processes-in-geostat/#generating-predictions","title":"Generating predictions","text":"<p>Call <code>model</code> to get predictions at the same mesh locations as before:</p> <pre><code>mean, var = model.predict(mesh.locations())\n</code></pre> <pre><code>meshx, meshy, mean2d = mesh.slice(mean) # Each return value is a 2d array.\nc = pp.pcolormesh(meshx, meshy, mean2d, vmin=vmin, vmax=vmax)\npp.colorbar(c)\npp.title('Prediction mean')\npp.show()\n</code></pre> <p></p> <p>For comparison, here's the original synthetic data:</p> <pre><code>c = pp.pcolormesh(meshx, meshy, mesh_obs_2d, vmin=vmin, vmax=vmax)\npp.colorbar(c)\npp.title('Synthetic data')\npp.show()\n</code></pre> <p></p> <p>And here's a plot of prediction variance, which accounts for, among other things, the noise that the model is unable to reconstruct.</p> <pre><code>meshx, meshy, var2d = mesh.slice(var) # Each return value is a 2d array.\nc = pp.pcolormesh(meshx, meshy, var2d, cmap='gist_heat_r')\npp.colorbar(c)\npp.title('Prediction variance')\npp.show()\n</code></pre> <p></p>"},{"location":"notebooks/gaussian-processes-in-tensorflow/gaussian-processes-in-tensorflow/","title":"Gaussian processes in Tensorflow","text":"<pre><code>import tensorflow as tf\ntf.__version__\n</code></pre> <pre><code>'1.8.0'\n</code></pre> <p>Geostatistical datasets are often a set of measurements with locations. Nearby measurements covary a lot. Distant measurements are nearly independent.</p> <p>Let's simulate this:</p> <ul> <li>Define \\(N\\) locations \\(x\\) by drawing uniformly at random from a square area.</li> <li>Create \\(N\\times N\\) distance matrix \\(D\\) (euclidean distance between locations).</li> <li>Define a covariance function: \\(c(d; r, s, n) = s \\cdot \\exp(-(d/r)^2) + n \\cdot \\delta_d\\), where \\(d\\) is distance, and \\((r, s, n)\\) correspond to range/sill/nugget on a variogram. (I think the convention varies between  calling either \\(s+n\\) or \\(s\\) the sill.) Also, \\(\\delta_d\\) is 1 when \\(d\\) is 0, and 0 otherwise.</li> <li>Use the covariance function to map \\(D\\) elementwise to a covariance matrix \\(C\\).</li> <li>Draw \\(u \\sim \\textrm{Normal}(\\beta_1 \\cdot \\mathbb{1}, C)\\) to obtain values, where \\(\\beta_1\\) is the (scalar) mean value of a single draw, and \\(\\mathbb{1}\\) is a vector of \\(N\\) ones.</li> </ul> <p>This is implemented as <code>simulate_gp</code> below. The range is called <code>vrange</code> to avoid conflicting with Python's <code>range</code>.</p> <p>The data consists of locations <code>x</code> (or equivalently, the distance matrix <code>D</code>) and measurements <code>u</code>.</p> <pre><code>import numpy as np\nfrom scipy.spatial.distance import cdist\nnp.set_printoptions(precision=2, threshold=50)\n\ndef simulate_gp(N, vrange, sill, nugget, offset):\n\n    # Sample N locations from square with corners at [\u00b110, \u00b110].\n    x = np.random.uniform(-10.0, 10.0, [N, 2]) \n\n    # Compute distance matrix for sampled locations.\n    D = cdist(x, x)\n\n    # Compute corresponding covariance matrix.\n    C = sill * np.exp(-np.square(D/vrange)) + nugget * np.eye(N)\n\n    # The mean is just a vector where every entry is the offset.\n    m = np.zeros([N]) + offset\n\n    # Simulate geospatial measurements by sampling using covariance matrix\n    u = np.random.multivariate_normal(m, C)\n\n    return x, D, C, m, u\n</code></pre> <p>Now we call <code>simulate_gp</code> and plot the result. (You may have to run this twice to get the plot to show up.)</p> <pre><code>x, D, C, m, u = simulate_gp(\n    N = 300,\n    vrange = 5.0,\n    sill = 2.0,\n    nugget = 2.0,\n    offset = 1.0)\n\nprint(\"Locations\")\nprint(x)\n\nprint(\"Distance matrix\")\nprint(D)\n\nprint(\"Covariance matrix\")\nprint(C)\n\nprint(\"Simulated measurements\")\nprint(u)\n\nimport matplotlib.pyplot as pp\n\npp.scatter(x[:, 0], x[:, 1], c=u)\npp.show()\n</code></pre> <pre><code>Locations\n[[ 0.83  9.5 ]\n [-6.51  0.99]\n [ 2.84  6.14]\n ...\n [-4.99  0.45]\n [-3.49  2.24]\n [ 5.49 -0.34]]\nDistance matrix\n[[ 0.   11.24  3.92 ... 10.76  8.45 10.89]\n [11.24  0.   10.68 ...  1.61  3.26 12.07]\n [ 3.92 10.68  0.   ...  9.68  7.44  7.  ]\n ...\n [10.76  1.61  9.68 ...  0.    2.33 10.51]\n [ 8.45  3.26  7.44 ...  2.33  0.    9.34]\n [10.89 12.07  7.   ... 10.51  9.34  0.  ]]\nCovariance matrix\n[[4.   0.01 1.08 ... 0.02 0.11 0.02]\n [0.01 4.   0.02 ... 1.8  1.31 0.01]\n [1.08 0.02 4.   ... 0.05 0.22 0.28]\n ...\n [0.02 1.8  0.05 ... 4.   1.61 0.02]\n [0.11 1.31 0.22 ... 1.61 4.   0.06]\n [0.02 0.01 0.28 ... 0.02 0.06 4.  ]]\nSimulated measurements\n[ 2.78 -0.64 -0.56 ... -0.27  2.02  2.  ]\n</code></pre> <p></p> <p>Now we make a function <code>infer_gp</code> to infer the gaussian process parameters (range, sill, nugget, offset) using maximum likelihood. We implement the same graph in Tensorflow as we did in NumPy, and tack on the negative log PDF of a multivariate normal distribution at the end, which we minimize. That is, we minimize: \\(\\(-\\log p(u \\mid m, C) = \\frac{1}{2}\\bigg[\\log\\,\\big|\\, 2\\pi C\\,\\big| + (u-m)^T C^{-1} (u-m) \\bigg],\\)\\) where \\(m = \\beta_1 \\cdot \\mathbb{1}\\).</p> <p>The function has two arguments:   * <code>inputs</code> is a list of <code>numpy</code> arrays:       * Distance matrix <code>D</code> (shape: [N, N])       * Measurements <code>u</code> (shape: [N])   *  <code>parameters</code> is a list of tensors for range, sill, nugget, and offset. Each tensor can be a <code>tf.Variables</code> (if it is to be inferred) or a constant (if it's a given).</p> <pre><code>def infer_gp(inputs, parameters):\n\n    D, u = inputs\n    vrange, sill, nugget, offset = parameters\n\n    # Construct covariance; boost diagonal by 1e-6 for numerical stability.\n    covariance = sill * tf.exp(-tf.square(tf.constant(D)/vrange)) \\\n               + (nugget + 1e-6) * tf.eye(D.shape[0], dtype=tf.float64)\n\n    # Log likelihood is the PDF of a multivariate gaussian.\n    u_adj = tf.constant(u) - offset\n    logdet = tf.linalg.logdet(2 * np.pi * covariance)\n    quad = tf.matmul(tf.expand_dims(u_adj, 0), tf.matrix_solve(covariance, tf.expand_dims(u_adj, -1)))[0, 0]\n    ll = -0.5 * (logdet + quad)\n\n    # Infer using an adaptive gradient descent optimizer.\n    train = tf.train.AdamOptimizer(1e-2).minimize(-ll)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for i in range(20):\n            for j in range(100):\n                sess.run(train)\n            print('[ll %7.2f] [range %4.2f] [sill %4.2f] [nugget %4.2f] [offset %4.2f]' % \n                  tuple(sess.run([ll, vrange, sill, nugget, offset])))\n\n        return sess.run([vrange, sill, nugget, offset])\n</code></pre> <p>Here we make a <code>tf.Variable</code> for each parameter, using a log as the underlying representation if the parameter is positive-only.</p> <pre><code># Define parameters using tf.Variable.\nlog_vrange = tf.Variable(0.0, dtype=tf.float64)\nlog_sill = tf.Variable(0.0, dtype=tf.float64)\nlog_nugget = tf.Variable(0.0, dtype=tf.float64)\n\nvrange = tf.exp(log_vrange)\nsill = tf.exp(log_sill)\nnugget = tf.exp(log_nugget)\noffset = tf.Variable(0.0, dtype=tf.float64)\n\nvrange_val, sill_val, nugget_val, offset_val = infer_gp([D, u], [vrange, sill, nugget, offset])\n</code></pre> <pre><code>[ll -552.48] [range 2.25] [sill 1.48] [nugget 1.66] [offset 0.63]\n[ll -537.88] [range 3.71] [sill 1.36] [nugget 1.68] [offset 0.84]\n[ll -536.83] [range 4.20] [sill 1.38] [nugget 1.70] [offset 0.96]\n[ll -536.62] [range 4.28] [sill 1.42] [nugget 1.71] [offset 1.05]\n[ll -536.51] [range 4.28] [sill 1.43] [nugget 1.71] [offset 1.12]\n[ll -536.45] [range 4.27] [sill 1.42] [nugget 1.71] [offset 1.18]\n[ll -536.42] [range 4.26] [sill 1.41] [nugget 1.71] [offset 1.23]\n[ll -536.41] [range 4.25] [sill 1.40] [nugget 1.71] [offset 1.26]\n[ll -536.40] [range 4.25] [sill 1.40] [nugget 1.71] [offset 1.27]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.29]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.29]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30]\n</code></pre> <p>We can do better inference by integrating over all possibilities for \\(\\beta_1\\).  (Integrating over range, sill, and nugget, which are parameters in \\(C\\), is hard; but integrating over parameters in \\(m\\) is relatively easy.) This corresponds to the following generative model.</p> <p>First, draw \\(\\beta_1\\) from a normal distribution:</p> \\[\\beta_1 \\sim \\mathcal{N}(0, 100).\\] <p>The variance should be large enough that the distribution assigns reasonably large probabilities to any plausible value for \\(\\beta_1\\).</p> <p>Next, draw \\(u\\) from a multivariate normal distribution, as we've been doing all along:</p> \\[u \\mid \\beta_1 \\sim \\mathcal{N}(\\beta_1 \\cdot \\mathbb{1}, C).\\] <p>From this we can derive a distribution for \\(u\\) by marginalizing (integrating) over \\(\\beta_1\\). That is, we can compute:</p> \\[p(u) = \\int_{-\\infty}^{\\infty} p(u\\mid\\beta_1) \\, p(\\beta_1) \\, d\\beta_1.\\] <p>We rely on the abstract fact that if \\(X_1 \\sim \\mathcal{N}(\\mu_1, \\Sigma_1)\\) and \\(X_2 \\mid X_1 \\sim \\mathcal{N}(\\mu_2, \\Sigma_2)\\), then \\(X_2 \\sim \\mathcal{N}(\\mu_1, \\Sigma_1 + \\Sigma_2)\\).  (\\(X_1\\) and \\(X_2\\) are vectors equal in length.) To apply this, we note that since \\(\\beta_1 \\sim \\mathcal{N}(0, 100)\\), then \\(\\beta_1 \\cdot \\mathbb{1} \\sim \\mathcal{N}(0, 100 \\cdot \\mathbb{1}\\mathbb{1}^T)\\), where \\(\\mathbb{1}\\mathbb{1}^T\\) is a matrix of all ones. From this it follows that</p> \\[u \\sim \\mathcal{N}(0, A)\\] <p>where \\(A = 100 \\cdot \\mathbb{1}\\mathbb{1}^T + C.\\) The function below implements inference with this model, where the offset is marginalized out.</p> <pre><code>def infer_gp_marginalize_over_offset(inputs, parameters, offset_prior):\n\n    D, u = inputs\n    vrange, sill, nugget = parameters\n\n    # Construct covariance; boost diagonal by 1e-6 for numerical stability.\n    covariance = sill * tf.exp(-tf.square(tf.constant(D)/vrange)) \\\n               + (nugget + 1e-6) * tf.eye(D.shape[0], dtype=tf.float64) \\\n               + offset_prior\n\n    # Log likelihood is the PDF of a multivariate gaussian.\n    u_adj = tf.constant(u) - offset\n    logdet = tf.linalg.logdet(2 * np.pi * covariance)\n    quad = tf.matmul(tf.expand_dims(u_adj, 0), tf.matrix_solve(covariance, tf.expand_dims(u_adj, -1)))[0, 0]\n    ll = -0.5 * (logdet + quad)\n\n    # Infer using an adaptive gradient descent optimizer.\n    train = tf.train.AdamOptimizer(1e-2).minimize(-ll)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for i in range(20):\n            for j in range(100):\n                sess.run(train)\n            print('[ll %7.2f] [range %4.2f] [sill %4.2f] [nugget %4.2f]' % tuple(sess.run([ll, vrange, sill, nugget])))\n\n        return sess.run([vrange, sill, nugget])\n</code></pre> <p>When we run this, we don't get an estimate for the offset, but the other estimates are often improved. Bear in mind that we can actually solve for the posterior distribution of the offset if we want to.</p> <pre><code># Define parameters using tf.Variable.\nlog_vrange = tf.Variable(0.0, dtype=tf.float64)\nlog_sill = tf.Variable(0.0, dtype=tf.float64)\nlog_nugget = tf.Variable(0.0, dtype=tf.float64)\n\nvrange = tf.exp(log_vrange)\nsill = tf.exp(log_sill)\nnugget = tf.exp(log_nugget)\n\noffset_prior = 100.0\n\nvrange_val, sill_val, nugget_val = infer_gp_marginalize_over_offset([D, u], [vrange, sill, nugget], offset_prior)\n</code></pre> <pre><code>[ll -541.34] [range 3.02] [sill 0.96] [nugget 1.69]\n[ll -537.97] [range 4.51] [sill 1.13] [nugget 1.73]\n[ll -537.93] [range 4.65] [sill 1.25] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n</code></pre> <p>Interpolation takes values at \\(N_1\\) locations, and gives means and variances for \\(N_2\\) locations. Formally, interpolation takes</p> <ul> <li>input locations \\(x_1\\), an \\(N_1 \\times 2\\) matrix,</li> <li>input values \\(u_1\\), a vector of \\(N_1\\) elements,</li> <li>output locations \\(x_2\\), an \\(N_2 \\times 2\\) matrix.</li> </ul> <p>and gives a distribution for output values \\(u_2\\), a vector of \\(N_2\\) elements. For notational convenience, define</p> \\[x = \\begin{bmatrix}x_1\\\\x_2\\end{bmatrix} \\textrm{ and } u = \\begin{bmatrix}u_1\\\\u_2\\end{bmatrix}.\\] <p>The model remains the same as before, so \\(u \\sim \\mathcal{N}(0, A)\\), where \\(A\\) is constructed from a distance matrix of all locations \\(x\\) as before. For clarity, let's expand \\(u\\) and \\(A\\):</p> \\[\\begin{bmatrix}u_1\\\\u_2\\end{bmatrix} \\sim \\mathcal{N}\\bigg(0, \\begin{bmatrix}A_{11} &amp; A_{12}\\\\A_{21} &amp; A_{22}\\end{bmatrix}\\bigg).\\] <p>Interpolation consists of getting a distribution for \\(u_2\\) given \\(u_1\\). This is a textbook thing to do with a multivariate normal distribution, and the solution is:</p> \\[u_2 \\mid u_1 \\sim \\mathcal{N}(A_{21}A_{11}^{-1}u_1,\\ A_{22} - A_{21}A_{11}^{-1}A_{12}).\\] <p>Bear in mind, if we just want  the marginal variance of each element in \\(u_2\\), we only need to compute the diagonal entries of \\(A_{22} - A_{21}A_{11}^{-1}A_{12}\\).</p> <p>Refer to Wikipedia for more details.</p> <pre><code>def interpolate_gp(x1, u1, x2, parameter_vals, offset_prior):\n\n    vrange, sill, nugget = parameter_vals\n\n    # Compute distance matrices for sampled locations.\n    D11 = cdist(x1, x1)\n    D12 = cdist(x1, x2)\n    D21 = cdist(x2, x1)\n    D22 = cdist(x2, x2)\n\n    # Compute covariance matrices.\n    C11 = sill * np.exp(-np.square(D11/vrange)) + nugget * np.eye(len(x1)) + offset_prior\n    C12 = sill * np.exp(-np.square(D12/vrange)) + offset_prior # No nugget for off-diagonal entries\n    C21 = sill * np.exp(-np.square(D21/vrange)) + offset_prior # No nugget for off-diagonal entries\n    C22 = sill * np.exp(-np.square(D22/vrange)) + nugget * np.eye(len(x2)) + offset_prior\n\n    u2_mean = np.matmul(C21, np.linalg.solve(C11, u))\n    u2_var = np.diag(C22) -  np.sum(C12 * np.linalg.solve(C11, C12), axis=0)\n\n    return u2_mean, u2_var\n\nMX = 61\nMY = 61\nM = MX * MY # Number of points to infer.\n\n# Gross code to get mesh locations.\nxx, yy = np.meshgrid(np.linspace(-12, 12, MX), np.linspace(-12, 12, MY))\nx2 = np.hstack([xx.reshape((M, 1)), yy.reshape((M, 1))])\n\n# Interpolate!\nu2_mean, u2_var = interpolate_gp(x, u, x2, [vrange_val, sill_val, nugget_val], offset_prior)                                                                \n\n# Plot old values, new value means, new value variances\nfor locations, values in [(x, u), (x2, u2_mean), (x2, u2_var)]:\n    pp.scatter(locations[:, 0], locations[:, 1], c=values)\n    pp.xlim(-12, 12)\n    pp.ylim(-12, 12)\n    pp.show()\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"notebooks/predictions-with-mesh/predictions-with-mesh/","title":"Predictions with mesh","text":"<pre><code>from geostat import GP, Model, Mesh, NormalizingFeaturizer, Parameters\nimport geostat.kernel as krn\nimport matplotlib.pyplot as pp\nimport numpy as np\nfrom shapely.geometry import Point, Polygon\nimport shapely.vectorized as shv\nimport geopandas as gpd\nimport contextily as ctx\n</code></pre>"},{"location":"notebooks/predictions-with-mesh/predictions-with-mesh/#overview","title":"Overview","text":"<p>In this notebook we will show how <code>Mesh</code> is used to make prediction locations. We will:   * Generate synthetic data on a map of watersheds in Berkeley, California.   * Fit a Gaussian Process <code>Model</code> to the data.   * Make predictions using <code>Mesh</code>.</p>"},{"location":"notebooks/predictions-with-mesh/predictions-with-mesh/#synthesizing-data","title":"Synthesizing data","text":"<p>We will synthesize data at 200 random locations drawn from inside a polygon for Berkeley watersheds.</p> <pre><code>berkeleydf = gpd.read_file(\"./berkeley-watershed.zip\")\nberkeley = berkeleydf['geometry'].iloc[0]\n\nx0, y0, x1, y1 = berkeley.bounds\nlocs = np.random.uniform(size = [2000, 2]) * [x1-x0, y1-y0] + [x0, y0] # Generate 2000 points.\nmask = [berkeley.contains(Point(p)) for p in locs]\nlocs = locs[mask, :][:200, :] # Filter away points outside of shape and keep just 200.\n</code></pre> <p>Declare the terms of the spatial trend:</p> <pre><code>def trend_terms(x, y): return x, y\n</code></pre> <p>Create a featurizer that the Gaussian process class <code>GP</code> will use to convert locations into trend features:</p> <pre><code>featurizer = NormalizingFeaturizer(trend_terms, locs)\n</code></pre> <p>Make geostatistical parameters for the <code>GP</code>, instiate the <code>GP</code>, instantiate a <code>Model</code>, and call <code>generate</code> to generate synthetic observations.</p> <pre><code>p = Parameters(alpha=0.25, range=2000, sill=1., nugget=0.25)\n\ngp = GP(0, krn.TrendPrior(featurizer, alpha=p.alpha) + \\\n           krn.SquaredExponential(range=p.range, sill=p.sill) + \\\n           krn.Noise(nugget=p.nugget))\n\nmodel = Model(gp)\n\nobs = model.generate(locs).vals\n\nvmin, vmax = obs.min(), obs.max()\n</code></pre> <p>When the data is plotted, you can see an overall trend with some localized variations.</p> <pre><code>fig, ax = pp.subplots(figsize=(7, 7), dpi=120)\n\ncax = pp.scatter(locs[:, 0], locs[:, 1], c=obs, vmin=vmin, vmax=vmax)\n\nberkeleydf.plot(ax=ax, fc='none', ec='black', lw=1)\n\npp.colorbar(cax, shrink=0.7)\n\nctx.add_basemap(ax, attribution=False, crs=berkeleydf.crs.to_string(), \n                source='https://basemap.nationalmap.gov/arcgis/rest/services/'\n                       'USGSImageryOnly/MapServer/tile/{z}/{y}/{x}')\n\npp.title('Synthetic data')\n\npp.tight_layout()\npp.show()\n</code></pre> <p></p>"},{"location":"notebooks/predictions-with-mesh/predictions-with-mesh/#inferring-parameters","title":"Inferring parameters","text":"<p>Now set the parameters to something arbitrary to see if we can successfully infer model parameters. We call <code>fit</code> with the data (<code>locs</code> and <code>obs</code>).</p> <pre><code>model.set(alpha=1.0, range=1000.0, sill=0.5, nugget=0.5)\nmodel.fit(locs, obs, iters=300)\nNone\n</code></pre> <pre><code>[iter    30 ll -194.17 time  1.49 reg  0.00 alpha  1.24 sill  0.42 range 1336.73 nugget  0.37]\n[iter    60 ll -184.73 time  0.36 reg  0.00 alpha  1.12 sill  0.52 range 1687.33 nugget  0.29]\n[iter    90 ll -181.58 time  0.35 reg  0.00 alpha  1.01 sill  0.68 range 1964.81 nugget  0.25]\n[iter   120 ll -180.89 time  0.34 reg  0.00 alpha  0.96 sill  0.85 range 2135.51 nugget  0.24]\n[iter   150 ll -180.72 time  0.35 reg  0.00 alpha  0.93 sill  0.99 range 2234.71 nugget  0.24]\n[iter   180 ll -180.67 time  0.35 reg  0.00 alpha  0.92 sill  1.08 range 2291.47 nugget  0.24]\n[iter   210 ll -180.66 time  0.35 reg  0.00 alpha  0.91 sill  1.13 range 2321.38 nugget  0.24]\n[iter   240 ll -180.66 time  0.35 reg  0.00 alpha  0.91 sill  1.16 range 2336.16 nugget  0.24]\n[iter   270 ll -180.65 time  0.34 reg  0.00 alpha  0.90 sill  1.17 range 2342.93 nugget  0.24]\n[iter   300 ll -180.65 time  0.34 reg  0.00 alpha  0.90 sill  1.18 range 2345.77 nugget  0.24]\n</code></pre>"},{"location":"notebooks/predictions-with-mesh/predictions-with-mesh/#generating-predictions-in-convex-hull","title":"Generating predictions in convex hull","text":"<p>Create a mesh using a convex hull for making predictions.</p> <pre><code>mesh = Mesh.from_convex_hull(locs, nx=200)\n</code></pre> <p>Call <code>Model</code> to get predictions at mesh locations:</p> <pre><code>mean, var = model.predict(mesh.locations())\n</code></pre> <p>Create a slice for prediction mean and plot:</p> <pre><code>meshx, meshy, value = mesh.slice(mean)\n\nfig, ax = pp.subplots(figsize=(7, 7), dpi=120)\n\ncax = pp.pcolormesh(meshx, meshy, value, vmin=vmin, vmax=vmax)\n\nberkeleydf.plot(ax=ax, fc='none', ec='black', lw=1)\n\npp.colorbar(cax, shrink=0.7)\n\nctx.add_basemap(ax, attribution=False, crs=berkeleydf.crs.to_string(), \n                source='https://basemap.nationalmap.gov/arcgis/rest/services/'\n                       'USGSImageryOnly/MapServer/tile/{z}/{y}/{x}')\n\npp.title('Prediction mean')\n\npp.tight_layout()\npp.show()\n</code></pre> <p></p> <p>Do the same for prediction variance:</p> <pre><code>meshx, meshy, value = mesh.slice(var)\n\nfig, ax = pp.subplots(figsize=(7, 7), dpi=120)\n\ncax = pp.pcolormesh(meshx, meshy, value, cmap='gist_heat_r')\n\nberkeleydf.plot(ax=ax, fc='none', ec='black', lw=1)\n\npp.colorbar(cax, shrink=0.7)\n\nctx.add_basemap(ax, attribution=False, crs=berkeleydf.crs.to_string(), \n                source='https://basemap.nationalmap.gov/arcgis/rest/services/'\n                       'USGSImageryOnly/MapServer/tile/{z}/{y}/{x}')\n\npp.title('Prediction variance')\n\npp.tight_layout()\npp.show()\n</code></pre> <p></p>"},{"location":"notebooks/predictions-with-mesh/predictions-with-mesh/#generating-predictions-in-arbitrary-shape","title":"Generating predictions in arbitrary shape","text":"<p>The convex hull produces predictions both inside and outside the shape for Berkeley watersheds, which can be a bit awkward. Now instead, let's create a mesh using the shape, for making predictions.</p> <pre><code>mesh = Mesh.from_polygon(berkeley, nx=200)\n</code></pre> <p>Make predictions:</p> <pre><code>mean, var = model.predict(mesh.locations())\n</code></pre> <p>Create a slice for prediction mean and plot:</p> <pre><code>meshx, meshy, value = mesh.slice(mean)\n\nfig, ax = pp.subplots(figsize=(7, 7), dpi=120)\n\ncax = pp.pcolormesh(meshx, meshy, value, vmin=vmin, vmax=vmax)\n\n# Add contour\nvalue_contains = shv.contains(berkeleydf.geometry.item(), meshx, meshy)\nvalue_mask = np.where(value_contains, value, np.nan)\npp.contour(meshx, meshy, value_mask, colors='k', linewidths=0.5, alpha=0.8)\n\n\nberkeleydf.plot(ax=ax, fc='none', ec='black', lw=1)\n\npp.colorbar(cax, shrink=0.7)\n\nctx.add_basemap(ax, attribution=False, crs=berkeleydf.crs.to_string(), \n                source='https://basemap.nationalmap.gov/arcgis/rest/services/'\n                       'USGSImageryOnly/MapServer/tile/{z}/{y}/{x}')\n\npp.title('Prediction mean')\n\npp.tight_layout()\npp.show()\n</code></pre> <p></p> <p>Do the same for prediction variance:</p> <pre><code>meshx, meshy, value = mesh.slice(var)\n\nfig, ax = pp.subplots(figsize=(7, 7), dpi=120)\n\ncax = pp.pcolormesh(meshx, meshy, value, cmap='gist_heat_r')\n\nberkeleydf.plot(ax=ax, fc='none', ec='black', lw=1)\n\npp.colorbar(cax, shrink=0.7)\n\nctx.add_basemap(ax, attribution=False, crs=berkeleydf.crs.to_string(), \n                source='https://basemap.nationalmap.gov/arcgis/rest/services/'\n                       'USGSImageryOnly/MapServer/tile/{z}/{y}/{x}')\n\npp.title('Prediction variance')\n\npp.tight_layout()\npp.show()\n</code></pre> <p></p>"},{"location":"notebooks/wiener-process/wiener-process/","title":"Wiener process","text":"<pre><code>from geostat import GP, Model, Featurizer, Parameters\nimport geostat.kernel as krn\nimport matplotlib.pyplot as plt\nimport numpy as np\n</code></pre> <pre><code>x = np.linspace(-8, 8, 321)[:, np.newaxis]\n\nlocs = np.array([-4, -3, -2, -1, 0, 1, 2, 3, 4], dtype=float)[:, np.newaxis]\nobs = np.array([-2, -2, -2, -1, 0, 1, 2, 2, 2], dtype=float)\n\nfeaturizer = Featurizer(lambda x: (1., x))\nfeaturizer2 = Featurizer(lambda x: (1.,))\n\np = Parameters(alpha=0.25)\nkernel = krn.TrendPrior(featurizer, alpha=p.alpha) + krn.Noise(0.25)\nmodel = Model(GP(0, kernel)).fit(locs, obs, iters=500)\nmu1, _ = model.predict(x)\n\np = Parameters(range=0.33, sill=1.)\nkernel = krn.SquaredExponential(range=p.range, sill=p.sill) + krn.Noise(0.25)\nmodel = Model(GP(0, kernel)).fit(locs, obs, iters=500)\nmu2, _ = model.predict(x)\n\np = Parameters(alpha=0.25, sill=1.)\nkernel = krn.TrendPrior(featurizer2, alpha=p.alpha) \\\n       + krn.Constant(sill=p.sill) \\\n       * krn.Wiener(axis=0, start=-4) + krn.Noise(nugget=0.25)\nmodel = Model(GP(0, kernel)).fit(locs, obs, iters=500)\nmu3, _ = model.predict(x)\n</code></pre> <pre><code>[iter    50 ll -9.88 time  0.43 reg  0.00 alpha  0.21]\n[iter   100 ll -9.88 time  0.23 reg  0.00 alpha  0.21]\n[iter   150 ll -9.88 time  0.23 reg  0.00 alpha  0.21]\n[iter   200 ll -9.88 time  0.23 reg  0.00 alpha  0.21]\n[iter   250 ll -9.88 time  0.23 reg  0.00 alpha  0.21]\n[iter   300 ll -9.88 time  0.23 reg  0.00 alpha  0.21]\n[iter   350 ll -9.88 time  0.23 reg  0.00 alpha  0.21]\n[iter   400 ll -9.88 time  0.23 reg  0.00 alpha  0.21]\n[iter   450 ll -9.88 time  0.23 reg  0.00 alpha  0.21]\n[iter   500 ll -9.88 time  0.23 reg  0.00 alpha  0.21]\n[iter    50 ll -16.37 time  0.50 reg  0.00 sill  1.54 range  0.59]\n[iter   100 ll -13.03 time  0.21 reg  0.00 sill  1.72 range  1.09]\n[iter   150 ll -10.63 time  0.21 reg  0.00 sill  1.75 range  1.85]\n[iter   200 ll -9.75 time  0.21 reg  0.00 sill  1.93 range  2.56]\n[iter   250 ll -9.57 time  0.21 reg  0.00 sill  2.25 range  2.91]\n[iter   300 ll -9.52 time  0.21 reg  0.00 sill  2.55 range  3.03]\n[iter   350 ll -9.50 time  0.20 reg  0.00 sill  2.76 range  3.10]\n[iter   400 ll -9.50 time  0.21 reg  0.00 sill  2.90 range  3.14]\n[iter   450 ll -9.50 time  0.20 reg  0.00 sill  2.98 range  3.16]\n[iter   500 ll -9.50 time  0.21 reg  0.00 sill  3.02 range  3.18]\n[iter    50 ll -14.39 time  1.96 reg  0.00 alpha  0.41 sill  0.68]\n[iter   100 ll -13.64 time  0.21 reg  0.00 alpha  0.64 sill  0.65]\n[iter   150 ll -13.18 time  0.22 reg  0.00 alpha  0.93 sill  0.64]\n[iter   200 ll -12.91 time  0.22 reg  0.00 alpha  1.25 sill  0.64]\n[iter   250 ll -12.75 time  0.22 reg  0.00 alpha  1.59 sill  0.64]\n[iter   300 ll -12.66 time  0.22 reg  0.00 alpha  1.93 sill  0.64]\n[iter   350 ll -12.61 time  0.22 reg  0.00 alpha  2.24 sill  0.64]\n[iter   400 ll -12.58 time  0.22 reg  0.00 alpha  2.52 sill  0.64]\n[iter   450 ll -12.57 time  0.22 reg  0.00 alpha  2.77 sill  0.64]\n[iter   500 ll -12.56 time  0.22 reg  0.00 alpha  2.98 sill  0.64]\n</code></pre> <pre><code>plt.scatter(locs[:, 0], obs, marker='o', color='black')\nplt.plot(x[:, 0], mu1, label='Trend')\nplt.plot(x[:, 0], mu2, label='Stationary')\nplt.plot(x[:, 0], mu3, label='Wiener process')\nplt.legend(title='GP models')\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x7f9df0363ee0&gt;\n</code></pre>"}]}