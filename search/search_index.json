{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to GeoStat","text":"<p>Model space-time data with Gaussian processes.</p> <p>Geostat makes it easy to write Gaussian Process (GP) models with complex covariance functions. It uses maximum likelihood to fit model parameters. Under the hood it uses Tensorflow to fit models and do inference on GPUs. A good consumer GPU such as an Nvidia RTX 4090 can handle 10k data points.</p> <p>Visit our GitHub repository here.</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Install Geostat using pip:</p> <pre><code>pip install geostat\n</code></pre>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>README.md     # The readme file.\nmkdocs.yml    # The configuration file.\ndoc/\ndocs/\n    about.md  # The about page.\n    index.md  # The documentation homepage.\nsrc/geostat/\n    __init__.py\n    custom_op.py\n    kernel.py\n    krige.py\n    mean.py\n    mesh.py\n    metric.py\n    model.py\n    op.py\n    param.py\ntests/\n</code></pre>"},{"location":"about/","title":"About","text":"<p>Some background about the library and it's intended uses. Mention contributors, licences, and changelog.</p>"},{"location":"about/#disclaimer","title":"Disclaimer","text":"<p>This software is preliminary or provisional and is subject to revision. It is being provided to meet the need for timely best science. The software has not received final approval by the U.S. Geological Survey (USGS). No warranty, expressed or implied, is made by the USGS or the U.S. Government as to the functionality of the software and related material nor shall the fact of release constitute any such warranty. The software is provided on the condition that neither the USGS nor the U.S. Government shall be held liable for any damages resulting from the authorized or unauthorized use of the software.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#src.geostat.custom_op","title":"<code>src.geostat.custom_op</code>","text":""},{"location":"api/#src.geostat.custom_op.function","title":"<code>function(f)</code>","text":"A custom op involves two functions <ul> <li>f, where it is defined, and</li> <li>g, where it is introduced into the graph.</li> </ul> Source code in <code>src/geostat/custom_op.py</code> <pre><code>def function(f):\n    \"\"\"\n    A custom op involves two functions:\n      * f, where it is defined, and\n      * g, where it is introduced into the graph.\n    \"\"\"\n    def g(*args):\n      # Assume for now that all arguments are Parameters\n      return CustomOp(f, **{p.name: p for p in args})\n    return g\n</code></pre>"},{"location":"api/#src.geostat.kernel","title":"<code>src.geostat.kernel</code>","text":""},{"location":"api/#src.geostat.kernel.Kernel","title":"<code>Kernel</code>","text":"<p>               Bases: <code>Op</code></p> Source code in <code>src/geostat/kernel.py</code> <pre><code>class Kernel(Op):\n    def __init__(self, fa, autoinputs):\n        if 'offset' not in autoinputs: autoinputs['offset'] = 'offset'\n        if 'locs1' not in autoinputs: autoinputs['locs1'] = 'locs1'\n        if 'locs2' not in autoinputs: autoinputs['locs2'] = 'locs2'\n        super().__init__(fa, autoinputs)\n\n    def __add__(self, other):\n        if other is None:\n            return self\n        else:\n            return Stack([self]) + other\n\n    def __mul__(self, other):\n        return Product([self]) * other\n\n    def call(self, e):\n        \"\"\"\n        Returns tuple `(mean, covariance)` for locations.\n        Return values may be unbroadcasted.\n        \"\"\"\n        pass\n\n    def __call__(self, e):\n        \"\"\"\n        Returns tuple `(mean, covariance)` for locations.\n        Return values have correct shapes.\n        \"\"\"\n        C = self.call(e)\n        if C is None: C = 0.\n        n1 = tf.shape(e['locs1'])[0]\n        n2 = tf.shape(e['locs2'])[0]\n        C = tf.broadcast_to(C, [n1, n2])\n        return C\n\n    def report(self):\n        string = ', '.join('%s %4.2f' % (v.name, p[v.name]) for v in self.vars())\n        return '[' + string + ']'\n</code></pre>"},{"location":"api/#src.geostat.kernel.Kernel.__call__","title":"<code>__call__(e)</code>","text":"<p>Returns tuple <code>(mean, covariance)</code> for locations. Return values have correct shapes.</p> Source code in <code>src/geostat/kernel.py</code> <pre><code>def __call__(self, e):\n    \"\"\"\n    Returns tuple `(mean, covariance)` for locations.\n    Return values have correct shapes.\n    \"\"\"\n    C = self.call(e)\n    if C is None: C = 0.\n    n1 = tf.shape(e['locs1'])[0]\n    n2 = tf.shape(e['locs2'])[0]\n    C = tf.broadcast_to(C, [n1, n2])\n    return C\n</code></pre>"},{"location":"api/#src.geostat.kernel.Kernel.call","title":"<code>call(e)</code>","text":"<p>Returns tuple <code>(mean, covariance)</code> for locations. Return values may be unbroadcasted.</p> Source code in <code>src/geostat/kernel.py</code> <pre><code>def call(self, e):\n    \"\"\"\n    Returns tuple `(mean, covariance)` for locations.\n    Return values may be unbroadcasted.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#src.geostat.kernel.Mix","title":"<code>Mix</code>","text":"<p>               Bases: <code>Kernel</code></p> Source code in <code>src/geostat/kernel.py</code> <pre><code>class Mix(Kernel):\n    def __init__(self, inputs, weights=None):\n        self.inputs = inputs\n        fa = {}\n        ai = dict(cats1='cats1', cats2='cats2')\n\n        # Special case if weights is not given.\n        if weights is not None:\n            fa['weights'] = weights\n            ai['inputs'] = inputs\n\n        super().__init__(fa, ai)\n\n    def gather_vars(self, cache=None):\n        \"\"\"Make a special version of gather_vars because\n           we want to gather variables from `inputs`\n           even when it's not in autoinputs\"\"\"\n        vv = super().gather_vars(cache)\n        for iput in self.inputs:\n            cache[id(self)] |= iput.gather_vars(cache)\n        return cache[id(self)]\n\n    def vars(self):\n        if 'weights' in self.fa:\n            return {k: p for row in self.fa['weights']\n                      for k, p in get_trend_coefs(row).items()}\n        else:\n            return {}\n\n    def call(self, e):\n        if 'weights' in e:\n            weights = []\n            for row in e['weights']:\n                if isinstance(row, (tuple, list)):\n                    row = tf.stack(row)\n                    weights.append(row)\n            weights = tf.stack(weights)\n            C = tf.stack(e['inputs'], axis=-1) # [locs, locs, numinputs].\n            Aaug1 = tf.gather(weights, e['cats1']) # [locs, numinputs].\n            Aaug2 = tf.gather(weights, e['cats2']) # [locs, numinputs].\n            outer = tf.einsum('ac,bc-&gt;abc', Aaug1, Aaug2) # [locs, locs, numinputs].\n            C = tf.einsum('abc,abc-&gt;ab', C, outer) # [locs, locs].\n            return C\n        else:\n            # When weights is not given, exploit the fact that we don't have\n            # to compute every element in component covariance matrices.\n            N = len(self.inputs)\n            catcounts1 = tf.math.bincount(e['cats1'], minlength=N, maxlength=N)\n            catcounts2 = tf.math.bincount(e['cats2'], minlength=N, maxlength=N)\n            catindices1 = tf.math.cumsum(catcounts1, exclusive=True)\n            catindices2 = tf.math.cumsum(catcounts2, exclusive=True)\n            catdiffs = tf.unstack(catindices2 - catindices1, num=N)\n            locsegs1 = tf.split(e['locs1'], catcounts1, num=N)\n            locsegs2 = tf.split(e['locs2'], catcounts2, num=N)\n\n            # TODO: Check that the below is still correct.\n            CC = [] # Observation noise submatrices.\n            for sublocs1, sublocs2, catdiff, iput in zip(locsegs1, locsegs2, catdiffs, self.inputs):\n                cache = dict(\n                    offset = e['offset'] + catdiff,\n                    locs1 = sublocs1,\n                    locs2 = sublocs2)\n                cache['per_axis_dist2'] = PerAxisDist2().run(cache)\n                cache['euclidean'] = Euclidean().run(cache)\n                Csub = iput.run(cache)\n                CC.append(Csub)\n\n            return block_diag(CC)\n</code></pre>"},{"location":"api/#src.geostat.kernel.Mix.gather_vars","title":"<code>gather_vars(cache=None)</code>","text":"<p>Make a special version of gather_vars because we want to gather variables from <code>inputs</code> even when it's not in autoinputs</p> Source code in <code>src/geostat/kernel.py</code> <pre><code>def gather_vars(self, cache=None):\n    \"\"\"Make a special version of gather_vars because\n       we want to gather variables from `inputs`\n       even when it's not in autoinputs\"\"\"\n    vv = super().gather_vars(cache)\n    for iput in self.inputs:\n        cache[id(self)] |= iput.gather_vars(cache)\n    return cache[id(self)]\n</code></pre>"},{"location":"api/#src.geostat.kernel.Observation","title":"<code>Observation</code>","text":"<p>               Bases: <code>Op</code></p> Source code in <code>src/geostat/kernel.py</code> <pre><code>class Observation(Op):\n\n    def __init__(self,\n        coefs: List,\n        noise: Kernel\n    ):\n        self.coefs = coefs\n        self.noise = noise\n        super().__init__({}, self.noise)\n\n    def vars(self):\n        vv = {k: p for c in self.coefs for k, p in upp(c)}\n        vv |= self.noise.vars()\n        return vv\n\n    def __call__(self, e):\n        \"\"\"\n        Dummy.\n        \"\"\"\n        return 0.\n</code></pre>"},{"location":"api/#src.geostat.kernel.Observation.__call__","title":"<code>__call__(e)</code>","text":"<p>Dummy.</p> Source code in <code>src/geostat/kernel.py</code> <pre><code>def __call__(self, e):\n    \"\"\"\n    Dummy.\n    \"\"\"\n    return 0.\n</code></pre>"},{"location":"api/#src.geostat.kernel.block_diag","title":"<code>block_diag(blocks)</code>","text":"<p>Return a dense block-diagonal matrix.</p> Source code in <code>src/geostat/kernel.py</code> <pre><code>def block_diag(blocks):\n    \"\"\"Return a dense block-diagonal matrix.\"\"\"\n    return LOBlockDiag([LOFullMatrix(b) for b in blocks]).to_dense()\n</code></pre>"},{"location":"api/#src.geostat.kernel.quadstack","title":"<code>quadstack(x, sills, ranges)</code>","text":"<p><code>x</code> has arbitrary shape [...], but must be non-negative. <code>sills</code> and <code>ranges</code> both have shape [K].</p> Source code in <code>src/geostat/kernel.py</code> <pre><code>@tf.recompute_grad\ndef quadstack(x, sills, ranges):\n    \"\"\"\n    `x` has arbitrary shape [...], but must be non-negative.\n    `sills` and `ranges` both have shape [K].\n    \"\"\"\n    ex = ed(x)\n    ax = tf.maximum(0., 1. - tf.abs(ex) / ranges) # [..., 1]\n    y = sills * tf.square(ax) # [..., K]\n    return tf.reduce_sum(y, -1)\n</code></pre>"},{"location":"api/#src.geostat.kernel.rampstack","title":"<code>rampstack(x, sills, ranges)</code>","text":"<p><code>x</code> has arbitrary shape [...], but must be non-negative. <code>sills</code> and <code>ranges</code> both have shape [K].</p> Source code in <code>src/geostat/kernel.py</code> <pre><code>@tf.custom_gradient\ndef rampstack(x, sills, ranges):\n    \"\"\"\n    `x` has arbitrary shape [...], but must be non-negative.\n    `sills` and `ranges` both have shape [K].\n    \"\"\"\n    ax = ed(tf.abs(x)) # [..., 1]\n    y = sills * tf.maximum(0., 1. - ax / ranges) # [..., K]\n    def grad(upstream):\n        ax = ed(tf.abs(x)) # [..., 1]\n        y = sills * tf.maximum(0., 1. - ax / ranges) # [..., K]\n        K = tf.shape(sills)[0]\n        small = ax &lt; ranges\n        grad_x = upstream * tf.reduce_sum(tf.where(small, -tf.sign(ed(x)) * (sills / ranges), 0.), -1) # [...]\n        grad_sills = tf.einsum('ak,a-&gt;k', tf.reshape(y, [-1, K]), tf.reshape(upstream, [-1]))\n        grad_ranges = tf.where(small, ax * (sills / tf.square(ranges)), 0.) # [..., K}\n        grad_ranges = tf.einsum('ak,a-&gt;k', tf.reshape(grad_ranges, [-1, K]), tf.reshape(upstream, [-1]))\n        return grad_x, grad_sills, grad_ranges\n    return tf.reduce_sum(y, -1), grad\n</code></pre>"},{"location":"api/#src.geostat.kernel.smooth_convex","title":"<code>smooth_convex(x, sills, ranges)</code>","text":"<p><code>x</code> has arbitrary shape [...], but must be non-negative. <code>sills</code> and <code>ranges</code> both have shape [K].</p> Source code in <code>src/geostat/kernel.py</code> <pre><code>@tf.recompute_grad\ndef smooth_convex(x, sills, ranges):\n    \"\"\"\n    `x` has arbitrary shape [...], but must be non-negative.\n    `sills` and `ranges` both have shape [K].\n    \"\"\"\n    r2 = ranges\n    r1 = tf.pad(ranges[:-1], [[1, 0]])\n    ex = ed(x)\n    ax = tf.abs(ex)\n    rx = ax / r2 - 1.\n\n    c1 = 2. / (r1 + r2)\n    c2 = 1. / (1. - tf.square(r1/r2))\n\n    # i1 = tf.cast(ax &lt;= r1, tf.float32) # Indicates x &lt;= r1.\n    # i2 = tf.cast(ax &lt;= r2, tf.float32) * (1. - i1) # Indicates r1 &lt; x &lt;= r2.\n    # v = i1 * (1. - c1 * ax) + i2 * c2 * tf.square(rx)\n\n    v = tf.where(ax &lt;= r1, 1. - c1 * ax, c2 * tf.square(rx))\n    v = tf.where(ax &lt;= r2, v, 0.)\n\n    y = tf.einsum('...k,k-&gt;...', v, sills)\n    return y\n</code></pre>"},{"location":"api/#src.geostat.kernel.smooth_convex_grad","title":"<code>smooth_convex_grad(x, sills, ranges)</code>","text":"<p><code>x</code> has arbitrary shape [...], but must be non-negative. <code>sills</code> and <code>ranges</code> both have shape [K].</p> Source code in <code>src/geostat/kernel.py</code> <pre><code>@tf.custom_gradient\ndef smooth_convex_grad(x, sills, ranges):\n    \"\"\"\n    `x` has arbitrary shape [...], but must be non-negative.\n    `sills` and `ranges` both have shape [K].\n    \"\"\"\n    r2 = ranges\n    r1 = tf.pad(ranges[:-1], [[1, 0]])\n    ex = ed(x)\n    ax = tf.abs(ex)\n    rx = ax / r2 - 1.\n\n    c1 = 2. / (r1 + r2)\n    c2 = 1. / (1. - tf.square(r1/r2))\n\n    v = tf.where(ax &lt;= r1, 1. - c1 * ax, c2 * tf.square(rx))\n    v = tf.where(ax &lt;= r2, v, 0.)\n\n    y = tf.einsum('...k,k-&gt;...', v, sills)\n\n    def grad(upstream):\n        r2 = ranges\n        r1 = tf.pad(ranges[:-1], [[1, 0]])\n        ex = ed(x)\n        ax = tf.abs(ex)\n        rx = ax / r2 - 1.\n        i1 = tf.cast(ax &lt;= r1, tf.float32) # Indicates x &lt;= r1.\n        i2 = tf.cast(ax &lt;= r2, tf.float32) * (1. - i1) # Indicates r1 &lt; x &lt;= r2.\n\n        c1 = 2. / (r1 + r2)\n        c2 = 1. / (1. - tf.square(r1 / r2))\n        c3 = 1. / (r2 - tf.square(r1) / r2)\n\n        v = i1 * (1. - c1 * ax) + i2 * c2 * tf.square(rx)\n\n        sx = tf.sign(ex)\n\n        K = tf.shape(sills)[0]\n        gx = sx * sills * (i1 * -c1 + i2 * rx * (2 * c3))\n        grad_x = upstream * tf.reduce_sum(gx, -1) # [...]\n\n        grad_sills = tf.einsum('ak,a-&gt;k', tf.reshape(v, [-1, K]), tf.reshape(upstream, [-1]))\n\n        u = 2 / tf.square(r1 + r2) * ax * i1\n        yr1 = u + i2 * tf.square(rx * c3) * 2 * r1\n        yr2 = u - 2 * i2 * (rx * c3 + tf.square(rx * c2) / r2)\n        yr1 = sills * tf.reshape(yr1, [-1, K])\n        yr2 = sills * tf.reshape(yr2, [-1, K])\n        yr = tf.pad(yr1[:, 1:], [[0, 0], [0, 1]]) + yr2\n        grad_ranges = tf.einsum('ak,a-&gt;k', yr, tf.reshape(upstream, [-1]))\n\n        return grad_x, grad_sills, grad_ranges\n\n    return y, grad\n</code></pre>"},{"location":"api/#src.geostat.krige","title":"<code>src.geostat.krige</code>","text":""},{"location":"api/#src.geostat.krige.Krige","title":"<code>Krige</code>","text":"Source code in <code>src/geostat/krige.py</code> <pre><code>class Krige():\n\n    def __init__(self, \n                 x1, u1, \n                 bins,\n                 variogram_func=None, \n                 variogram_params=None, \n                 cutoff_dist='auto',\n                 featurization=None,\n                 projection=None,\n                 show_plots=True, \n                 verbose=True,\n                 ):\n\n        '''\n        Parameters:\n                x1 : n-dim array\n                    Locations of input data.\n\n                u1 : 1-d array\n                    Values to be kriged.\n\n                bins : int or None\n                    The number of bins to use on the variogram cloud.\n                    If None, variogram function is fit to the variogram cloud\n                    and is not binned first.\n\n                variogram_func : str\n                     Name of the variogram model to use in the kriging. \n                     Should be 'linear', 'gaussian', or 'spherical'.\n                     Default is 'gaussian'.\n\n                cutoff_dist : str or int, optional\n                    The maximum lag distance to include in variogram modeling.\n\n                featurization : function, optional\n                    Should be a function that takes x1 (n-dim array of input data) \n                    and returns the coordinates, i.e., x, y, x**2, y**2.\n                    Example below.\n                    Default is None.\n\n                project : function, opt\n                    A function that takes multiple vectors, and returns\n                    a tuple of projected vectors.\n\n                epsg_proj : str\n                    The projected coordinate system to use. Ignored if project=False.\n                    Default is 'EPSG:3310' (California Albers).\n\n                show_plots : boolean, optional\n                    Whether or not to show variogram plots.\n                    Default is True.\n\n                verbose : boolean, optional\n                    Whether or not to print parameters.\n                    Default is True.\n\n\n        Performs experimental variogram calculation, bins data, and fits variogram model to estimate variogram parameters.       \n\n        Performs ordinary and universal kriging in up to 3 spatial dimensions.\n\n\n        Trend model example:\n        def featurization(x1):    \n            return x1[:, 0], x1[:, 1], x1[:, 0]**2, x1[:, 1]**2\n\n\n        '''\n\n        super().__init__(projection=projection)\n\n        # Save original x1 for stats function.\n        self.x1_original = x1\n\n        # Check and change the shape of u1 if needed (for pdist).\n        if u1.ndim == 1:\n            self.u1 = u1\n        elif u1.shape[1] == 1:\n            self.u1 = u1.reshape(-1)\n        else:\n            raise ValueError(\"Check dimensions of 'u1'.\")\n\n        # Projection.\n        self.x1 = self.project(x1)\n\n        # Variogram.\n        if variogram_func == 'gaussian':\n            self.variogram_func = gaussian\n        elif variogram_func == 'spherical':\n            self.variogram_func = spherical\n        elif variogram_func == 'linear':\n            self.variogram_func = linear\n        else:  \n            raise ValueError(\"Variogram function must be 'linear', 'gaussian', or 'spherical'.\")\n\n\n        if cutoff_dist == 'auto':\n            self.cutoff_dist = cutoff_dist_func(self.x1)\n        else:\n            self.cutoff_dist = cutoff_dist\n\n\n        self.verbose = verbose\n        self.show_plots = show_plots\n        self.bins = bins\n        self.featurization = featurization\n\n\n        # Lags and semivariance.\n        dist = pdist(self.x1, metric='euclidean')\n        gamma = 0.5 * pdist(self.u1.reshape(-1, 1), metric='sqeuclidean')\n\n\n        if self.bins != None:  # Use bins.\n            # Variogram cloud calculation.\n            bin_means, bin_edges, binnumber = binned_statistic(dist, gamma, \n                                                               statistic='mean', \n                                                               bins=self.bins, \n                                                               range=[dist.min(), \n                                                                      self.cutoff_dist])\n            bin_width = (bin_edges[1] - bin_edges[0])\n            bin_centers = bin_edges[1:] - bin_width / 2\n\n            # Bin counts calculation.\n            bin_count, bin_count_edges, bin_count_number = binned_statistic(dist, gamma, \n                                                                            statistic='count', \n                                                                            bins=self.bins)\n            bin_count_width = (bin_count_edges[1] - bin_count_edges[0])\n            bin_count_centers = bin_count_edges[1:] - bin_count_width/2\n\n        if self.show_plots == True:\n            if self.bins == None:\n\n                # Variogram cloud plot.\n                plt.figure(dpi=100)\n                plt.scatter(dist, gamma, ec='C0', fc='none', alpha=0.3)\n                plt.ylabel('$\\gamma(h)$')\n                plt.xlabel('$h$')\n                plt.grid(alpha=0.4)\n                plt.show()\n\n            if self.bins != None:\n\n                # Variogram cloud plot.\n                plt.figure(dpi=100)\n                plt.scatter(dist, gamma, ec='C0', fc='none', alpha=0.3)\n                plt.hlines(bin_means, bin_edges[:-1], bin_edges[1:], zorder=1, color='k')\n                plt.scatter(bin_centers, bin_means, ec='k', lw=0.5)\n                plt.ylabel('$\\gamma(h)$')\n                plt.xlabel('$h$')\n                plt.grid(alpha=0.4)\n                plt.show()\n\n                # Bin counts plot.\n                plt.figure(dpi=100)\n                plt.hlines(bin_count, bin_count_edges[:-1], bin_count_edges[1:], zorder=1, color='k')\n                plt.scatter(bin_count_centers, bin_count, ec='k', lw=0.5)\n                plt.ylabel('Bin count')\n                plt.xlabel('$h$')\n                plt.grid(alpha=0.4)\n                plt.show()\n\n\n\n        ############ Fit the variogram model.\n\n        if not variogram_params:  # Fit to the data.\n\n            if self.bins == None:  # Fit the variogram cloud.\n\n                if self.variogram_func == gaussian or self.variogram_func == spherical:\n\n                    # Initial guess for the parameters.\n                    p0 = [0.25 * np.max(dist),            # range\n                          np.max(gamma) - np.min(gamma),  # sill\n                          np.min(gamma) + 1e-6]           # nugget\n\n                    # Bounds with constraints.\n                    bounds = [(1e-6, 1e-6, 1e-6), (np.inf, np.inf, np.inf)]\n\n                    # Apply the cutoff.\n                    dist_cut = np.where(dist &lt; self.cutoff_dist, dist, np.nan)\n                    gamma_cut = np.where(dist &lt; self.cutoff_dist, gamma, np.nan)\n\n                    # Remove nans for curve_fit.\n                    dist_cut = dist_cut[~np.isnan(dist_cut)]\n                    gamma_cut = gamma_cut[~np.isnan(gamma_cut)]\n\n                    popt, pcov = curve_fit(self.variogram_func, dist_cut, gamma_cut, p0=p0, bounds=bounds)\n\n                    if self.show_plots == True:\n\n                        # Calculate 2d kde to help confirm cutoff.\n                        xi = np.linspace(np.min(dist), np.max(dist), 60)\n                        yi = np.linspace(np.min(gamma), np.max(gamma), 60)\n                        xi, yi = np.meshgrid(xi, yi)\n                        xyi = np.stack([xi.reshape(-1), yi.reshape(-1)], axis=1).T\n\n                        kde = stats.gaussian_kde([dist, gamma])\n                        z = kde.evaluate(xyi)\n                        z = z.reshape(len(xi), len(yi))\n\n                        plt.figure(dpi=100)\n                        plt.pcolormesh(xi, yi, z, cmap=plt.cm.Blues, shading='auto')\n                        plt.ylabel('$\\gamma(h)$')\n                        plt.xlabel('$h$')\n                        plt.grid(alpha=0.4)\n                        plt.show()\n\n                        # With cutoff and variogram model.\n                        xnew = np.linspace(np.min(dist_cut), self.cutoff_dist, 100)\n                        plt.figure(dpi=100)\n                        plt.scatter(dist_cut, gamma_cut, fc='none', ec='C1', lw=0.5, alpha=0.3)\n                        plt.plot(xnew, self.variogram_func(xnew, *popt), color='k')\n                        plt.ylabel('$\\gamma(h)$')\n                        plt.xlabel('$h$')\n                        plt.grid(alpha=0.4)\n                        plt.show()\n\n\n                    vrange = popt[0]\n                    sill = popt[1]\n                    nugget = popt[2]\n\n\n                    if self.verbose == True:\n                        print('variogram model: {}'.format(variogram_func))\n                        print('cutoff: {:.2f}'.format(self.cutoff_dist))\n                        print('range: {:.5f}'.format(vrange))\n                        print('sill: {:.5f}'.format(sill))\n                        print('nugget: {:.5f}'.format(nugget))\n                        print('full sill: {:.5f}'.format(sill + nugget))\n\n                    self.parameter_vals = [vrange, sill, nugget]\n\n                elif self.variogram_func == linear:\n\n                    # Initial guess for the parameters.\n                    p0 = [(np.max(dist) - np.min(dist)) / (np.max(gamma) - np.min(gamma)),  # slope\n                          np.min(gamma) + 1e-6]                                             # nugget\n\n                    # Bounds with constraints.\n                    bounds = [(1e-6, 1e-6), (np.inf, np.inf)]\n\n                    # Apply the cutoff.\n                    dist_cut = np.where(dist &lt; self.cutoff_dist, dist, np.nan)\n                    gamma_cut = np.where(dist &lt; self.cutoff_dist, gamma, np.nan)\n\n                    # Remove nans for curve_fit.\n                    dist_cut = dist_cut[~np.isnan(dist_cut)]\n                    gamma_cut = gamma_cut[~np.isnan(gamma_cut)]\n\n                    popt, pcov = curve_fit(self.variogram_func, dist_cut, gamma_cut, p0=p0, bounds=bounds)\n\n                    if self.show_plots == True:\n\n                        # Calculate 2d kde to help confirm cutoff.\n                        xi = np.linspace(np.min(dist), np.max(dist), 60)\n                        yi = np.linspace(np.min(gamma), np.max(gamma), 60)\n                        xi, yi = np.meshgrid(xi, yi)\n                        xyi = np.stack([xi.reshape(-1), yi.reshape(-1)], axis=1).T\n\n                        kde = stats.gaussian_kde([dist, gamma])\n                        z = kde.evaluate(xyi)\n                        z = z.reshape(len(xi), len(yi))\n\n                        plt.figure(dpi=100)\n                        plt.pcolormesh(xi, yi, z, cmap=plt.cm.Blues, shading='auto')\n                        plt.ylabel('$\\gamma(h)$')\n                        plt.xlabel('$h$')\n                        plt.grid(alpha=0.4)\n                        plt.show()\n\n                        # With cutoff and variogram model.\n                        xnew = np.linspace(np.min(dist_cut), self.cutoff_dist, 100)\n                        plt.figure(dpi=100)\n                        plt.scatter(dist_cut, gamma_cut, fc='none', ec='C1', lw=0.5, alpha=0.3)\n                        plt.plot(xnew, self.variogram_func(xnew, *popt), color='k')\n                        plt.ylabel('$\\gamma(h)$')\n                        plt.xlabel('$h$')\n                        plt.grid(alpha=0.4)\n                        plt.show()\n\n\n                    slope = popt[0]\n                    nugget = popt[1]\n\n                    if self.verbose == True:\n                        print('variogram model: {}'.format(variogram_func))\n                        print('cutoff: {:.2f}'.format(self.cutoff_dist))\n                        print('slope: {:.5f}'.format(slope))\n                        print('nugget: {:.5f}'.format(nugget))\n\n\n                    self.parameter_vals = [slope, nugget]\n\n\n\n\n            elif self.bins != None:  # Fit the binned data.\n                if self.variogram_func == gaussian or self.variogram_func == spherical:\n\n                    # Initial guess for the parameters.\n                    p0 = [0.25 * np.max(bin_centers),                 # range\n                              np.max(bin_means) - np.min(bin_means),  # sill\n                              np.min(bin_means)]                      # nugget\n\n                    # Bounds with constraints.\n                    bounds = [(1e-6, 1e-6, 1e-6), (np.inf, np.inf, np.inf)]\n\n                    popt, pcov = curve_fit(self.variogram_func, bin_centers, bin_means, p0=p0, bounds=bounds)\n\n                    if self.show_plots == True:\n\n                        # Fit variogram plot.\n                        plt.figure(dpi=100)\n                        plt.scatter(bin_centers, bin_means, c='C1', ec='k', lw=0.5)\n                        plt.plot(bin_centers, self.variogram_func(bin_centers, *popt), color='k')\n                        plt.ylabel('$\\gamma(h)$')\n                        plt.xlabel('$h$')\n                        plt.grid(alpha=0.4)\n                        plt.show()\n\n                    vrange = popt[0]\n                    sill = popt[1]\n                    nugget = popt[2]\n\n\n                    if self.verbose == True:\n                        print('variogram model: {}'.format(variogram_func))\n                        print('cutoff: {:.2f}'.format(self.cutoff_dist))\n                        print('range: {:.5f}'.format(vrange))\n                        print('sill: {:.5f}'.format(sill))\n                        print('nugget: {:.5f}'.format(nugget))\n                        print('full sill: {:.5f}'.format(sill + nugget))\n\n                    self.parameter_vals = [vrange, sill, nugget]\n\n                elif self.variogram_func == linear:\n\n                    # Initial guess for the parameters.\n                    p0 = [(np.max(bin_centers) - np.min(bin_centers)) / (np.max(bin_means) - np.min(bin_means)),  # slope\n                          np.min(bin_means)]                                                                      # nugget\n\n                    # Bounds with constraints.\n                    bounds = [(1e-6, 1e-6), (np.inf, np.inf)]\n\n                    popt, pcov = curve_fit(self.variogram_func, bin_centers, bin_means, p0=p0, bounds=bounds)\n\n                    if self.show_plots == True:\n\n                        # Fit variogram plot.\n                        plt.figure(dpi=100)\n                        plt.scatter(bin_centers, bin_means, c='C1', ec='k', lw=0.5)\n                        plt.plot(bin_centers, self.variogram_func(bin_centers, *popt), color='k')\n                        plt.ylabel('$\\gamma(h)$')\n                        plt.xlabel('$h$')\n                        plt.grid(alpha=0.4)\n                        plt.show()\n\n                    slope = popt[0]\n                    nugget = popt[1]\n\n                    if self.verbose == True:\n                        print('variogram model: {}'.format(variogram_func))\n                        print('cutoff: {:.2f}'.format(self.cutoff_dist))\n                        print('slope: {:.5f}'.format(slope))\n                        print('nugget: {:.5f}'.format(nugget))\n\n\n                    self.parameter_vals = [slope, nugget]\n\n\n        else:  # Use the given variogram parameters.\n            self.parameter_vals = variogram_params\n\n            # Apply the cutoff.\n            dist_cut = np.where(dist &lt; self.cutoff_dist, dist, np.nan)\n            gamma_cut = np.where(dist &lt; self.cutoff_dist, gamma, np.nan)\n\n            # Remove nans for curve_fit.\n            dist_cut = dist_cut[~np.isnan(dist_cut)]\n            gamma_cut = gamma_cut[~np.isnan(gamma_cut)]\n\n\n            if self.show_plots == True:\n\n                if self.bins == None:\n\n                    # Calculate 2d kde to help confirm cutoff.\n                    xi = np.linspace(np.min(dist), np.max(dist), 60)\n                    yi = np.linspace(np.min(gamma), np.max(gamma), 60)\n                    xi, yi = np.meshgrid(xi, yi)\n                    xyi = np.stack([xi.reshape(-1), yi.reshape(-1)], axis=1).T\n\n                    kde = stats.gaussian_kde([dist, gamma])\n                    z = kde.evaluate(xyi)\n                    z = z.reshape(len(xi), len(yi))\n\n                    plt.figure(dpi=100)\n                    plt.pcolormesh(xi, yi, z, cmap=plt.cm.Blues, shading='auto')\n                    plt.ylabel('$\\gamma(h)$')\n                    plt.xlabel('$h$')\n                    plt.grid(alpha=0.4)\n                    plt.show()\n\n                    # With cutoff and variogram model.\n                    xnew = np.linspace(np.min(dist_cut), self.cutoff_dist, 100)\n                    plt.figure(dpi=100)\n                    plt.scatter(dist_cut, gamma_cut, fc='none', ec='C1', lw=0.5, alpha=0.3)\n                    plt.plot(xnew, self.variogram_func(xnew, *self.parameter_vals), color='k')\n                    plt.ylabel('$\\gamma(h)$')\n                    plt.xlabel('$h$')\n                    plt.grid(alpha=0.4)\n                    plt.show()\n\n                else:\n\n                    # Fit variogram plot.\n                    plt.figure(dpi=100)\n                    plt.scatter(bin_centers, bin_means, fc='C1', ec='k', lw=0.5)\n                    plt.plot(bin_centers, self.variogram_func(bin_centers, *self.parameter_vals), color='k')\n                    plt.ylabel('$\\gamma(h)$')\n                    plt.xlabel('$h$')\n                    plt.grid(alpha=0.4)\n                    plt.show()\n\n            if self.variogram_func == gaussian or self.variogram_func == spherical:\n                vrange = self.parameter_vals[0]\n                sill = self.parameter_vals[1]\n                nugget = self.parameter_vals[2]\n\n                if self.verbose == True:\n                    print('variogram model: {}'.format(variogram_func))\n                    print('cutoff: {:.2f}'.format(self.cutoff_dist))\n                    print('range: {:.5f}'.format(vrange))\n                    print('sill: {:.5f}'.format(sill))\n                    print('nugget: {:.5f}'.format(nugget))\n                    print('full sill: {:.5f}'.format(sill + nugget))\n\n            elif self.variogram_func == linear:\n                slope = self.parameter_vals[0]\n                nugget = self.parameter_vals[1]\n\n                if self.verbose == True:\n                    print('variogram model: {}'.format(variogram_func))\n                    print('cutoff: {:.2f}'.format(self.cutoff_dist))\n                    print('slope: {:.5f}'.format(slope))\n                    print('nugget: {:.5f}'.format(nugget))\n\n\n\n\n####################################################\n\n    def predict(self, x2_pred):\n\n        '''\n        Parameters:\n                x2 : n-dim array\n                    Locations to make kriging predictions.\n\n        Returns:\n                u2_mean : float\n                    Kriging mean.\n\n                u2_var : float\n                    Kriging variance.\n\n\n        Performs ordinary or universal kriging using the estimated variogram parameters.\n\n        '''\n\n        self.x2 = self.project(x2_pred)\n\n        n1 = len(self.x1)\n        n2 = len(self.x2)\n\n        # Universal krige.\n        if self.featurization:            \n            # Ax = b with a trend.\n\n            # Build A\n            D1 = cdist(self.x1, self.x1)\n\n            drift_data = np.array(list(self.featurization(self.x1)))\n\n            An = n1 + 1 + drift_data.shape[0]\n\n            A = np.zeros((An, An))\n            A[:n1, :n1] = -self.variogram_func(D1, *self.parameter_vals)\n            np.fill_diagonal(A, 0.)\n            A[n1, :n1] = 1.\n            A[:n1, n1] = 1.\n            A[n1, n1] = 0.\n\n            # Add in the trend for A.\n            for i in range(drift_data.shape[0]):\n                A[n1+i+1, :n1] = drift_data[i]\n                A[:n1, n1+i+1] = drift_data[i]\n\n            # Build b.\n            D2 = cdist(self.x2, self.x1)\n            b = np.zeros((D2.shape[0], D2.shape[1] + 1 + drift_data.shape[0]))\n            b[:n2, :n1] = -self.variogram_func(D2, *self.parameter_vals)\n            b = b.T\n            b[n1, :] = 1.\n\n            # Add the trend for b.\n            drift_pred = np.array(list(self.featurization(self.x2)))\n\n            for i in range(drift_pred.shape[0]):\n                b[n1+1+i, :] = drift_pred[i]\n\n            # Solve.\n            x = np.linalg.solve(A, b)\n\n            u2_mean = np.tensordot(self.u1, x[:n1], axes=1)\n            u2_var = np.sum(x.T * -b.T, axis=1)\n\n            return u2_mean, u2_var\n\n\n        # Ordinary krige.\n        else:\n            # Ax = b.\n\n            # Build A.\n            D1 = cdist(self.x1, self.x1)\n            A = np.zeros((n1+1, n1+1))\n            A[:n1, :n1] = -self.variogram_func(D1, *self.parameter_vals)\n            np.fill_diagonal(A, 0.)\n            A[n1, :] = 1.\n            A[:, n1] = 1.\n            A[n1, n1] = 0.\n\n            # Build b.\n            D2 = cdist(self.x2, self.x1)\n            b = np.zeros((D2.shape[0], D2.shape[1]+1))\n            b[:n2, :n1] = -self.variogram_func(D2, *self.parameter_vals)\n            b = b.T\n            b[n1, :] = 1.\n\n            # Solve.\n            x = np.linalg.solve(A, b)\n\n            u2_mean = np.tensordot(self.u1, x[:n1], axes=1)\n            u2_var = np.sum(x.T * -b.T, axis=1)\n\n            return u2_mean, u2_var\n\n    #########################################################\n    # Access the projected coords of the input data.\n    def get_projected(self):\n        return self.x1[:, 0], self.x1[:, 1]\n\n    #########################################################   \n    # Provide some stats on the model.\n    def stats(self):         \n\n        u2_mean_for_u1, u2_var_for_u1 = self.predict(self.x1_original)\n\n        self.residuals = self.u1 - u2_mean_for_u1     \n\n        self.res_mean = np.mean(self.residuals)\n        self.res_std = np.std(self.residuals)\n        self.res_skew = stats.skew(self.residuals)\n        self.res_kurt = stats.kurtosis(self.residuals)\n\n        if self.show_plots == True:\n\n            plt.figure()\n            plt.hist(self.u1, label='Input values')\n            plt.ylabel('Count')\n            plt.xlabel('Input values')\n            plt.show()\n\n            plt.figure()\n            plt.hist(self.residuals, label='Residuals')\n            plt.ylabel('Count')\n            plt.xlabel('Residuals')\n            plt.show()\n\n        if self.verbose == True:\n            print('Residual mean: {:.3e}'.format(self.res_mean))\n            print('Residual standard deviation: {:.3f}'.format(self.res_std))\n            print('Residual skewness: {:.3f}'.format(self.res_skew))\n            print('Residual kurtosis: {:.3f}'.format(self.res_kurt))\n\n    def get_residuals(self):\n        return self.residuals\n\n    def get_residual_moments(self):\n        return dict(mean=self.res_mean, \n                    std=self.res_std, \n                    skew=self.res_skew, \n                    kurt=self.res_kurt)\n</code></pre>"},{"location":"api/#src.geostat.krige.Krige.__init__","title":"<code>__init__(x1, u1, bins, variogram_func=None, variogram_params=None, cutoff_dist='auto', featurization=None, projection=None, show_plots=True, verbose=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x1</code> <p>n-dim array Locations of input data.</p> required <code>u1</code> <p>1-d array Values to be kriged.</p> required <code>bins</code> <p>int or None The number of bins to use on the variogram cloud. If None, variogram function is fit to the variogram cloud and is not binned first.</p> required <code>variogram_func</code> <p>str Name of the variogram model to use in the kriging.  Should be 'linear', 'gaussian', or 'spherical'. Default is 'gaussian'.</p> <code>None</code> <code>cutoff_dist</code> <p>str or int, optional The maximum lag distance to include in variogram modeling.</p> <code>'auto'</code> <code>featurization</code> <p>function, optional Should be a function that takes x1 (n-dim array of input data)  and returns the coordinates, i.e., x, y, x2, y2. Example below. Default is None.</p> <code>None</code> <code>project</code> <p>function, opt A function that takes multiple vectors, and returns a tuple of projected vectors.</p> required <code>epsg_proj</code> <p>str The projected coordinate system to use. Ignored if project=False. Default is 'EPSG:3310' (California Albers).</p> required <code>show_plots</code> <p>boolean, optional Whether or not to show variogram plots. Default is True.</p> <code>True</code> <code>verbose</code> <p>boolean, optional Whether or not to print parameters. Default is True.</p> <code>True</code> <p>Performs experimental variogram calculation, bins data, and fits variogram model to estimate variogram parameters.       </p> <p>Performs ordinary and universal kriging in up to 3 spatial dimensions.</p> <p>Trend model example: def featurization(x1):       return x1[:, 0], x1[:, 1], x1[:, 0]2, x1[:, 1]2</p> Source code in <code>src/geostat/krige.py</code> <pre><code>def __init__(self, \n             x1, u1, \n             bins,\n             variogram_func=None, \n             variogram_params=None, \n             cutoff_dist='auto',\n             featurization=None,\n             projection=None,\n             show_plots=True, \n             verbose=True,\n             ):\n\n    '''\n    Parameters:\n            x1 : n-dim array\n                Locations of input data.\n\n            u1 : 1-d array\n                Values to be kriged.\n\n            bins : int or None\n                The number of bins to use on the variogram cloud.\n                If None, variogram function is fit to the variogram cloud\n                and is not binned first.\n\n            variogram_func : str\n                 Name of the variogram model to use in the kriging. \n                 Should be 'linear', 'gaussian', or 'spherical'.\n                 Default is 'gaussian'.\n\n            cutoff_dist : str or int, optional\n                The maximum lag distance to include in variogram modeling.\n\n            featurization : function, optional\n                Should be a function that takes x1 (n-dim array of input data) \n                and returns the coordinates, i.e., x, y, x**2, y**2.\n                Example below.\n                Default is None.\n\n            project : function, opt\n                A function that takes multiple vectors, and returns\n                a tuple of projected vectors.\n\n            epsg_proj : str\n                The projected coordinate system to use. Ignored if project=False.\n                Default is 'EPSG:3310' (California Albers).\n\n            show_plots : boolean, optional\n                Whether or not to show variogram plots.\n                Default is True.\n\n            verbose : boolean, optional\n                Whether or not to print parameters.\n                Default is True.\n\n\n    Performs experimental variogram calculation, bins data, and fits variogram model to estimate variogram parameters.       \n\n    Performs ordinary and universal kriging in up to 3 spatial dimensions.\n\n\n    Trend model example:\n    def featurization(x1):    \n        return x1[:, 0], x1[:, 1], x1[:, 0]**2, x1[:, 1]**2\n\n\n    '''\n\n    super().__init__(projection=projection)\n\n    # Save original x1 for stats function.\n    self.x1_original = x1\n\n    # Check and change the shape of u1 if needed (for pdist).\n    if u1.ndim == 1:\n        self.u1 = u1\n    elif u1.shape[1] == 1:\n        self.u1 = u1.reshape(-1)\n    else:\n        raise ValueError(\"Check dimensions of 'u1'.\")\n\n    # Projection.\n    self.x1 = self.project(x1)\n\n    # Variogram.\n    if variogram_func == 'gaussian':\n        self.variogram_func = gaussian\n    elif variogram_func == 'spherical':\n        self.variogram_func = spherical\n    elif variogram_func == 'linear':\n        self.variogram_func = linear\n    else:  \n        raise ValueError(\"Variogram function must be 'linear', 'gaussian', or 'spherical'.\")\n\n\n    if cutoff_dist == 'auto':\n        self.cutoff_dist = cutoff_dist_func(self.x1)\n    else:\n        self.cutoff_dist = cutoff_dist\n\n\n    self.verbose = verbose\n    self.show_plots = show_plots\n    self.bins = bins\n    self.featurization = featurization\n\n\n    # Lags and semivariance.\n    dist = pdist(self.x1, metric='euclidean')\n    gamma = 0.5 * pdist(self.u1.reshape(-1, 1), metric='sqeuclidean')\n\n\n    if self.bins != None:  # Use bins.\n        # Variogram cloud calculation.\n        bin_means, bin_edges, binnumber = binned_statistic(dist, gamma, \n                                                           statistic='mean', \n                                                           bins=self.bins, \n                                                           range=[dist.min(), \n                                                                  self.cutoff_dist])\n        bin_width = (bin_edges[1] - bin_edges[0])\n        bin_centers = bin_edges[1:] - bin_width / 2\n\n        # Bin counts calculation.\n        bin_count, bin_count_edges, bin_count_number = binned_statistic(dist, gamma, \n                                                                        statistic='count', \n                                                                        bins=self.bins)\n        bin_count_width = (bin_count_edges[1] - bin_count_edges[0])\n        bin_count_centers = bin_count_edges[1:] - bin_count_width/2\n\n    if self.show_plots == True:\n        if self.bins == None:\n\n            # Variogram cloud plot.\n            plt.figure(dpi=100)\n            plt.scatter(dist, gamma, ec='C0', fc='none', alpha=0.3)\n            plt.ylabel('$\\gamma(h)$')\n            plt.xlabel('$h$')\n            plt.grid(alpha=0.4)\n            plt.show()\n\n        if self.bins != None:\n\n            # Variogram cloud plot.\n            plt.figure(dpi=100)\n            plt.scatter(dist, gamma, ec='C0', fc='none', alpha=0.3)\n            plt.hlines(bin_means, bin_edges[:-1], bin_edges[1:], zorder=1, color='k')\n            plt.scatter(bin_centers, bin_means, ec='k', lw=0.5)\n            plt.ylabel('$\\gamma(h)$')\n            plt.xlabel('$h$')\n            plt.grid(alpha=0.4)\n            plt.show()\n\n            # Bin counts plot.\n            plt.figure(dpi=100)\n            plt.hlines(bin_count, bin_count_edges[:-1], bin_count_edges[1:], zorder=1, color='k')\n            plt.scatter(bin_count_centers, bin_count, ec='k', lw=0.5)\n            plt.ylabel('Bin count')\n            plt.xlabel('$h$')\n            plt.grid(alpha=0.4)\n            plt.show()\n\n\n\n    ############ Fit the variogram model.\n\n    if not variogram_params:  # Fit to the data.\n\n        if self.bins == None:  # Fit the variogram cloud.\n\n            if self.variogram_func == gaussian or self.variogram_func == spherical:\n\n                # Initial guess for the parameters.\n                p0 = [0.25 * np.max(dist),            # range\n                      np.max(gamma) - np.min(gamma),  # sill\n                      np.min(gamma) + 1e-6]           # nugget\n\n                # Bounds with constraints.\n                bounds = [(1e-6, 1e-6, 1e-6), (np.inf, np.inf, np.inf)]\n\n                # Apply the cutoff.\n                dist_cut = np.where(dist &lt; self.cutoff_dist, dist, np.nan)\n                gamma_cut = np.where(dist &lt; self.cutoff_dist, gamma, np.nan)\n\n                # Remove nans for curve_fit.\n                dist_cut = dist_cut[~np.isnan(dist_cut)]\n                gamma_cut = gamma_cut[~np.isnan(gamma_cut)]\n\n                popt, pcov = curve_fit(self.variogram_func, dist_cut, gamma_cut, p0=p0, bounds=bounds)\n\n                if self.show_plots == True:\n\n                    # Calculate 2d kde to help confirm cutoff.\n                    xi = np.linspace(np.min(dist), np.max(dist), 60)\n                    yi = np.linspace(np.min(gamma), np.max(gamma), 60)\n                    xi, yi = np.meshgrid(xi, yi)\n                    xyi = np.stack([xi.reshape(-1), yi.reshape(-1)], axis=1).T\n\n                    kde = stats.gaussian_kde([dist, gamma])\n                    z = kde.evaluate(xyi)\n                    z = z.reshape(len(xi), len(yi))\n\n                    plt.figure(dpi=100)\n                    plt.pcolormesh(xi, yi, z, cmap=plt.cm.Blues, shading='auto')\n                    plt.ylabel('$\\gamma(h)$')\n                    plt.xlabel('$h$')\n                    plt.grid(alpha=0.4)\n                    plt.show()\n\n                    # With cutoff and variogram model.\n                    xnew = np.linspace(np.min(dist_cut), self.cutoff_dist, 100)\n                    plt.figure(dpi=100)\n                    plt.scatter(dist_cut, gamma_cut, fc='none', ec='C1', lw=0.5, alpha=0.3)\n                    plt.plot(xnew, self.variogram_func(xnew, *popt), color='k')\n                    plt.ylabel('$\\gamma(h)$')\n                    plt.xlabel('$h$')\n                    plt.grid(alpha=0.4)\n                    plt.show()\n\n\n                vrange = popt[0]\n                sill = popt[1]\n                nugget = popt[2]\n\n\n                if self.verbose == True:\n                    print('variogram model: {}'.format(variogram_func))\n                    print('cutoff: {:.2f}'.format(self.cutoff_dist))\n                    print('range: {:.5f}'.format(vrange))\n                    print('sill: {:.5f}'.format(sill))\n                    print('nugget: {:.5f}'.format(nugget))\n                    print('full sill: {:.5f}'.format(sill + nugget))\n\n                self.parameter_vals = [vrange, sill, nugget]\n\n            elif self.variogram_func == linear:\n\n                # Initial guess for the parameters.\n                p0 = [(np.max(dist) - np.min(dist)) / (np.max(gamma) - np.min(gamma)),  # slope\n                      np.min(gamma) + 1e-6]                                             # nugget\n\n                # Bounds with constraints.\n                bounds = [(1e-6, 1e-6), (np.inf, np.inf)]\n\n                # Apply the cutoff.\n                dist_cut = np.where(dist &lt; self.cutoff_dist, dist, np.nan)\n                gamma_cut = np.where(dist &lt; self.cutoff_dist, gamma, np.nan)\n\n                # Remove nans for curve_fit.\n                dist_cut = dist_cut[~np.isnan(dist_cut)]\n                gamma_cut = gamma_cut[~np.isnan(gamma_cut)]\n\n                popt, pcov = curve_fit(self.variogram_func, dist_cut, gamma_cut, p0=p0, bounds=bounds)\n\n                if self.show_plots == True:\n\n                    # Calculate 2d kde to help confirm cutoff.\n                    xi = np.linspace(np.min(dist), np.max(dist), 60)\n                    yi = np.linspace(np.min(gamma), np.max(gamma), 60)\n                    xi, yi = np.meshgrid(xi, yi)\n                    xyi = np.stack([xi.reshape(-1), yi.reshape(-1)], axis=1).T\n\n                    kde = stats.gaussian_kde([dist, gamma])\n                    z = kde.evaluate(xyi)\n                    z = z.reshape(len(xi), len(yi))\n\n                    plt.figure(dpi=100)\n                    plt.pcolormesh(xi, yi, z, cmap=plt.cm.Blues, shading='auto')\n                    plt.ylabel('$\\gamma(h)$')\n                    plt.xlabel('$h$')\n                    plt.grid(alpha=0.4)\n                    plt.show()\n\n                    # With cutoff and variogram model.\n                    xnew = np.linspace(np.min(dist_cut), self.cutoff_dist, 100)\n                    plt.figure(dpi=100)\n                    plt.scatter(dist_cut, gamma_cut, fc='none', ec='C1', lw=0.5, alpha=0.3)\n                    plt.plot(xnew, self.variogram_func(xnew, *popt), color='k')\n                    plt.ylabel('$\\gamma(h)$')\n                    plt.xlabel('$h$')\n                    plt.grid(alpha=0.4)\n                    plt.show()\n\n\n                slope = popt[0]\n                nugget = popt[1]\n\n                if self.verbose == True:\n                    print('variogram model: {}'.format(variogram_func))\n                    print('cutoff: {:.2f}'.format(self.cutoff_dist))\n                    print('slope: {:.5f}'.format(slope))\n                    print('nugget: {:.5f}'.format(nugget))\n\n\n                self.parameter_vals = [slope, nugget]\n\n\n\n\n        elif self.bins != None:  # Fit the binned data.\n            if self.variogram_func == gaussian or self.variogram_func == spherical:\n\n                # Initial guess for the parameters.\n                p0 = [0.25 * np.max(bin_centers),                 # range\n                          np.max(bin_means) - np.min(bin_means),  # sill\n                          np.min(bin_means)]                      # nugget\n\n                # Bounds with constraints.\n                bounds = [(1e-6, 1e-6, 1e-6), (np.inf, np.inf, np.inf)]\n\n                popt, pcov = curve_fit(self.variogram_func, bin_centers, bin_means, p0=p0, bounds=bounds)\n\n                if self.show_plots == True:\n\n                    # Fit variogram plot.\n                    plt.figure(dpi=100)\n                    plt.scatter(bin_centers, bin_means, c='C1', ec='k', lw=0.5)\n                    plt.plot(bin_centers, self.variogram_func(bin_centers, *popt), color='k')\n                    plt.ylabel('$\\gamma(h)$')\n                    plt.xlabel('$h$')\n                    plt.grid(alpha=0.4)\n                    plt.show()\n\n                vrange = popt[0]\n                sill = popt[1]\n                nugget = popt[2]\n\n\n                if self.verbose == True:\n                    print('variogram model: {}'.format(variogram_func))\n                    print('cutoff: {:.2f}'.format(self.cutoff_dist))\n                    print('range: {:.5f}'.format(vrange))\n                    print('sill: {:.5f}'.format(sill))\n                    print('nugget: {:.5f}'.format(nugget))\n                    print('full sill: {:.5f}'.format(sill + nugget))\n\n                self.parameter_vals = [vrange, sill, nugget]\n\n            elif self.variogram_func == linear:\n\n                # Initial guess for the parameters.\n                p0 = [(np.max(bin_centers) - np.min(bin_centers)) / (np.max(bin_means) - np.min(bin_means)),  # slope\n                      np.min(bin_means)]                                                                      # nugget\n\n                # Bounds with constraints.\n                bounds = [(1e-6, 1e-6), (np.inf, np.inf)]\n\n                popt, pcov = curve_fit(self.variogram_func, bin_centers, bin_means, p0=p0, bounds=bounds)\n\n                if self.show_plots == True:\n\n                    # Fit variogram plot.\n                    plt.figure(dpi=100)\n                    plt.scatter(bin_centers, bin_means, c='C1', ec='k', lw=0.5)\n                    plt.plot(bin_centers, self.variogram_func(bin_centers, *popt), color='k')\n                    plt.ylabel('$\\gamma(h)$')\n                    plt.xlabel('$h$')\n                    plt.grid(alpha=0.4)\n                    plt.show()\n\n                slope = popt[0]\n                nugget = popt[1]\n\n                if self.verbose == True:\n                    print('variogram model: {}'.format(variogram_func))\n                    print('cutoff: {:.2f}'.format(self.cutoff_dist))\n                    print('slope: {:.5f}'.format(slope))\n                    print('nugget: {:.5f}'.format(nugget))\n\n\n                self.parameter_vals = [slope, nugget]\n\n\n    else:  # Use the given variogram parameters.\n        self.parameter_vals = variogram_params\n\n        # Apply the cutoff.\n        dist_cut = np.where(dist &lt; self.cutoff_dist, dist, np.nan)\n        gamma_cut = np.where(dist &lt; self.cutoff_dist, gamma, np.nan)\n\n        # Remove nans for curve_fit.\n        dist_cut = dist_cut[~np.isnan(dist_cut)]\n        gamma_cut = gamma_cut[~np.isnan(gamma_cut)]\n\n\n        if self.show_plots == True:\n\n            if self.bins == None:\n\n                # Calculate 2d kde to help confirm cutoff.\n                xi = np.linspace(np.min(dist), np.max(dist), 60)\n                yi = np.linspace(np.min(gamma), np.max(gamma), 60)\n                xi, yi = np.meshgrid(xi, yi)\n                xyi = np.stack([xi.reshape(-1), yi.reshape(-1)], axis=1).T\n\n                kde = stats.gaussian_kde([dist, gamma])\n                z = kde.evaluate(xyi)\n                z = z.reshape(len(xi), len(yi))\n\n                plt.figure(dpi=100)\n                plt.pcolormesh(xi, yi, z, cmap=plt.cm.Blues, shading='auto')\n                plt.ylabel('$\\gamma(h)$')\n                plt.xlabel('$h$')\n                plt.grid(alpha=0.4)\n                plt.show()\n\n                # With cutoff and variogram model.\n                xnew = np.linspace(np.min(dist_cut), self.cutoff_dist, 100)\n                plt.figure(dpi=100)\n                plt.scatter(dist_cut, gamma_cut, fc='none', ec='C1', lw=0.5, alpha=0.3)\n                plt.plot(xnew, self.variogram_func(xnew, *self.parameter_vals), color='k')\n                plt.ylabel('$\\gamma(h)$')\n                plt.xlabel('$h$')\n                plt.grid(alpha=0.4)\n                plt.show()\n\n            else:\n\n                # Fit variogram plot.\n                plt.figure(dpi=100)\n                plt.scatter(bin_centers, bin_means, fc='C1', ec='k', lw=0.5)\n                plt.plot(bin_centers, self.variogram_func(bin_centers, *self.parameter_vals), color='k')\n                plt.ylabel('$\\gamma(h)$')\n                plt.xlabel('$h$')\n                plt.grid(alpha=0.4)\n                plt.show()\n\n        if self.variogram_func == gaussian or self.variogram_func == spherical:\n            vrange = self.parameter_vals[0]\n            sill = self.parameter_vals[1]\n            nugget = self.parameter_vals[2]\n\n            if self.verbose == True:\n                print('variogram model: {}'.format(variogram_func))\n                print('cutoff: {:.2f}'.format(self.cutoff_dist))\n                print('range: {:.5f}'.format(vrange))\n                print('sill: {:.5f}'.format(sill))\n                print('nugget: {:.5f}'.format(nugget))\n                print('full sill: {:.5f}'.format(sill + nugget))\n\n        elif self.variogram_func == linear:\n            slope = self.parameter_vals[0]\n            nugget = self.parameter_vals[1]\n\n            if self.verbose == True:\n                print('variogram model: {}'.format(variogram_func))\n                print('cutoff: {:.2f}'.format(self.cutoff_dist))\n                print('slope: {:.5f}'.format(slope))\n                print('nugget: {:.5f}'.format(nugget))\n</code></pre>"},{"location":"api/#src.geostat.krige.Krige.predict","title":"<code>predict(x2_pred)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x2</code> <p>n-dim array Locations to make kriging predictions.</p> required <p>Returns:</p> Name Type Description <code>u2_mean</code> <p>float Kriging mean.</p> <code>u2_var</code> <p>float Kriging variance.</p> <p>Performs ordinary or universal kriging using the estimated variogram parameters.</p> Source code in <code>src/geostat/krige.py</code> <pre><code>def predict(self, x2_pred):\n\n    '''\n    Parameters:\n            x2 : n-dim array\n                Locations to make kriging predictions.\n\n    Returns:\n            u2_mean : float\n                Kriging mean.\n\n            u2_var : float\n                Kriging variance.\n\n\n    Performs ordinary or universal kriging using the estimated variogram parameters.\n\n    '''\n\n    self.x2 = self.project(x2_pred)\n\n    n1 = len(self.x1)\n    n2 = len(self.x2)\n\n    # Universal krige.\n    if self.featurization:            \n        # Ax = b with a trend.\n\n        # Build A\n        D1 = cdist(self.x1, self.x1)\n\n        drift_data = np.array(list(self.featurization(self.x1)))\n\n        An = n1 + 1 + drift_data.shape[0]\n\n        A = np.zeros((An, An))\n        A[:n1, :n1] = -self.variogram_func(D1, *self.parameter_vals)\n        np.fill_diagonal(A, 0.)\n        A[n1, :n1] = 1.\n        A[:n1, n1] = 1.\n        A[n1, n1] = 0.\n\n        # Add in the trend for A.\n        for i in range(drift_data.shape[0]):\n            A[n1+i+1, :n1] = drift_data[i]\n            A[:n1, n1+i+1] = drift_data[i]\n\n        # Build b.\n        D2 = cdist(self.x2, self.x1)\n        b = np.zeros((D2.shape[0], D2.shape[1] + 1 + drift_data.shape[0]))\n        b[:n2, :n1] = -self.variogram_func(D2, *self.parameter_vals)\n        b = b.T\n        b[n1, :] = 1.\n\n        # Add the trend for b.\n        drift_pred = np.array(list(self.featurization(self.x2)))\n\n        for i in range(drift_pred.shape[0]):\n            b[n1+1+i, :] = drift_pred[i]\n\n        # Solve.\n        x = np.linalg.solve(A, b)\n\n        u2_mean = np.tensordot(self.u1, x[:n1], axes=1)\n        u2_var = np.sum(x.T * -b.T, axis=1)\n\n        return u2_mean, u2_var\n\n\n    # Ordinary krige.\n    else:\n        # Ax = b.\n\n        # Build A.\n        D1 = cdist(self.x1, self.x1)\n        A = np.zeros((n1+1, n1+1))\n        A[:n1, :n1] = -self.variogram_func(D1, *self.parameter_vals)\n        np.fill_diagonal(A, 0.)\n        A[n1, :] = 1.\n        A[:, n1] = 1.\n        A[n1, n1] = 0.\n\n        # Build b.\n        D2 = cdist(self.x2, self.x1)\n        b = np.zeros((D2.shape[0], D2.shape[1]+1))\n        b[:n2, :n1] = -self.variogram_func(D2, *self.parameter_vals)\n        b = b.T\n        b[n1, :] = 1.\n\n        # Solve.\n        x = np.linalg.solve(A, b)\n\n        u2_mean = np.tensordot(self.u1, x[:n1], axes=1)\n        u2_var = np.sum(x.T * -b.T, axis=1)\n\n        return u2_mean, u2_var\n</code></pre>"},{"location":"api/#src.geostat.krige.cutoff_dist_func","title":"<code>cutoff_dist_func(x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <p>n-dim array Locations of input data.</p> required <p>Returns:</p> Name Type Description <code>cutoff</code> <p>float The maximum lag distance to use in fitting the variogram. Found using Pythagorean Theorem to roughly find one half the distance across the study area.</p> Source code in <code>src/geostat/krige.py</code> <pre><code>def cutoff_dist_func(x):\n\n    '''\n        Parameters:\n                x : n-dim array\n                    Locations of input data.\n\n        Returns:\n                cutoff : float\n                    The maximum lag distance to use in fitting the variogram.\n                    Found using Pythagorean Theorem to roughly find one half the distance across the study area.\n\n    '''\n\n    a2 = np.square(x[:, 0].max() - x[:, 0].min())\n    b2 = np.square(x[:, 1].max() - x[:, 1].min())\n\n    cutoff = np.sqrt(a2 + b2) / 2\n\n    return cutoff\n</code></pre>"},{"location":"api/#src.geostat.mean","title":"<code>src.geostat.mean</code>","text":""},{"location":"api/#src.geostat.mean.Mean","title":"<code>Mean</code>","text":"<p>               Bases: <code>Op</code></p> Source code in <code>src/geostat/mean.py</code> <pre><code>class Mean(Op):\n    def __init__(self, fa, autoinputs):\n        if 'locs1' not in autoinputs: autoinputs['locs1'] = 'locs1'\n        super().__init__(fa, autoinputs)\n\n    def __add__(self, other):\n        if isinstance(other, ZeroTrend):\n            return self\n        elif isinstance(self, ZeroTrend):\n            return other\n        else:\n            return Stack([self]) + other\n\n    def call(self, e):\n        pass\n\n    def __call__(self, e):\n        \"\"\"\n        Returns tuple `(mean, covariance)` for locations.\n        Return values have correct shapes.\n        \"\"\"\n        M = self.call(e)\n        if M is None: M = 0.\n        n1 = tf.shape(e['locs1'])[0]\n        M = tf.broadcast_to(M, [n1])\n        return M\n</code></pre>"},{"location":"api/#src.geostat.mean.Mean.__call__","title":"<code>__call__(e)</code>","text":"<p>Returns tuple <code>(mean, covariance)</code> for locations. Return values have correct shapes.</p> Source code in <code>src/geostat/mean.py</code> <pre><code>def __call__(self, e):\n    \"\"\"\n    Returns tuple `(mean, covariance)` for locations.\n    Return values have correct shapes.\n    \"\"\"\n    M = self.call(e)\n    if M is None: M = 0.\n    n1 = tf.shape(e['locs1'])[0]\n    M = tf.broadcast_to(M, [n1])\n    return M\n</code></pre>"},{"location":"api/#src.geostat.mesh","title":"<code>src.geostat.mesh</code>","text":""},{"location":"api/#src.geostat.metric","title":"<code>src.geostat.metric</code>","text":""},{"location":"api/#src.geostat.model","title":"<code>src.geostat.model</code>","text":""},{"location":"api/#src.geostat.model.Featurizer","title":"<code>Featurizer</code>","text":"<p>Featurizer class for producing feature matrices (F matrix) from location data.</p> <p>The <code>Featurizer</code> applies a specified featurization function to the input location data  and generates the corresponding feature matrix. If no featurization function is provided,  it produces a matrix with appropriate dimensions containing only ones.</p>"},{"location":"api/#src.geostat.model.Featurizer--parameters","title":"Parameters","text":"<ul> <li>featurization : Callable or None     A function that takes in the individual components of location data and returns the features.     If set to <code>None</code>, the featurizer will produce an empty feature matrix (i.e., only ones).</li> </ul>"},{"location":"api/#src.geostat.model.Featurizer--examples","title":"Examples","text":"<p>Creating a <code>Featurizer</code> using a custom featurization function:</p> <pre><code>import tensorflow as tf\nfrom geostat.model import Featurizer\n\n# Define a custom featurization function\ndef simple_featurizer(x, y):\n    return x, y, x * y\n\n# Initialize the Featurizer\nfeaturizer = Featurizer(simple_featurizer)\n</code></pre> <p>Using the <code>Featurizer</code> to transform location data:</p> <pre><code>locs = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\nF_matrix = featurizer(locs)\n# F_matrix will contain the features: (x, y, x*y) for each location\n</code></pre> <p>Handling the case where no featurization is provided:</p> <pre><code>featurizer_no_feat = Featurizer(None)\nF_matrix = featurizer_no_feat(locs)\n# Since no featurization function is provided, F_matrix will have shape (3, 0)\n</code></pre>"},{"location":"api/#src.geostat.model.Featurizer--notes","title":"Notes","text":"<ul> <li>The <code>__call__</code> method is used to apply the featurization to input location data.</li> <li>If <code>featurization</code> returns a tuple, it is assumed to represent multiple features,    which will be stacked to form the feature matrix.</li> </ul> Source code in <code>src/geostat/model.py</code> <pre><code>class Featurizer:\n    \"\"\"\n    Featurizer class for producing feature matrices (F matrix) from location data.\n\n    The `Featurizer` applies a specified featurization function to the input location data \n    and generates the corresponding feature matrix. If no featurization function is provided, \n    it produces a matrix with appropriate dimensions containing only ones.\n\n    Parameters\n    ----------\n    * featurization : Callable or None\n        A function that takes in the individual components of location data and returns the features.\n        If set to `None`, the featurizer will produce an empty feature matrix (i.e., only ones).\n\n    Examples\n    --------\n    Creating a `Featurizer` using a custom featurization function:\n\n    ```\n    import tensorflow as tf\n    from geostat.model import Featurizer\n\n    # Define a custom featurization function\n    def simple_featurizer(x, y):\n        return x, y, x * y\n\n    # Initialize the Featurizer\n    featurizer = Featurizer(simple_featurizer)\n    ```\n\n    Using the `Featurizer` to transform location data:\n\n    ```\n    locs = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n    F_matrix = featurizer(locs)\n    # F_matrix will contain the features: (x, y, x*y) for each location\n    ```\n\n    Handling the case where no featurization is provided:\n\n    ```\n    featurizer_no_feat = Featurizer(None)\n    F_matrix = featurizer_no_feat(locs)\n    # Since no featurization function is provided, F_matrix will have shape (3, 0)\n    ```\n\n    Notes\n    -----\n    - The `__call__` method is used to apply the featurization to input location data.\n    - If `featurization` returns a tuple, it is assumed to represent multiple features, \n      which will be stacked to form the feature matrix.\n    \"\"\"\n\n    def __init__(self, featurization):\n        self.featurization = featurization\n\n    def __call__(self, locs):\n        locs = tf.cast(locs, tf.float32)\n        if self.featurization is None: # No features.\n            return tf.ones([tf.shape(locs)[0], 0], dtype=tf.float32)\n\n        feats = self.featurization(*tf.unstack(locs, axis=1))\n        if isinstance(feats, tuple): # One or many features.\n            if len(feats) == 0:\n                return tf.ones([tf.shape(locs)[0], 0], dtype=tf.float32)\n            else:\n                feats = self.featurization(*tf.unstack(locs, axis=1))\n                feats = [tf.broadcast_to(tf.cast(f, tf.float32), [tf.shape(locs)[0]]) for f in feats]\n                return tf.stack(feats, axis=1)\n        else: # One feature.\n            return e(feats)\n</code></pre>"},{"location":"api/#src.geostat.model.GP","title":"<code>GP</code>  <code>dataclass</code>","text":"<p>Gaussian Process (GP) model class with a mean function and a kernel.</p> <p>This class represents a Gaussian Process with specified mean and kernel functions. If no mean is provided, a zero mean is used by default. The kernel must always be specified. The class supports addition to combine two GP models, and it allows gathering variables  from the mean and kernel.</p>"},{"location":"api/#src.geostat.model.GP--parameters","title":"Parameters","text":"<ul> <li>mean : mn.Trend, optional     The mean function of the Gaussian Process. If not provided or set to 0,      a ZeroTrend is used as the default mean.</li> <li>kernel : krn.Kernel     The kernel function of the Gaussian Process. This parameter is required.</li> </ul>"},{"location":"api/#src.geostat.model.GP--examples","title":"Examples","text":"<p>Creating a simple Gaussian Process with default mean and a Noise kernel:</p> <pre><code>import geostat.mean as mn\nimport geostat.kernel as krn\nfrom geostat import Parameters, GP\np = Parameters(nugget=1., sill=1., beta=[4., 3., 2., 1.])\nkernel = krn.Noise(p.nugget)\ngp = GP(kernel=kernel)\n</code></pre> <p>The mean defaults to ZeroTrend if not provided.</p> <pre><code>gp.mean\n</code></pre> <p> <p>Specifying both mean and kernel:</p> <pre><code>@geostat.featurizer()\ndef trend_featurizer(x, y): return 1., x, y, x*y\nmean_function = mn.Trend(trend_featurizer, beta=p.beta)\ngp = GP(mean=mean_function, kernel=kernel)\n</code></pre> <p>Adding two GP objects:</p> <pre><code>gp1 = GP(kernel=krn.Noise(p.nugget))\ngp2 = GP(mean=mean_function, kernel=krn.Delta(p.sill))\ncombined_gp = gp1 + gp2\ncombined_gp.mean  # &lt;ZeroTrend + Trend object&gt;\ncombined_gp.kernel  # &lt;Noise + Delta object&gt;\n</code></pre>"},{"location":"api/#src.geostat.model.GP--notes","title":"Notes","text":"<ul> <li>The <code>__tf_tracing_type__</code> method is used for TensorFlow tracing purposes and typically not called directly.</li> <li>Ensure that the kernel is always provided upon initialization.</li> </ul> Source code in <code>src/geostat/model.py</code> <pre><code>@dataclass\nclass GP:\n    \"\"\"\n    Gaussian Process (GP) model class with a mean function and a kernel.\n\n    This class represents a Gaussian Process with specified mean and kernel functions.\n    If no mean is provided, a zero mean is used by default. The kernel must always be specified.\n    The class supports addition to combine two GP models, and it allows gathering variables \n    from the mean and kernel.\n\n    Parameters\n    ----------\n    * mean : mn.Trend, optional\n        The mean function of the Gaussian Process. If not provided or set to 0, \n        a ZeroTrend is used as the default mean.\n    * kernel : krn.Kernel\n        The kernel function of the Gaussian Process. This parameter is required.\n\n    Examples\n    --------\n    Creating a simple Gaussian Process with default mean and a Noise kernel:\n\n    ```\n    import geostat.mean as mn\n    import geostat.kernel as krn\n    from geostat import Parameters, GP\n    p = Parameters(nugget=1., sill=1., beta=[4., 3., 2., 1.])\n    kernel = krn.Noise(p.nugget)\n    gp = GP(kernel=kernel)\n    ```\n\n    The mean defaults to ZeroTrend if not provided.\n\n    ```\n    gp.mean\n    ```\n    &lt;ZeroTrend object&gt;\n\n    Specifying both mean and kernel:\n\n    ```\n    @geostat.featurizer()\n    def trend_featurizer(x, y): return 1., x, y, x*y\n    mean_function = mn.Trend(trend_featurizer, beta=p.beta)\n    gp = GP(mean=mean_function, kernel=kernel)\n    ```\n\n    Adding two GP objects:\n\n    ```\n    gp1 = GP(kernel=krn.Noise(p.nugget))\n    gp2 = GP(mean=mean_function, kernel=krn.Delta(p.sill))\n    combined_gp = gp1 + gp2\n    combined_gp.mean  # &lt;ZeroTrend + Trend object&gt;\n    combined_gp.kernel  # &lt;Noise + Delta object&gt;\n    ```\n\n    Notes\n    -----\n    - The `__tf_tracing_type__` method is used for TensorFlow tracing purposes and typically not called directly.\n    - Ensure that the kernel is always provided upon initialization.\n    \"\"\"\n\n    mean: mn.Trend = None\n    kernel: krn.Kernel = None\n\n    def __post_init__(self):\n        if self.mean is None or self.mean == 0:\n            self.mean = mn.ZeroTrend()\n        assert self.kernel is not None\n\n    def __add__(self, other):\n        return GP(self.mean + other.mean, self.kernel + other.kernel)\n\n    def __tf_tracing_type__(self, context):\n        return SingletonTraceType(self)\n\n    def gather_vars(self):\n        return self.mean.gather_vars() | self.kernel.gather_vars()\n</code></pre>"},{"location":"api/#src.geostat.model.Model","title":"<code>Model</code>  <code>dataclass</code>","text":"<p>Model class for performing Gaussian Process (GP) training and prediction with optional warping.</p> <p>The <code>Model</code> class integrates a GP model with optional data warping, and supports data generation on given location, training on given location and observation data, and prediction on given location.</p>"},{"location":"api/#src.geostat.model.Model--parameters","title":"Parameters","text":"<ul> <li>gp : GP     The Gaussian Process model to be used for training and prediction.</li> <li>warp : Warp, optional     An optional warping transformation applied to the data. If not specified, <code>NoWarp</code>      is used by default.</li> <li>parameter_sample_size : int, optional     The number of parameter samples to draw. Default is None.</li> <li>locs : np.ndarray, optional     A NumPy array containing location data.</li> <li>vals : np.ndarray, optional     A NumPy array containing observed values corresponding to <code>locs</code>.</li> <li>cats : np.ndarray, optional     A NumPy array containing categorical data.</li> <li>report : Callable, optional     A custom reporting function to display model parameters. If not provided, a default      reporting function is used.</li> <li>verbose : bool, default=True     Whether to print model parameters and status updates.</li> </ul>"},{"location":"api/#src.geostat.model.Model--examples","title":"Examples","text":"<p>Initializing a <code>Model</code> with a Gaussian Process:</p> <pre><code>from geostat import GP, Model\nfrom geostat.kernel import Noise\nimport numpy as np\n\ngp = GP(kernel=Noise(1.0))\nlocs = np.array([[0.0, 1.0], [1.0, 2.0]])\nvals = np.array([1.0, 2.0])\nmodel = Model(gp=gp, locs=locs, vals=vals)\n</code></pre>"},{"location":"api/#src.geostat.model.Model--notes","title":"Notes","text":"<ul> <li>The <code>__post_init__</code> method sets up default values, initializes the warping if not provided,    and sets up reporting and data preprocessing.</li> </ul> Source code in <code>src/geostat/model.py</code> <pre><code>@dataclass\nclass Model():\n    \"\"\"\n    Model class for performing Gaussian Process (GP) training and prediction with optional warping.\n\n    The `Model` class integrates a GP model with optional data warping, and supports data generation on given location,\n    training on given location and observation data, and prediction on given location.\n\n    Parameters\n    ----------\n    * gp : GP\n        The Gaussian Process model to be used for training and prediction.\n    * warp : Warp, optional\n        An optional warping transformation applied to the data. If not specified, `NoWarp` \n        is used by default.\n    * parameter_sample_size : int, optional\n        The number of parameter samples to draw. Default is None.\n    * locs : np.ndarray, optional\n        A NumPy array containing location data.\n    * vals : np.ndarray, optional\n        A NumPy array containing observed values corresponding to `locs`.\n    * cats : np.ndarray, optional\n        A NumPy array containing categorical data.\n    * report : Callable, optional\n        A custom reporting function to display model parameters. If not provided, a default \n        reporting function is used.\n    * verbose : bool, default=True\n        Whether to print model parameters and status updates.\n\n    Examples\n    --------\n    Initializing a `Model` with a Gaussian Process:\n\n    ```\n    from geostat import GP, Model\n    from geostat.kernel import Noise\n    import numpy as np\n\n    gp = GP(kernel=Noise(1.0))\n    locs = np.array([[0.0, 1.0], [1.0, 2.0]])\n    vals = np.array([1.0, 2.0])\n    model = Model(gp=gp, locs=locs, vals=vals)\n    ```\n\n    Notes\n    -----\n    - The `__post_init__` method sets up default values, initializes the warping if not provided, \n      and sets up reporting and data preprocessing.\n    \"\"\"\n\n    gp: GP\n    warp: Warp = None\n    parameter_sample_size: Optional[int] = None\n    locs: np.ndarray = None\n    vals: np.ndarray = None\n    cats: np.ndarray = None\n    report: Callable = None\n    verbose: bool = True\n\n    def __post_init__(self):\n\n        # '''\n        # Parameters:\n        #         x : Pandas DataFrame with columns for locations.\n\n        #         u : A Pandas Series containing observations.\n\n        #         featurization : function, optional\n        #             Should be a function that takes x1 (n-dim array of input data)\n        #             and returns the coordinates, i.e., x, y, x**2, y**2.\n        #             Example: def featurization(x1):\n        #                         return x1[:, 0], x1[:, 1], x1[:, 0]**2, x1[:, 1]**2.\n        #             Default is None.\n\n        #         latent : List[GP]\n        #              Name of the covariance function to use in the GP.\n        #              Should be 'squared-exp' or 'gamma-exp'.\n        #              Default is 'squared-exp'.\n\n        #         verbose : boolean, optional\n        #             Whether or not to print parameters.\n        #             Default is True.\n\n        # Performs Gaussian process training and prediction.\n        # '''\n\n        if self.warp is None: self.warp = NoWarp()\n\n        # Default reporting function.\n        def default_report(p, prefix=None):\n            if prefix: print(prefix, end=' ')\n\n            def fmt(x):\n                if isinstance(x, tf.Tensor):\n                    x = x.numpy()\n\n                if isinstance(x, (int, np.int32, np.int64)):\n                    return '{:5d}'.format(x)\n                if isinstance(x, (float, np.float32, np.float64)):\n                    return '{:5.2f}'.format(x)\n                else:\n                    with np.printoptions(precision=2, formatter={'floatkind': '{:5.2f}'.format}):\n                        return str(x)\n\n            print('[%s]' % (' '.join('%s %s' % (k, fmt(v)) for k, v in p.items())))\n\n        if self.report == None: self.report = default_report\n\n        if self.locs is not None: self.locs = np.array(self.locs)\n        if self.vals is not None: self.vals = np.array(self.vals)\n        if self.cats is not None: self.cats = np.array(self.cats)\n\n        # Collect parameters and create TF parameters.\n        for p in self.gather_vars().values():\n            p.create_tf_variable()\n\n    def gather_vars(self):\n        return self.gp.gather_vars() | self.warp.gather_vars()\n\n    def set(self, **values):\n        \"\"\"\n        Sets the values of the model's parameters based on the provided keyword arguments.\n        Each parameter specified must exist in the model; otherwise, a `ValueError` is raised.\n\n        Parameters\n        ----------\n        * values : keyword arguments\n            A dictionary of parameter names and their corresponding values that should be \n            set in the model. Each key corresponds to a parameter name, and the value is \n            the value to be assigned to that parameter.\n\n        Returns\n        -------\n        * self : Model\n            The model instance with updated parameter values, allowing for method chaining.\n\n        Raises\n        ------\n        * ValueError\n            If a provided parameter name does not exist in the model's parameters.\n\n        Examples\n        --------\n        Update parameter value using `set`:\n\n        ```\n        from geostat import GP, Model\n        from geostat.kernel import Noise\n\n        # Create model\n        kernel = Noise(nugget=1.0)\n        model = Model(GP(0, kernel))\n\n        # Update parameters\n        model.set(nugget=0.5)\n        ```\n\n        Notes\n        -----\n        - The `set` method retrieves the current parameters using `gather_vars` and updates \n        their values. The associated TensorFlow variables are also recreated.\n        - This method is useful for dynamically updating the model's parameters after initialization.\n        \"\"\"\n\n        parameters = self.gather_vars()\n        for name, v in values.items():\n            if name in parameters:\n                parameters[name].value = v\n                parameters[name].create_tf_variable()\n            else:\n                raise ValueError(f\"{k} is not a parameter\")\n        return self\n\n    def fit(self, locs, vals, cats=None, step_size=0.01, iters=100, reg=None):\n        \"\"\"\n        Trains the model using the provided location and value data by optimizing the parameters of the Gaussian Process (GP)\n        using the Adam optimizer. Optionally performs regularization and can handle categorical data.\n\n        Parameters\n        ----------\n        * locs : np.ndarray\n            A NumPy array containing the input locations for training.\n        * vals : np.ndarray\n            A NumPy array containing observed values corresponding to the `locs`.\n        * cats : np.ndarray, optional\n            A NumPy array containing categorical data for each observation in `locs`. If provided,\n            the data is sorted according to `cats` to enable stratified training. Defaults to None.\n        * step_size : float, default=0.01\n            The learning rate for the Adam optimizer.\n        * iters : int, default=100\n            The total number of iterations to run for training.\n        * reg : float or None, optional\n            Regularization penalty parameter. If None, no regularization is applied.\n\n        Returns\n        -------\n        * self : Model\n            The model instance with updated parameters, allowing for method chaining.\n\n        Examples\n        --------\n        Fitting a model using training data:\n\n        ```\n        from geostat import GP, Model\n        from geostat.kernel import Noise\n\n        # Create model\n        kernel = Noise(nugget=1.0)\n        model = Model(GP(0, kernel))\n\n        # Fit model\n        locs = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])\n        vals = np.array([10.0, 15.0, 20.0])\n        model.fit(locs, vals, step_size=0.05, iters=500)\n        ```\n\n        Using categorical data for training:\n\n        ```\n        cats = np.array([1, 1, 2])\n        model.fit(locs, vals, cats=cats, step_size=0.01, iters=300)\n        ```\n\n        Notes\n        -----\n        - The `fit` method uses the Adam optimizer to minimize the negative log-likelihood (`ll`) and any regularization \n        penalties specified by `reg`.\n        - During training, if `cats` are provided, data points are sorted according to `cats` to ensure grouped training.\n        - The `verbose` flag determines whether training progress is printed after each iteration.\n        - After training, parameter values are saved and can be accessed or updated using the model's attributes.\n        \"\"\"\n\n        # Collect parameters and create TF parameters.\n        parameters = self.gather_vars()\n\n        # Permute datapoints if cats is given.\n        if cats is not None:\n            cats = np.array(cats)\n            perm = np.argsort(cats)\n            locs, vals, cats = locs[perm], vals[perm], cats[perm]\n        else:\n            cats = np.zeros(locs.shape[:1], np.int32)\n            perm = None\n\n        # Data dict.\n        self.data = {\n            'warplocs': self.warp(locs),\n            'vals': tf.constant(vals, dtype=tf.float32),\n            'cats': tf.constant(cats, dtype=tf.int32)}\n\n        optimizer = tf.keras.optimizers.Adam(learning_rate=step_size)\n\n        j = 0 # Iteration count.\n        for i in range(10):\n            t0 = time.time()\n            while j &lt; (i + 1) * iters / 10:\n                ll, reg_penalty = gp_train_step(\n                    optimizer, self.data, parameters, self.gp, reg)\n                j += 1\n\n            time_elapsed = time.time() - t0\n            if self.verbose == True:\n                self.report(\n                  dict(iter=j, ll=ll, time=time_elapsed, reg=reg_penalty) |\n                  {p.name: p.surface() for p in parameters.values()})\n\n        # Save parameter values.\n        for p in parameters.values():\n            p.update_value()\n\n        # Restore order if things were permuted.\n        if perm is not None:\n            revperm = np.argsort(perm)\n            locs, vals, cats = locs[revperm], vals[revperm], cats[revperm]\n\n        self.locs = locs\n        self.vals = vals\n        self.cats = cats\n\n        return self\n\n    def mcmc(self, locs, vals, cats=None,\n            chains=4, step_size=0.1, move_prob=0.5,\n            samples=1000, burnin=500, report_interval=100):\n\n        assert samples % report_interval == 0, '`samples` must be a multiple of `report_interval`'\n        assert burnin % report_interval == 0, '`burnin` must be a multiple of `report_interval`'\n\n        # Permute datapoints if cats is given.\n        if cats is not None:\n            cats = np.array(cats)\n            perm = np.argsort(cats)\n            locs, vals, cats = locs[perm], vals[perm], cats[perm]\n\n        # Data dict.\n        self.data = {\n            'locs': tf.constant(locs, dtype=tf.float32),\n            'vals': tf.constant(vals, dtype=tf.float32),\n            'cats': None if cats is None else tf.constant(cats, dtype=tf.int32)}\n\n        # Initial MCMC state.\n        initial_up = self.parameter_space.get_underlying(self.parameters)\n\n        # Unnormalized log posterior distribution.\n        def g(up):\n            sp = self.parameter_space.get_surface(up)\n            return gp_log_likelihood(self.data, sp, self.gp)\n\n        def f(*up_flat):\n            up = tf.nest.pack_sequence_as(initial_up, up_flat)\n            ll = tf.map_fn(g, up, fn_output_signature=tf.float32)\n            # log_prior = -tf.reduce_sum(tf.math.log(1. + tf.square(up_flat)), axis=0)\n            return ll # + log_prior\n\n        # Run the chain for a burst.\n        @tf.function\n        def run_chain(current_state, final_results, kernel, iters):\n            samples, results, final_results = tfp.mcmc.sample_chain(\n                num_results=iters,\n                current_state=current_state,\n                kernel=kernel,\n                return_final_kernel_results=True,\n                trace_fn=lambda _, results: results)\n\n            return samples, results, final_results\n\n        def new_state_fn(scale, dtype):\n          direction_dist = tfd.Normal(loc=dtype(0), scale=dtype(1))\n          scale_dist = tfd.Exponential(rate=dtype(1/scale))\n          pick_dist = tfd.Bernoulli(probs=move_prob)\n\n          def _fn(state_parts, seed):\n            next_state_parts = []\n            part_seeds = tfp.random.split_seed(\n                seed, n=len(state_parts), salt='rwmcauchy')\n            for sp, ps in zip(state_parts, part_seeds):\n                pick = tf.cast(pick_dist.sample(sample_shape=sp.shape, seed=ps), tf.float32)\n                direction = direction_dist.sample(sample_shape=sp.shape, seed=ps)\n                scale_val = scale_dist.sample(seed=ps)\n                next_state_parts.append(sp + tf.einsum('a...,a-&gt;a...', pick * direction, scale_val))\n            return next_state_parts\n          return _fn\n\n        inv_temps = 0.5**np.arange(chains, dtype=np.float32)\n\n        def make_kernel_fn(target_log_prob_fn):\n            return tfp.mcmc.RandomWalkMetropolis(\n                target_log_prob_fn=target_log_prob_fn,\n                new_state_fn=new_state_fn(scale=step_size / np.sqrt(inv_temps), dtype=np.float32))\n\n        kernel = tfp.mcmc.ReplicaExchangeMC(\n            target_log_prob_fn=f,\n            inverse_temperatures=inv_temps,\n            make_kernel_fn=make_kernel_fn)\n\n        # Do bursts.\n        current_state = tf.nest.flatten(initial_up)\n        final_results = None\n        acc_states = []\n        num_bursts = (samples + burnin) // report_interval\n        burnin_bursts = burnin // report_interval\n        for i in range(num_bursts):\n            is_burnin = i &lt; burnin_bursts\n\n            if self.verbose and (i == 0 or i == burnin_bursts):\n                print('BURNIN\\n' if is_burnin else '\\nSAMPLING')\n\n            t0 = time.time()\n            states, results, final_results = run_chain(current_state, final_results, kernel, report_interval)\n\n            if self.verbose == True:\n                if not is_burnin: print()\n                accept_rates = results.post_swap_replica_results.is_accepted.numpy().mean(axis=0)\n                print('[iter {:4d}] [time {:.1f}] [accept rates {}]'.format(\n                    ((i if is_burnin else i - burnin_bursts) + 1) * report_interval,\n                    time.time() - t0,\n                    ' '.join([f'{x:.2f}' for x in accept_rates.tolist()])))\n\n            if not is_burnin:\n                acc_states.append(tf.nest.map_structure(lambda x: x.numpy(), states))\n                all_states = [np.concatenate(x, 0) for x in zip(*acc_states)]\n                up = tf.nest.pack_sequence_as(initial_up, all_states)\n                sp = self.parameter_space.get_surface(up, numpy=True) \n\n                # Reporting\n                if self.verbose == True:\n                    for p in [5, 50, 95]:\n                        x = tf.nest.map_structure(lambda x: np.percentile(x, p, axis=0), sp)\n                        self.report(x, prefix=f'{p:02d}%ile')\n\n            current_state = [s[-1] for s in states]\n\n        posterior = self.parameter_space.get_surface(up, numpy=True)\n\n        # Restore order if things were permuted.\n        if cats is not None:\n            revperm = np.argsort(perm)\n            locs, vals, cats = locs[revperm], vals[revperm], cats[revperm]\n\n        return replace(self, \n            parameters=posterior,\n            parameter_sample_size=samples,\n            locs=locs, vals=vals, cats=cats)\n\n    def generate(self, locs, cats=None):\n        \"\"\"\n        Generates synthetic data values from the Gaussian Process (GP) model based on the provided location data.\n        This method simulates values based on the GP's covariance structure, allowing for random sample generation.\n\n        Parameters\n        ----------\n        * locs : np.ndarray\n            A NumPy array containing the input locations for which to generate synthetic values.\n        * cats : np.ndarray, optional\n            A NumPy array containing categorical data corresponding to `locs`. If provided, data points \n            are permuted according to `cats` for stratified generation. Defaults to None.\n\n        Returns\n        -------\n        * self : Model\n            The model instance with generated values stored in `self.vals` and corresponding locations stored \n            in `self.locs`. This enables method chaining.\n\n        Examples\n        --------\n        Generating synthetic values for a set of locations:\n\n        ```\n        from geostat import GP, Model\n        from geostat.kernel import Noise\n\n        # Create model\n        kernel = Noise(nugget=1.0)\n        model = Model(GP(0, kernel))\n\n        # Generate values based on locs\n        locs = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n        model.generate(locs)\n        generated_vals = model.vals  # Access the generated values\n        ```\n\n        Notes\n        -----\n        - Conditional generation is currently not supported, and this method will raise an assertion error if \n        `self.locs` and `self.vals` are already defined.\n        - Generation from a distribution is not yet supported, and an assertion error will be raised if \n        `self.parameter_sample_size` is not `None`.\n        - If `cats` are provided, the data is permuted according to `cats` for stratified generation, and \n        the original order is restored before returning.\n        \"\"\"\n\n        assert self.locs is None and self.vals is None, 'Conditional generation not yet supported'\n        assert self.parameter_sample_size is None, 'Generation from a distribution not yet supported'\n\n        locs = np.array(locs)\n\n        # Permute datapoints if cats is given.\n        if cats is not None:\n            cats = np.array(cats)\n            perm = np.argsort(cats)\n            locs, cats = locs[perm], cats[perm]\n        else:\n            cats = np.zeros(locs.shape[:1], np.int32)\n            perm = None\n\n        m, S = gp_covariance(\n            self.gp,\n            self.warp(locs).run({}),\n            None if cats is None else tf.constant(cats, dtype=tf.int32))\n\n        vals = MVN(m, tf.linalg.cholesky(S)).sample().numpy()\n\n        # Restore order if things were permuted.\n        if perm is not None:\n            revperm = np.argsort(perm)\n            locs, vals, cats = locs[revperm], vals[revperm], cats[revperm]\n\n        self.locs = locs\n        self.vals = vals\n        self.cats = cats\n\n        return self\n\n    def predict(self, locs2, cats2=None, *, subsample=None, reduce=None, tracker=None, pair=False):\n        \"\"\"\n        Performs Gaussian Process (GP) predictions of the mean and variance for the given location data.\n        Supports batch predictions for large datasets and can handle categorical data.\n\n        Parameters\n        ----------\n        * locs2 : np.ndarray\n            A NumPy array containing the input locations for which predictions are to be made.\n        * cats2 : np.ndarray, optional\n            A NumPy array containing categorical data for the prediction locations (`locs2`). If provided,\n            the data points will be permuted according to `cats2`. Default is None.\n        * subsample : int, optional\n            Specifies the number of parameter samples to be used for prediction when `parameter_sample_size` is set.\n            Only valid if parameters are sampled. Default is None.\n        * reduce : str, optional\n            Specifies the reduction method ('mean' or 'median') to aggregate predictions from multiple parameter samples.\n            Only valid if parameters are sampled. Default is None.\n        * tracker : Callable, optional\n            A tracking function for monitoring progress when making predictions across multiple samples. Default is None.\n        * pair : bool, default=False\n            If True, performs pairwise predictions of mean and variance for each pair of input points in `locs2`.\n\n        Returns\n        -------\n        * m : np.ndarray\n            The predicted mean values for the input locations.\n        * v : np.ndarray\n            The predicted variances for the input locations.\n\n        Examples\n        --------\n        Making predictions for a set of locations:\n\n        ```\n        from geostat import GP, Model\n        from geostat.kernel import Noise\n\n        # Create model\n        kernel = Noise(nugget=1.0)\n        model = Model(GP(0, kernel))\n        locs2 = np.array([[7.0, 8.0], [9.0, 10.0]])\n        mean, variance = model.predict(locs2)\n        ```\n\n        Making predictions with categorical data:\n\n        ```\n        cats2 = np.array([1, 2])\n        mean, variance = model.predict(locs2, cats2=cats2)\n        ```\n\n        Notes\n        -----\n        - If `subsample` is specified, it should be used only when `parameter_sample_size` is defined.\n        - The `reduce` parameter allows aggregation of predictions, but it's valid only with sampled parameters.\n        - The method supports pairwise predictions by setting `pair=True`, which is useful for predicting \n        the covariance between two sets of locations.\n        - The internal `interpolate_batch` and `interpolate_pair_batch` functions handle the prediction computations\n        in a batched manner to support large datasets efficiently.\n        \"\"\"\n\n        assert subsample is None or self.parameter_sample_size is not None, \\\n            '`subsample` is only valid with sampled parameters'\n\n        assert reduce is None or self.parameter_sample_size is not None, \\\n            '`reduce` is only valid with sampled parameters'\n\n        assert subsample is None or reduce is None, \\\n            '`subsample` and `reduce` cannot both be given'\n\n        if tracker is None: tracker = lambda x: x\n\n        assert self.locs.shape[-1] == locs2.shape[-1], 'Mismatch in location dimensions'\n        if cats2 is not None:\n            assert cats2.shape == locs2.shape[:1], 'Mismatched shapes in cats and locs'\n        else:\n            cats2 = np.zeros(locs2.shape[:1], np.int32)\n\n        def interpolate_batch(A11i, locs1, vals1diff, cats1, locs2, cats2):\n            \"\"\"\n            Inputs:\n              locs1.shape = [N1, K]\n              vals1diff.shape = [N1]\n              cats1.shape = [N1]\n              locs2.shape = [N2, K]\n              cats2.shape = [N2]\n\n            Outputs:\n              u2_mean.shape = [N2]\n              u2_var.shape = [N2]\n            \"\"\"\n\n            N1 = len(locs1) # Number of measurements.\n\n            # Permute datapoints if cats is given.\n            if cats2 is not None:\n                perm = np.argsort(cats2)\n                locs2, cats2 = locs2[perm], cats2[perm]\n                locs2 = self.warp(locs2).run({})\n\n            _, A12 = gp_covariance2(\n                self.gp,\n                tf.constant(locs1, dtype=tf.float32),\n                tf.constant(cats1, dtype=tf.int32),\n                tf.constant(locs2, dtype=tf.float32),\n                tf.constant(cats2, dtype=tf.int32),\n                N1)\n\n            m2, A22 = gp_covariance(\n                self.gp,\n                tf.constant(locs2, dtype=tf.float32),\n                tf.constant(cats2, dtype=tf.int32))\n\n            # Restore order if things were permuted.\n            if cats2 is not None:\n                revperm = np.argsort(perm)\n                m2 = tf.gather(m2, revperm)\n                A12 = tf.gather(A12, revperm, axis=-1)\n                A22 = tf.gather(tf.gather(A22, revperm), revperm, axis=-1)\n\n            u2_mean = m2 + tf.einsum('ab,a-&gt;b', A12, tf.einsum('ab,b-&gt;a', A11i, vals1diff))\n            u2_var = tf.linalg.diag_part(A22) -  tf.einsum('ab,ab-&gt;b', A12, tf.matmul(A11i, A12))\n\n            return u2_mean, u2_var\n\n        def interpolate_pair_batch(A11i, locs1, vals1diff, cats1, locs2, cats2):\n            \"\"\"\n            Inputs:\n              locs1.shape = [N1, K]\n              vals1diff.shape = [N1]\n              cats1.shape = [N1]\n              locs2.shape = [N2, 2, K]\n              cats2.shape = [N2]\n\n            Outputs:\n              u2_mean.shape = [N2, 2]\n              u2_var.shape = [N2, 2, 2]\n            \"\"\"\n\n            N1 = len(locs1) # Number of measurements.\n            N2 = len(locs2) # Number of prediction pairs.\n\n            # Permute datapoints if cats is given.\n            perm = np.argsort(cats2)\n            locs2, cats2 = locs2[perm], cats2[perm]\n\n            # Warp locs2.\n            locs2_shape = locs2.shape\n            locs2 = locs2.reshape([-1, locs2_shape[-1]])  # Shape into matrix.\n            locs2 = self.warp(locs2).run({})\n            locs2 = tf.reshape(locs2, locs2_shape)  # Revert shape.\n\n            _, A12 = gp_covariance2(\n                self.gp,\n                tf.constant(locs1, dtype=tf.float32),\n                tf.constant(cats1, dtype=tf.int32),\n                tf.constant(locs2[:, 0, :], dtype=tf.float32),\n                tf.constant(cats2, dtype=tf.int32),\n                N1)\n\n            _, A13 = gp_covariance2(\n                self.gp,\n                tf.constant(locs1, dtype=tf.float32),\n                tf.constant(cats1, dtype=tf.int32),\n                tf.constant(locs2[:, 1, :], dtype=tf.float32),\n                tf.constant(cats2, dtype=tf.int32),\n                N1)\n\n            m2, A22 = gp_covariance(\n                self.gp,\n                tf.constant(locs2[:, 0, :], dtype=tf.float32),\n                tf.constant(cats2, dtype=tf.int32))\n\n            m3, A33 = gp_covariance(\n                self.gp,\n                tf.constant(locs2[:, 1, :], dtype=tf.float32),\n                tf.constant(cats2, dtype=tf.int32))\n\n            _, A23 = gp_covariance2(\n                self.gp,\n                tf.constant(locs2[:, 0, :], dtype=tf.float32),\n                tf.constant(cats2, dtype=tf.int32),\n                tf.constant(locs2[:, 1, :], dtype=tf.float32),\n                tf.constant(cats2, dtype=tf.int32),\n                N2)\n\n            # Reassemble into more useful shapes.\n\n            A12 = tf.stack([A12, A13], axis=-1) # [N1, N2, 2]\n\n            m2 = tf.stack([m2, m3], axis=-1) # [N2, 2]\n\n            A22 = tf.linalg.diag_part(A22)\n            A33 = tf.linalg.diag_part(A33)\n            A23 = tf.linalg.diag_part(A23)\n            A22 = tf.stack([tf.stack([A22, A23], axis=-1), tf.stack([A23, A33], axis=-1)], axis=-2) # [N2, 2, 2]\n\n            # Restore order if things were permuted.\n            revperm = np.argsort(perm)\n            m2 = tf.gather(m2, revperm)\n            A12 = tf.gather(A12, revperm, axis=1)\n            A22 = tf.gather(A22, revperm)\n\n            u2_mean = m2 + tf.einsum('abc,a-&gt;bc', A12, tf.einsum('ab,b-&gt;a', A11i, vals1diff))\n            u2_var = A22 - tf.einsum('abc,abd-&gt;bcd', A12, tf.einsum('ae,ebd-&gt;abd', A11i, A12))\n\n            return u2_mean, u2_var\n\n        def interpolate(locs1, vals1, cats1, locs2, cats2, pair=False):\n            # Interpolate in batches.\n            batch_size = locs1.shape[0] // 2\n\n            for_gp = []\n\n            for start in np.arange(0, len(locs2), batch_size):\n                stop = start + batch_size\n                subset = locs2[start:stop], cats2[start:stop]\n                for_gp.append(subset)\n\n            # Permute datapoints if cats is given.\n            if cats1 is not None:\n                perm = np.argsort(cats1)\n                locs1, vals1, cats1 = locs1[perm], vals1[perm], cats1[perm]\n\n            locs1 = self.warp(locs1).run({})\n\n            m1, A11 = gp_covariance(\n                self.gp,\n                tf.constant(locs1, dtype=tf.float32),\n                tf.constant(cats1, dtype=tf.int32))\n\n            A11i = tf.linalg.inv(A11)\n\n            u2_mean_s = []\n            u2_var_s = []\n\n            f = interpolate_pair_batch if pair else interpolate_batch\n\n            for locs_subset, cats_subset in for_gp:\n                u2_mean, u2_var = f(A11i, locs1, vals1 - m1, cats1, locs_subset, cats_subset)\n                u2_mean = u2_mean.numpy()\n                u2_var = u2_var.numpy()\n                u2_mean_s.append(u2_mean)\n                u2_var_s.append(u2_var)\n\n            u2_mean = np.concatenate(u2_mean_s)\n            u2_var = np.concatenate(u2_var_s)\n\n            return u2_mean, u2_var\n\n        if self.parameter_sample_size is None:\n            m, v = interpolate(self.locs, self.vals, self.cats, locs2, cats2, pair)\n        elif reduce == 'median':\n            raise NotImplementedError\n            p = tf.nest.map_structure(lambda x: np.quantile(x, 0.5, axis=0).astype(np.float32), self.parameters)\n            m, v = interpolate(self.locs, self.vals, self.cats, locs2, cats2, p, pair)\n        elif reduce == 'mean':\n            raise NotImplementedError\n            p = tf.nest.map_structure(lambda x: x.mean(axis=0).astype(np.float32), self.parameters)\n            m, v = interpolate(self.locs, self.vals, self.cats, locs2, cats2, p, pair)\n        else:\n            raise NotImplementedError\n            samples = self.parameter_sample_size\n\n            if subsample is not None:\n                assert subsample &lt;= samples, '`subsample` may not exceed sample size'\n            else:\n                subsample = samples\n\n            # Thin by picking roughly equally-spaced samples.\n            a = np.arange(samples) * subsample / samples % 1\n            pick = np.concatenate([[True], a[1:] &gt;= a[:-1]])\n            parameters = tf.nest.map_structure(lambda x: x[pick], self.parameters)\n\n            # Make a prediction for each sample.\n            results = []\n            for i in tracker(range(subsample)):\n                p = tf.nest.map_structure(lambda x: x[i], parameters)\n                results.append(interpolate(self.locs, self.vals, self.cats, locs2, cats2, p, pair))\n\n            mm, vv = [np.stack(x) for x in zip(*results)]\n            m = mm.mean(axis=0)\n            v = (np.square(mm) + vv).mean(axis=0) - np.square(m)\n\n        return m, v\n</code></pre>"},{"location":"api/#src.geostat.model.Model.fit","title":"<code>fit(locs, vals, cats=None, step_size=0.01, iters=100, reg=None)</code>","text":"<p>Trains the model using the provided location and value data by optimizing the parameters of the Gaussian Process (GP) using the Adam optimizer. Optionally performs regularization and can handle categorical data.</p>"},{"location":"api/#src.geostat.model.Model.fit--parameters","title":"Parameters","text":"<ul> <li>locs : np.ndarray     A NumPy array containing the input locations for training.</li> <li>vals : np.ndarray     A NumPy array containing observed values corresponding to the <code>locs</code>.</li> <li>cats : np.ndarray, optional     A NumPy array containing categorical data for each observation in <code>locs</code>. If provided,     the data is sorted according to <code>cats</code> to enable stratified training. Defaults to None.</li> <li>step_size : float, default=0.01     The learning rate for the Adam optimizer.</li> <li>iters : int, default=100     The total number of iterations to run for training.</li> <li>reg : float or None, optional     Regularization penalty parameter. If None, no regularization is applied.</li> </ul>"},{"location":"api/#src.geostat.model.Model.fit--returns","title":"Returns","text":"<ul> <li>self : Model     The model instance with updated parameters, allowing for method chaining.</li> </ul>"},{"location":"api/#src.geostat.model.Model.fit--examples","title":"Examples","text":"<p>Fitting a model using training data:</p> <pre><code>from geostat import GP, Model\nfrom geostat.kernel import Noise\n\n# Create model\nkernel = Noise(nugget=1.0)\nmodel = Model(GP(0, kernel))\n\n# Fit model\nlocs = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])\nvals = np.array([10.0, 15.0, 20.0])\nmodel.fit(locs, vals, step_size=0.05, iters=500)\n</code></pre> <p>Using categorical data for training:</p> <pre><code>cats = np.array([1, 1, 2])\nmodel.fit(locs, vals, cats=cats, step_size=0.01, iters=300)\n</code></pre>"},{"location":"api/#src.geostat.model.Model.fit--notes","title":"Notes","text":"<ul> <li>The <code>fit</code> method uses the Adam optimizer to minimize the negative log-likelihood (<code>ll</code>) and any regularization  penalties specified by <code>reg</code>.</li> <li>During training, if <code>cats</code> are provided, data points are sorted according to <code>cats</code> to ensure grouped training.</li> <li>The <code>verbose</code> flag determines whether training progress is printed after each iteration.</li> <li>After training, parameter values are saved and can be accessed or updated using the model's attributes.</li> </ul> Source code in <code>src/geostat/model.py</code> <pre><code>def fit(self, locs, vals, cats=None, step_size=0.01, iters=100, reg=None):\n    \"\"\"\n    Trains the model using the provided location and value data by optimizing the parameters of the Gaussian Process (GP)\n    using the Adam optimizer. Optionally performs regularization and can handle categorical data.\n\n    Parameters\n    ----------\n    * locs : np.ndarray\n        A NumPy array containing the input locations for training.\n    * vals : np.ndarray\n        A NumPy array containing observed values corresponding to the `locs`.\n    * cats : np.ndarray, optional\n        A NumPy array containing categorical data for each observation in `locs`. If provided,\n        the data is sorted according to `cats` to enable stratified training. Defaults to None.\n    * step_size : float, default=0.01\n        The learning rate for the Adam optimizer.\n    * iters : int, default=100\n        The total number of iterations to run for training.\n    * reg : float or None, optional\n        Regularization penalty parameter. If None, no regularization is applied.\n\n    Returns\n    -------\n    * self : Model\n        The model instance with updated parameters, allowing for method chaining.\n\n    Examples\n    --------\n    Fitting a model using training data:\n\n    ```\n    from geostat import GP, Model\n    from geostat.kernel import Noise\n\n    # Create model\n    kernel = Noise(nugget=1.0)\n    model = Model(GP(0, kernel))\n\n    # Fit model\n    locs = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])\n    vals = np.array([10.0, 15.0, 20.0])\n    model.fit(locs, vals, step_size=0.05, iters=500)\n    ```\n\n    Using categorical data for training:\n\n    ```\n    cats = np.array([1, 1, 2])\n    model.fit(locs, vals, cats=cats, step_size=0.01, iters=300)\n    ```\n\n    Notes\n    -----\n    - The `fit` method uses the Adam optimizer to minimize the negative log-likelihood (`ll`) and any regularization \n    penalties specified by `reg`.\n    - During training, if `cats` are provided, data points are sorted according to `cats` to ensure grouped training.\n    - The `verbose` flag determines whether training progress is printed after each iteration.\n    - After training, parameter values are saved and can be accessed or updated using the model's attributes.\n    \"\"\"\n\n    # Collect parameters and create TF parameters.\n    parameters = self.gather_vars()\n\n    # Permute datapoints if cats is given.\n    if cats is not None:\n        cats = np.array(cats)\n        perm = np.argsort(cats)\n        locs, vals, cats = locs[perm], vals[perm], cats[perm]\n    else:\n        cats = np.zeros(locs.shape[:1], np.int32)\n        perm = None\n\n    # Data dict.\n    self.data = {\n        'warplocs': self.warp(locs),\n        'vals': tf.constant(vals, dtype=tf.float32),\n        'cats': tf.constant(cats, dtype=tf.int32)}\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=step_size)\n\n    j = 0 # Iteration count.\n    for i in range(10):\n        t0 = time.time()\n        while j &lt; (i + 1) * iters / 10:\n            ll, reg_penalty = gp_train_step(\n                optimizer, self.data, parameters, self.gp, reg)\n            j += 1\n\n        time_elapsed = time.time() - t0\n        if self.verbose == True:\n            self.report(\n              dict(iter=j, ll=ll, time=time_elapsed, reg=reg_penalty) |\n              {p.name: p.surface() for p in parameters.values()})\n\n    # Save parameter values.\n    for p in parameters.values():\n        p.update_value()\n\n    # Restore order if things were permuted.\n    if perm is not None:\n        revperm = np.argsort(perm)\n        locs, vals, cats = locs[revperm], vals[revperm], cats[revperm]\n\n    self.locs = locs\n    self.vals = vals\n    self.cats = cats\n\n    return self\n</code></pre>"},{"location":"api/#src.geostat.model.Model.generate","title":"<code>generate(locs, cats=None)</code>","text":"<p>Generates synthetic data values from the Gaussian Process (GP) model based on the provided location data. This method simulates values based on the GP's covariance structure, allowing for random sample generation.</p>"},{"location":"api/#src.geostat.model.Model.generate--parameters","title":"Parameters","text":"<ul> <li>locs : np.ndarray     A NumPy array containing the input locations for which to generate synthetic values.</li> <li>cats : np.ndarray, optional     A NumPy array containing categorical data corresponding to <code>locs</code>. If provided, data points      are permuted according to <code>cats</code> for stratified generation. Defaults to None.</li> </ul>"},{"location":"api/#src.geostat.model.Model.generate--returns","title":"Returns","text":"<ul> <li>self : Model     The model instance with generated values stored in <code>self.vals</code> and corresponding locations stored      in <code>self.locs</code>. This enables method chaining.</li> </ul>"},{"location":"api/#src.geostat.model.Model.generate--examples","title":"Examples","text":"<p>Generating synthetic values for a set of locations:</p> <pre><code>from geostat import GP, Model\nfrom geostat.kernel import Noise\n\n# Create model\nkernel = Noise(nugget=1.0)\nmodel = Model(GP(0, kernel))\n\n# Generate values based on locs\nlocs = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\nmodel.generate(locs)\ngenerated_vals = model.vals  # Access the generated values\n</code></pre>"},{"location":"api/#src.geostat.model.Model.generate--notes","title":"Notes","text":"<ul> <li>Conditional generation is currently not supported, and this method will raise an assertion error if  <code>self.locs</code> and <code>self.vals</code> are already defined.</li> <li>Generation from a distribution is not yet supported, and an assertion error will be raised if  <code>self.parameter_sample_size</code> is not <code>None</code>.</li> <li>If <code>cats</code> are provided, the data is permuted according to <code>cats</code> for stratified generation, and  the original order is restored before returning.</li> </ul> Source code in <code>src/geostat/model.py</code> <pre><code>def generate(self, locs, cats=None):\n    \"\"\"\n    Generates synthetic data values from the Gaussian Process (GP) model based on the provided location data.\n    This method simulates values based on the GP's covariance structure, allowing for random sample generation.\n\n    Parameters\n    ----------\n    * locs : np.ndarray\n        A NumPy array containing the input locations for which to generate synthetic values.\n    * cats : np.ndarray, optional\n        A NumPy array containing categorical data corresponding to `locs`. If provided, data points \n        are permuted according to `cats` for stratified generation. Defaults to None.\n\n    Returns\n    -------\n    * self : Model\n        The model instance with generated values stored in `self.vals` and corresponding locations stored \n        in `self.locs`. This enables method chaining.\n\n    Examples\n    --------\n    Generating synthetic values for a set of locations:\n\n    ```\n    from geostat import GP, Model\n    from geostat.kernel import Noise\n\n    # Create model\n    kernel = Noise(nugget=1.0)\n    model = Model(GP(0, kernel))\n\n    # Generate values based on locs\n    locs = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n    model.generate(locs)\n    generated_vals = model.vals  # Access the generated values\n    ```\n\n    Notes\n    -----\n    - Conditional generation is currently not supported, and this method will raise an assertion error if \n    `self.locs` and `self.vals` are already defined.\n    - Generation from a distribution is not yet supported, and an assertion error will be raised if \n    `self.parameter_sample_size` is not `None`.\n    - If `cats` are provided, the data is permuted according to `cats` for stratified generation, and \n    the original order is restored before returning.\n    \"\"\"\n\n    assert self.locs is None and self.vals is None, 'Conditional generation not yet supported'\n    assert self.parameter_sample_size is None, 'Generation from a distribution not yet supported'\n\n    locs = np.array(locs)\n\n    # Permute datapoints if cats is given.\n    if cats is not None:\n        cats = np.array(cats)\n        perm = np.argsort(cats)\n        locs, cats = locs[perm], cats[perm]\n    else:\n        cats = np.zeros(locs.shape[:1], np.int32)\n        perm = None\n\n    m, S = gp_covariance(\n        self.gp,\n        self.warp(locs).run({}),\n        None if cats is None else tf.constant(cats, dtype=tf.int32))\n\n    vals = MVN(m, tf.linalg.cholesky(S)).sample().numpy()\n\n    # Restore order if things were permuted.\n    if perm is not None:\n        revperm = np.argsort(perm)\n        locs, vals, cats = locs[revperm], vals[revperm], cats[revperm]\n\n    self.locs = locs\n    self.vals = vals\n    self.cats = cats\n\n    return self\n</code></pre>"},{"location":"api/#src.geostat.model.Model.predict","title":"<code>predict(locs2, cats2=None, *, subsample=None, reduce=None, tracker=None, pair=False)</code>","text":"<p>Performs Gaussian Process (GP) predictions of the mean and variance for the given location data. Supports batch predictions for large datasets and can handle categorical data.</p>"},{"location":"api/#src.geostat.model.Model.predict--parameters","title":"Parameters","text":"<ul> <li>locs2 : np.ndarray     A NumPy array containing the input locations for which predictions are to be made.</li> <li>cats2 : np.ndarray, optional     A NumPy array containing categorical data for the prediction locations (<code>locs2</code>). If provided,     the data points will be permuted according to <code>cats2</code>. Default is None.</li> <li>subsample : int, optional     Specifies the number of parameter samples to be used for prediction when <code>parameter_sample_size</code> is set.     Only valid if parameters are sampled. Default is None.</li> <li>reduce : str, optional     Specifies the reduction method ('mean' or 'median') to aggregate predictions from multiple parameter samples.     Only valid if parameters are sampled. Default is None.</li> <li>tracker : Callable, optional     A tracking function for monitoring progress when making predictions across multiple samples. Default is None.</li> <li>pair : bool, default=False     If True, performs pairwise predictions of mean and variance for each pair of input points in <code>locs2</code>.</li> </ul>"},{"location":"api/#src.geostat.model.Model.predict--returns","title":"Returns","text":"<ul> <li>m : np.ndarray     The predicted mean values for the input locations.</li> <li>v : np.ndarray     The predicted variances for the input locations.</li> </ul>"},{"location":"api/#src.geostat.model.Model.predict--examples","title":"Examples","text":"<p>Making predictions for a set of locations:</p> <pre><code>from geostat import GP, Model\nfrom geostat.kernel import Noise\n\n# Create model\nkernel = Noise(nugget=1.0)\nmodel = Model(GP(0, kernel))\nlocs2 = np.array([[7.0, 8.0], [9.0, 10.0]])\nmean, variance = model.predict(locs2)\n</code></pre> <p>Making predictions with categorical data:</p> <pre><code>cats2 = np.array([1, 2])\nmean, variance = model.predict(locs2, cats2=cats2)\n</code></pre>"},{"location":"api/#src.geostat.model.Model.predict--notes","title":"Notes","text":"<ul> <li>If <code>subsample</code> is specified, it should be used only when <code>parameter_sample_size</code> is defined.</li> <li>The <code>reduce</code> parameter allows aggregation of predictions, but it's valid only with sampled parameters.</li> <li>The method supports pairwise predictions by setting <code>pair=True</code>, which is useful for predicting  the covariance between two sets of locations.</li> <li>The internal <code>interpolate_batch</code> and <code>interpolate_pair_batch</code> functions handle the prediction computations in a batched manner to support large datasets efficiently.</li> </ul> Source code in <code>src/geostat/model.py</code> <pre><code>def predict(self, locs2, cats2=None, *, subsample=None, reduce=None, tracker=None, pair=False):\n    \"\"\"\n    Performs Gaussian Process (GP) predictions of the mean and variance for the given location data.\n    Supports batch predictions for large datasets and can handle categorical data.\n\n    Parameters\n    ----------\n    * locs2 : np.ndarray\n        A NumPy array containing the input locations for which predictions are to be made.\n    * cats2 : np.ndarray, optional\n        A NumPy array containing categorical data for the prediction locations (`locs2`). If provided,\n        the data points will be permuted according to `cats2`. Default is None.\n    * subsample : int, optional\n        Specifies the number of parameter samples to be used for prediction when `parameter_sample_size` is set.\n        Only valid if parameters are sampled. Default is None.\n    * reduce : str, optional\n        Specifies the reduction method ('mean' or 'median') to aggregate predictions from multiple parameter samples.\n        Only valid if parameters are sampled. Default is None.\n    * tracker : Callable, optional\n        A tracking function for monitoring progress when making predictions across multiple samples. Default is None.\n    * pair : bool, default=False\n        If True, performs pairwise predictions of mean and variance for each pair of input points in `locs2`.\n\n    Returns\n    -------\n    * m : np.ndarray\n        The predicted mean values for the input locations.\n    * v : np.ndarray\n        The predicted variances for the input locations.\n\n    Examples\n    --------\n    Making predictions for a set of locations:\n\n    ```\n    from geostat import GP, Model\n    from geostat.kernel import Noise\n\n    # Create model\n    kernel = Noise(nugget=1.0)\n    model = Model(GP(0, kernel))\n    locs2 = np.array([[7.0, 8.0], [9.0, 10.0]])\n    mean, variance = model.predict(locs2)\n    ```\n\n    Making predictions with categorical data:\n\n    ```\n    cats2 = np.array([1, 2])\n    mean, variance = model.predict(locs2, cats2=cats2)\n    ```\n\n    Notes\n    -----\n    - If `subsample` is specified, it should be used only when `parameter_sample_size` is defined.\n    - The `reduce` parameter allows aggregation of predictions, but it's valid only with sampled parameters.\n    - The method supports pairwise predictions by setting `pair=True`, which is useful for predicting \n    the covariance between two sets of locations.\n    - The internal `interpolate_batch` and `interpolate_pair_batch` functions handle the prediction computations\n    in a batched manner to support large datasets efficiently.\n    \"\"\"\n\n    assert subsample is None or self.parameter_sample_size is not None, \\\n        '`subsample` is only valid with sampled parameters'\n\n    assert reduce is None or self.parameter_sample_size is not None, \\\n        '`reduce` is only valid with sampled parameters'\n\n    assert subsample is None or reduce is None, \\\n        '`subsample` and `reduce` cannot both be given'\n\n    if tracker is None: tracker = lambda x: x\n\n    assert self.locs.shape[-1] == locs2.shape[-1], 'Mismatch in location dimensions'\n    if cats2 is not None:\n        assert cats2.shape == locs2.shape[:1], 'Mismatched shapes in cats and locs'\n    else:\n        cats2 = np.zeros(locs2.shape[:1], np.int32)\n\n    def interpolate_batch(A11i, locs1, vals1diff, cats1, locs2, cats2):\n        \"\"\"\n        Inputs:\n          locs1.shape = [N1, K]\n          vals1diff.shape = [N1]\n          cats1.shape = [N1]\n          locs2.shape = [N2, K]\n          cats2.shape = [N2]\n\n        Outputs:\n          u2_mean.shape = [N2]\n          u2_var.shape = [N2]\n        \"\"\"\n\n        N1 = len(locs1) # Number of measurements.\n\n        # Permute datapoints if cats is given.\n        if cats2 is not None:\n            perm = np.argsort(cats2)\n            locs2, cats2 = locs2[perm], cats2[perm]\n            locs2 = self.warp(locs2).run({})\n\n        _, A12 = gp_covariance2(\n            self.gp,\n            tf.constant(locs1, dtype=tf.float32),\n            tf.constant(cats1, dtype=tf.int32),\n            tf.constant(locs2, dtype=tf.float32),\n            tf.constant(cats2, dtype=tf.int32),\n            N1)\n\n        m2, A22 = gp_covariance(\n            self.gp,\n            tf.constant(locs2, dtype=tf.float32),\n            tf.constant(cats2, dtype=tf.int32))\n\n        # Restore order if things were permuted.\n        if cats2 is not None:\n            revperm = np.argsort(perm)\n            m2 = tf.gather(m2, revperm)\n            A12 = tf.gather(A12, revperm, axis=-1)\n            A22 = tf.gather(tf.gather(A22, revperm), revperm, axis=-1)\n\n        u2_mean = m2 + tf.einsum('ab,a-&gt;b', A12, tf.einsum('ab,b-&gt;a', A11i, vals1diff))\n        u2_var = tf.linalg.diag_part(A22) -  tf.einsum('ab,ab-&gt;b', A12, tf.matmul(A11i, A12))\n\n        return u2_mean, u2_var\n\n    def interpolate_pair_batch(A11i, locs1, vals1diff, cats1, locs2, cats2):\n        \"\"\"\n        Inputs:\n          locs1.shape = [N1, K]\n          vals1diff.shape = [N1]\n          cats1.shape = [N1]\n          locs2.shape = [N2, 2, K]\n          cats2.shape = [N2]\n\n        Outputs:\n          u2_mean.shape = [N2, 2]\n          u2_var.shape = [N2, 2, 2]\n        \"\"\"\n\n        N1 = len(locs1) # Number of measurements.\n        N2 = len(locs2) # Number of prediction pairs.\n\n        # Permute datapoints if cats is given.\n        perm = np.argsort(cats2)\n        locs2, cats2 = locs2[perm], cats2[perm]\n\n        # Warp locs2.\n        locs2_shape = locs2.shape\n        locs2 = locs2.reshape([-1, locs2_shape[-1]])  # Shape into matrix.\n        locs2 = self.warp(locs2).run({})\n        locs2 = tf.reshape(locs2, locs2_shape)  # Revert shape.\n\n        _, A12 = gp_covariance2(\n            self.gp,\n            tf.constant(locs1, dtype=tf.float32),\n            tf.constant(cats1, dtype=tf.int32),\n            tf.constant(locs2[:, 0, :], dtype=tf.float32),\n            tf.constant(cats2, dtype=tf.int32),\n            N1)\n\n        _, A13 = gp_covariance2(\n            self.gp,\n            tf.constant(locs1, dtype=tf.float32),\n            tf.constant(cats1, dtype=tf.int32),\n            tf.constant(locs2[:, 1, :], dtype=tf.float32),\n            tf.constant(cats2, dtype=tf.int32),\n            N1)\n\n        m2, A22 = gp_covariance(\n            self.gp,\n            tf.constant(locs2[:, 0, :], dtype=tf.float32),\n            tf.constant(cats2, dtype=tf.int32))\n\n        m3, A33 = gp_covariance(\n            self.gp,\n            tf.constant(locs2[:, 1, :], dtype=tf.float32),\n            tf.constant(cats2, dtype=tf.int32))\n\n        _, A23 = gp_covariance2(\n            self.gp,\n            tf.constant(locs2[:, 0, :], dtype=tf.float32),\n            tf.constant(cats2, dtype=tf.int32),\n            tf.constant(locs2[:, 1, :], dtype=tf.float32),\n            tf.constant(cats2, dtype=tf.int32),\n            N2)\n\n        # Reassemble into more useful shapes.\n\n        A12 = tf.stack([A12, A13], axis=-1) # [N1, N2, 2]\n\n        m2 = tf.stack([m2, m3], axis=-1) # [N2, 2]\n\n        A22 = tf.linalg.diag_part(A22)\n        A33 = tf.linalg.diag_part(A33)\n        A23 = tf.linalg.diag_part(A23)\n        A22 = tf.stack([tf.stack([A22, A23], axis=-1), tf.stack([A23, A33], axis=-1)], axis=-2) # [N2, 2, 2]\n\n        # Restore order if things were permuted.\n        revperm = np.argsort(perm)\n        m2 = tf.gather(m2, revperm)\n        A12 = tf.gather(A12, revperm, axis=1)\n        A22 = tf.gather(A22, revperm)\n\n        u2_mean = m2 + tf.einsum('abc,a-&gt;bc', A12, tf.einsum('ab,b-&gt;a', A11i, vals1diff))\n        u2_var = A22 - tf.einsum('abc,abd-&gt;bcd', A12, tf.einsum('ae,ebd-&gt;abd', A11i, A12))\n\n        return u2_mean, u2_var\n\n    def interpolate(locs1, vals1, cats1, locs2, cats2, pair=False):\n        # Interpolate in batches.\n        batch_size = locs1.shape[0] // 2\n\n        for_gp = []\n\n        for start in np.arange(0, len(locs2), batch_size):\n            stop = start + batch_size\n            subset = locs2[start:stop], cats2[start:stop]\n            for_gp.append(subset)\n\n        # Permute datapoints if cats is given.\n        if cats1 is not None:\n            perm = np.argsort(cats1)\n            locs1, vals1, cats1 = locs1[perm], vals1[perm], cats1[perm]\n\n        locs1 = self.warp(locs1).run({})\n\n        m1, A11 = gp_covariance(\n            self.gp,\n            tf.constant(locs1, dtype=tf.float32),\n            tf.constant(cats1, dtype=tf.int32))\n\n        A11i = tf.linalg.inv(A11)\n\n        u2_mean_s = []\n        u2_var_s = []\n\n        f = interpolate_pair_batch if pair else interpolate_batch\n\n        for locs_subset, cats_subset in for_gp:\n            u2_mean, u2_var = f(A11i, locs1, vals1 - m1, cats1, locs_subset, cats_subset)\n            u2_mean = u2_mean.numpy()\n            u2_var = u2_var.numpy()\n            u2_mean_s.append(u2_mean)\n            u2_var_s.append(u2_var)\n\n        u2_mean = np.concatenate(u2_mean_s)\n        u2_var = np.concatenate(u2_var_s)\n\n        return u2_mean, u2_var\n\n    if self.parameter_sample_size is None:\n        m, v = interpolate(self.locs, self.vals, self.cats, locs2, cats2, pair)\n    elif reduce == 'median':\n        raise NotImplementedError\n        p = tf.nest.map_structure(lambda x: np.quantile(x, 0.5, axis=0).astype(np.float32), self.parameters)\n        m, v = interpolate(self.locs, self.vals, self.cats, locs2, cats2, p, pair)\n    elif reduce == 'mean':\n        raise NotImplementedError\n        p = tf.nest.map_structure(lambda x: x.mean(axis=0).astype(np.float32), self.parameters)\n        m, v = interpolate(self.locs, self.vals, self.cats, locs2, cats2, p, pair)\n    else:\n        raise NotImplementedError\n        samples = self.parameter_sample_size\n\n        if subsample is not None:\n            assert subsample &lt;= samples, '`subsample` may not exceed sample size'\n        else:\n            subsample = samples\n\n        # Thin by picking roughly equally-spaced samples.\n        a = np.arange(samples) * subsample / samples % 1\n        pick = np.concatenate([[True], a[1:] &gt;= a[:-1]])\n        parameters = tf.nest.map_structure(lambda x: x[pick], self.parameters)\n\n        # Make a prediction for each sample.\n        results = []\n        for i in tracker(range(subsample)):\n            p = tf.nest.map_structure(lambda x: x[i], parameters)\n            results.append(interpolate(self.locs, self.vals, self.cats, locs2, cats2, p, pair))\n\n        mm, vv = [np.stack(x) for x in zip(*results)]\n        m = mm.mean(axis=0)\n        v = (np.square(mm) + vv).mean(axis=0) - np.square(m)\n\n    return m, v\n</code></pre>"},{"location":"api/#src.geostat.model.Model.set","title":"<code>set(**values)</code>","text":"<p>Sets the values of the model's parameters based on the provided keyword arguments. Each parameter specified must exist in the model; otherwise, a <code>ValueError</code> is raised.</p>"},{"location":"api/#src.geostat.model.Model.set--parameters","title":"Parameters","text":"<ul> <li>values : keyword arguments     A dictionary of parameter names and their corresponding values that should be      set in the model. Each key corresponds to a parameter name, and the value is      the value to be assigned to that parameter.</li> </ul>"},{"location":"api/#src.geostat.model.Model.set--returns","title":"Returns","text":"<ul> <li>self : Model     The model instance with updated parameter values, allowing for method chaining.</li> </ul>"},{"location":"api/#src.geostat.model.Model.set--raises","title":"Raises","text":"<ul> <li>ValueError     If a provided parameter name does not exist in the model's parameters.</li> </ul>"},{"location":"api/#src.geostat.model.Model.set--examples","title":"Examples","text":"<p>Update parameter value using <code>set</code>:</p> <pre><code>from geostat import GP, Model\nfrom geostat.kernel import Noise\n\n# Create model\nkernel = Noise(nugget=1.0)\nmodel = Model(GP(0, kernel))\n\n# Update parameters\nmodel.set(nugget=0.5)\n</code></pre>"},{"location":"api/#src.geostat.model.Model.set--notes","title":"Notes","text":"<ul> <li>The <code>set</code> method retrieves the current parameters using <code>gather_vars</code> and updates  their values. The associated TensorFlow variables are also recreated.</li> <li>This method is useful for dynamically updating the model's parameters after initialization.</li> </ul> Source code in <code>src/geostat/model.py</code> <pre><code>def set(self, **values):\n    \"\"\"\n    Sets the values of the model's parameters based on the provided keyword arguments.\n    Each parameter specified must exist in the model; otherwise, a `ValueError` is raised.\n\n    Parameters\n    ----------\n    * values : keyword arguments\n        A dictionary of parameter names and their corresponding values that should be \n        set in the model. Each key corresponds to a parameter name, and the value is \n        the value to be assigned to that parameter.\n\n    Returns\n    -------\n    * self : Model\n        The model instance with updated parameter values, allowing for method chaining.\n\n    Raises\n    ------\n    * ValueError\n        If a provided parameter name does not exist in the model's parameters.\n\n    Examples\n    --------\n    Update parameter value using `set`:\n\n    ```\n    from geostat import GP, Model\n    from geostat.kernel import Noise\n\n    # Create model\n    kernel = Noise(nugget=1.0)\n    model = Model(GP(0, kernel))\n\n    # Update parameters\n    model.set(nugget=0.5)\n    ```\n\n    Notes\n    -----\n    - The `set` method retrieves the current parameters using `gather_vars` and updates \n    their values. The associated TensorFlow variables are also recreated.\n    - This method is useful for dynamically updating the model's parameters after initialization.\n    \"\"\"\n\n    parameters = self.gather_vars()\n    for name, v in values.items():\n        if name in parameters:\n            parameters[name].value = v\n            parameters[name].create_tf_variable()\n        else:\n            raise ValueError(f\"{k} is not a parameter\")\n    return self\n</code></pre>"},{"location":"api/#src.geostat.model.NormalizingFeaturizer","title":"<code>NormalizingFeaturizer</code>","text":"<p>NormalizingFeaturizer class for producing normalized feature matrices (F matrix) with an intercept.</p> <p>The <code>NormalizingFeaturizer</code> takes raw location data and applies a specified featurization function. It normalizes the resulting features and remembers normalization parameters using the mean and standard deviation calculated from the  original data and adds an intercept feature (a column of ones) to the matrix.</p>"},{"location":"api/#src.geostat.model.NormalizingFeaturizer--parameters","title":"Parameters","text":"<ul> <li>featurization : Callable     A function or callable that defines how the input location data should be featurized.</li> <li>locs : array-like or Tensor     The input location data used for calculating normalization parameters (mean and standard      deviation) and featurizing new data.</li> </ul>"},{"location":"api/#src.geostat.model.NormalizingFeaturizer--examples","title":"Examples","text":"<p>Creating a <code>NormalizingFeaturizer</code> using a custom featurization function and location data:</p> <pre><code>import tensorflow as tf\nfrom geostat.model import NormalizingFeaturizer\n\n# Define a simple featurization function\ndef custom_featurizer(x, y):\n    return x, y, x * y\n\n# Sample location data\nlocs = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n\n# Create the NormalizingFeaturizer\nnorm_featurizer = NormalizingFeaturizer(custom_featurizer, locs)\n</code></pre> <p>Using the <code>NormalizingFeaturizer</code> to featurize new location data:</p> <pre><code>new_locs = tf.constant([[7.0, 8.0], [9.0, 10.0]])\nF_matrix = norm_featurizer(new_locs)\n# F_matrix will contain normalized features with an additional intercept column\n</code></pre>"},{"location":"api/#src.geostat.model.NormalizingFeaturizer--notes","title":"Notes","text":"<ul> <li>The normalization parameters (<code>unnorm_mean</code> and <code>unnorm_std</code>) are calculated based on the    initial <code>locs</code> data provided during initialization.</li> <li>The <code>__call__</code> method applies the normalization and adds an intercept feature when used    to featurize new location data.</li> </ul> Source code in <code>src/geostat/model.py</code> <pre><code>class NormalizingFeaturizer:\n    \"\"\"\n    NormalizingFeaturizer class for producing normalized feature matrices (F matrix) with an intercept.\n\n    The `NormalizingFeaturizer` takes raw location data and applies a specified featurization function.\n    It normalizes the resulting features and remembers normalization parameters using the mean and standard deviation calculated from the \n    original data and adds an intercept feature (a column of ones) to the matrix.\n\n    Parameters\n    ----------\n    * featurization : Callable\n        A function or callable that defines how the input location data should be featurized.\n    * locs : array-like or Tensor\n        The input location data used for calculating normalization parameters (mean and standard \n        deviation) and featurizing new data.\n\n    Examples\n    --------\n    Creating a `NormalizingFeaturizer` using a custom featurization function and location data:\n\n    ```\n    import tensorflow as tf\n    from geostat.model import NormalizingFeaturizer\n\n    # Define a simple featurization function\n    def custom_featurizer(x, y):\n        return x, y, x * y\n\n    # Sample location data\n    locs = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n\n    # Create the NormalizingFeaturizer\n    norm_featurizer = NormalizingFeaturizer(custom_featurizer, locs)\n    ```\n\n    Using the `NormalizingFeaturizer` to featurize new location data:\n\n    ```\n    new_locs = tf.constant([[7.0, 8.0], [9.0, 10.0]])\n    F_matrix = norm_featurizer(new_locs)\n    # F_matrix will contain normalized features with an additional intercept column\n    ```\n\n    Notes\n    -----\n    - The normalization parameters (`unnorm_mean` and `unnorm_std`) are calculated based on the \n      initial `locs` data provided during initialization.\n    - The `__call__` method applies the normalization and adds an intercept feature when used \n      to featurize new location data.\n    \"\"\"\n\n    def __init__(self, featurization, locs):\n        self.unnorm_featurizer = Featurizer(featurization)\n        F_unnorm = self.unnorm_featurizer(locs)\n        self.unnorm_mean = tf.reduce_mean(F_unnorm, axis=0)\n        self.unnorm_std = tf.math.reduce_std(F_unnorm, axis=0)\n\n    def __call__(self, locs):\n        ones = tf.ones([tf.shape(locs)[0], 1], dtype=tf.float32)\n        F_unnorm = self.unnorm_featurizer(locs)\n        F_norm = (F_unnorm - self.unnorm_mean) / self.unnorm_std\n        return tf.concat([ones, F_norm], axis=1)\n</code></pre>"},{"location":"api/#src.geostat.model.Warp","title":"<code>Warp</code>","text":"Source code in <code>src/geostat/model.py</code> <pre><code>class Warp:\n    def __call__(self, locs, prep):\n        \"\"\"\n        `locs` is numpy.\n\n        Returns a WarpLocations.\n        \"\"\"\n        pass\n\n    def gather_vars(self):\n        pass\n</code></pre>"},{"location":"api/#src.geostat.model.Warp.__call__","title":"<code>__call__(locs, prep)</code>","text":"<p><code>locs</code> is numpy.</p> <p>Returns a WarpLocations.</p> Source code in <code>src/geostat/model.py</code> <pre><code>def __call__(self, locs, prep):\n    \"\"\"\n    `locs` is numpy.\n\n    Returns a WarpLocations.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#src.geostat.model.gp_covariance2","title":"<code>gp_covariance2(gp, locs1, cats1, locs2, cats2, offset)</code>","text":"<p><code>offset</code> is i2-i1, where i1 and i2 are the starting indices of locs1 and locs2.  It is used to create the diagonal non-zero elements of a Noise covariance function.  An non-zero offset results in a covariance matrix with non-zero entries along an off-center diagonal.</p> Source code in <code>src/geostat/model.py</code> <pre><code>@tf.function\ndef gp_covariance2(gp, locs1, cats1, locs2, cats2, offset):\n    \"\"\"\n    `offset` is i2-i1, where i1 and i2 are the starting indices of locs1\n    and locs2.  It is used to create the diagonal non-zero elements\n    of a Noise covariance function.  An non-zero offset results in a\n    covariance matrix with non-zero entries along an off-center diagonal.\n    \"\"\"\n\n    # assert np.all(cats1 == np.sort(cats1)), '`cats1` must be in non-descending order'\n    # assert np.all(cats2 == np.sort(cats2)), '`cats2` must be in non-descending order'\n\n    cache = {}\n    cache['offset'] = offset\n    cache['locs1'] = locs1\n    cache['locs2'] = locs2\n    cache['cats1'] = cats1\n    cache['cats2'] = cats2\n    cache['per_axis_dist2'] = PerAxisDist2().run(cache)\n    cache['euclidean'] = Euclidean().run(cache)\n\n    M = gp.mean.run(cache)\n    C = gp.kernel.run(cache)\n    M = tf.cast(M, tf.float64)\n    C = tf.cast(C, tf.float64)\n    return M, C\n</code></pre>"},{"location":"api/#src.geostat.model.interpolate_1d_tf","title":"<code>interpolate_1d_tf(src, tgt, x)</code>","text":"<p><code>src</code>: (batch, breaks) <code>tgt</code>: (batch, breaks) <code>x</code>  : (batch)</p> Source code in <code>src/geostat/model.py</code> <pre><code>@tf.function\ndef interpolate_1d_tf(src, tgt, x):\n    \"\"\"\n    `src`: (batch, breaks)\n    `tgt`: (batch, breaks)\n    `x`  : (batch)\n    \"\"\"\n    x_shape = tf.shape(x)\n    x = tf.reshape(x, [-1, 1]) # (batch, 1)\n    bucket = tf.searchsorted(src, x)\n    bucket = tf.clip_by_value(bucket - 1, 0, tf.shape(tgt)[0] - 2)\n    src0 = tf.gather(src, bucket, batch_dims=1)\n    src1 = tf.gather(src, bucket + 1, batch_dims=1)\n    tgt0 = tf.gather(tgt, bucket, batch_dims=1)\n    tgt1 = tf.gather(tgt, bucket + 1, batch_dims=1)\n    xout = ((x - src0) * tgt1 + (src1 - x) * tgt0) / (src1 - src0)\n    return tf.reshape(xout, x_shape)\n</code></pre>"},{"location":"api/#src.geostat.model.mvn_log_pdf","title":"<code>mvn_log_pdf(u, m, cov)</code>","text":"<p>Log PDF of a multivariate gaussian.</p> Source code in <code>src/geostat/model.py</code> <pre><code>def mvn_log_pdf(u, m, cov):\n    \"\"\"Log PDF of a multivariate gaussian.\"\"\"\n    u_adj = u - m\n    logdet = tf.linalg.logdet(2 * np.pi * cov)\n    quad = tf.matmul(e(u_adj, 0), tf.linalg.solve(cov, e(u_adj, -1)))[0, 0]\n    return tf.cast(-0.5 * (logdet + quad), tf.float32)\n</code></pre>"},{"location":"api/#src.geostat.op","title":"<code>src.geostat.op</code>","text":""},{"location":"api/#src.geostat.op.Op","title":"<code>Op</code>  <code>dataclass</code>","text":"<p>The <code>autoinputs</code> parameter contains a blob of upstream ops. The leaves in the blob are either the op itself or a string identifier. In the latter case, the string identifier should be present as a key in the <code>cache</code> that gets passed in.</p> <p>This parameter links ops together in a DAG. We walk the DAG for various reasons.</p> <ul> <li><code>run</code> calls the op and puts the output in <code>self.out</code>,     after recursively doing this for autoinputs.</li> <li><code>gather_vars</code> recursively gathers variables from self and     autoinputs.</li> </ul> Source code in <code>src/geostat/op.py</code> <pre><code>@dataclass\nclass Op:\n    \"\"\"\n    The `autoinputs` parameter contains a blob of upstream ops. The leaves\n    in the blob are either the op itself or a string identifier. In the\n    latter case, the string identifier should be present as a key in the\n    `cache` that gets passed in.\n\n    This parameter links ops together in a DAG. We walk the DAG for\n    various reasons.\n\n      - `run` calls the op and puts the output in `self.out`,\n        after recursively doing this for autoinputs.\n      - `gather_vars` recursively gathers variables from self and\n        autoinputs.\n    \"\"\"\n    fa: Dict[str, object] # Formal arguments.\n    autoinputs: object # Blob of Ops and strings.\n\n    def vars(self): # Parameters\n        return []\n\n    def gather_vars(self, cache=None):\n        \"\"\"\n        `cache` maps from Op ids to sets of variable names.\n\n        Returns a dict of parameters, keyed by name.\n        \"\"\"\n        if cache is None: cache = {}\n        if id(self) not in cache:\n            vv = {k: v for op in tf.nest.flatten(self.autoinputs)\n                       if isinstance(op, Op)\n                       for k, v in op.gather_vars(cache).items()}\n            # print(self, '&lt;-', [x.name for x in vv], '|', [x.name for x in set(self.vars())], '\\n')\n            cache[id(self)] = vv | self.vars()\n        return cache[id(self)]\n\n    def __call__(self, e):\n        \"\"\"\n        `e` is a dict of params and evaluated inputs from upstream ops.\n        Other values in `e` are supplied by the caller.\n        \"\"\"\n        pass\n\n    def run(self, cache):\n        \"\"\"\n        If op has already been run, return result. Else:\n            - Assemble inputs by recursively calling upstream ops.\n            - Execute op by calling `__call__`.\n            - Store result in cache.\n        \"\"\"\n\n        def eval(op):\n            \"\"\"\n            Evaluate `op`. If `op` is a string, look up its value.\n            Otherwise execute it.\n            \"\"\"\n            if isinstance(op, str):\n                return cache[op]\n            else:\n                return op.run(cache)\n\n        if id(self) not in cache:\n            e = tf.nest.map_structure(lambda op: eval(op), self.autoinputs)\n            e |= get_parameter_values(self.fa)\n            cache[id(self)] = self(e)\n\n            # Save the Op so that its ID remains unique.\n            if '__save__' not in cache: cache['__save__'] = []\n            cache['__save__'].append(self)\n\n        return cache[id(self)]\n\n    def __tf_tracing_type__(self, context):\n        return SingletonTraceType(self)\n</code></pre>"},{"location":"api/#src.geostat.op.Op.__call__","title":"<code>__call__(e)</code>","text":"<p><code>e</code> is a dict of params and evaluated inputs from upstream ops. Other values in <code>e</code> are supplied by the caller.</p> Source code in <code>src/geostat/op.py</code> <pre><code>def __call__(self, e):\n    \"\"\"\n    `e` is a dict of params and evaluated inputs from upstream ops.\n    Other values in `e` are supplied by the caller.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#src.geostat.op.Op.gather_vars","title":"<code>gather_vars(cache=None)</code>","text":"<p><code>cache</code> maps from Op ids to sets of variable names.</p> <p>Returns a dict of parameters, keyed by name.</p> Source code in <code>src/geostat/op.py</code> <pre><code>def gather_vars(self, cache=None):\n    \"\"\"\n    `cache` maps from Op ids to sets of variable names.\n\n    Returns a dict of parameters, keyed by name.\n    \"\"\"\n    if cache is None: cache = {}\n    if id(self) not in cache:\n        vv = {k: v for op in tf.nest.flatten(self.autoinputs)\n                   if isinstance(op, Op)\n                   for k, v in op.gather_vars(cache).items()}\n        # print(self, '&lt;-', [x.name for x in vv], '|', [x.name for x in set(self.vars())], '\\n')\n        cache[id(self)] = vv | self.vars()\n    return cache[id(self)]\n</code></pre>"},{"location":"api/#src.geostat.op.Op.run","title":"<code>run(cache)</code>","text":"<p>If op has already been run, return result. Else:     - Assemble inputs by recursively calling upstream ops.     - Execute op by calling <code>__call__</code>.     - Store result in cache.</p> Source code in <code>src/geostat/op.py</code> <pre><code>def run(self, cache):\n    \"\"\"\n    If op has already been run, return result. Else:\n        - Assemble inputs by recursively calling upstream ops.\n        - Execute op by calling `__call__`.\n        - Store result in cache.\n    \"\"\"\n\n    def eval(op):\n        \"\"\"\n        Evaluate `op`. If `op` is a string, look up its value.\n        Otherwise execute it.\n        \"\"\"\n        if isinstance(op, str):\n            return cache[op]\n        else:\n            return op.run(cache)\n\n    if id(self) not in cache:\n        e = tf.nest.map_structure(lambda op: eval(op), self.autoinputs)\n        e |= get_parameter_values(self.fa)\n        cache[id(self)] = self(e)\n\n        # Save the Op so that its ID remains unique.\n        if '__save__' not in cache: cache['__save__'] = []\n        cache['__save__'].append(self)\n\n    return cache[id(self)]\n</code></pre>"},{"location":"api/#src.geostat.op.SingletonTraceType","title":"<code>SingletonTraceType</code>","text":"<p>               Bases: <code>TraceType</code></p> <p>A trace type to override TF's default behavior, which is to treat dataclass-based onjects as dicts.</p> Source code in <code>src/geostat/op.py</code> <pre><code>class SingletonTraceType(TraceType):\n    \"\"\"\n    A trace type to override TF's default behavior, which is\n    to treat dataclass-based onjects as dicts.\n    \"\"\"\n    def __init__(self, thing):\n        self.value = thing\n\n    def is_subtype_of(self, other):\n        return self.value is other.value\n\n    def most_specific_common_supertype(self, other):\n        if self.value is other.value:\n            return self.value\n        else:\n            return None\n\n    def placeholder_value(self, placeholder_context):\n        return self.value\n\n    def __eq__(self, other):\n        return self.value is other.value\n\n    def __hash__(self):\n        return hash(id(self.value))\n</code></pre>"},{"location":"api/#src.geostat.param","title":"<code>src.geostat.param</code>","text":""},{"location":"api/#src.geostat.param.Parameter","title":"<code>Parameter</code>  <code>dataclass</code>","text":"Source code in <code>src/geostat/param.py</code> <pre><code>@dataclass\nclass Parameter:\n    name: str\n    value: float\n    lo: float = np.nan\n    hi: float = np.nan\n    underlying: tf.Variable = None\n\n    def update_bounds(self, lo: float, hi: float):\n        if np.isnan(self.lo):\n            self.lo = lo\n        else:\n            assert self.lo == lo, f'Conflicting bounds for parameter {self.name}'\n        if np.isnan(self.hi):\n            self.hi = hi\n        else:\n            assert self.hi == hi, f'Conflicting bounds for parameter {self.name}'\n\n    def bounding(self):\n        return \\\n            ('u' if self.lo == float('-inf') else 'b') + \\\n            ('u' if self.hi == float('inf') else 'b')\n\n    def create_tf_variable(self):\n        \"\"\"Create TF variable for underlying parameter or update it\"\"\"\n        # Create underlying parameter.\n        b = self.bounding()\n        if b == 'bb':\n            init = logit((self.value - self.lo) / (self.hi - self.lo))\n        elif b == 'bu':\n            init = np.log(self.value - self.lo)\n        elif b == 'ub':\n            init = -np.log(self.hi - self.value)\n        else:\n            init = self.value\n\n        if self.underlying is None:\n            self.underlying = tf.Variable(init, name=self.name, dtype=tf.float32)\n        else:\n            self.underlying.assign(init)\n\n    def surface(self):\n        \"\"\" Create tensor for surface parameter\"\"\"\n        # Create surface parameter.\n        b = self.bounding()\n        v = self.underlying\n        if b == 'bb':\n            v = tf.math.sigmoid(v) * (self.hi - self.lo) + self.lo\n        elif b == 'bu':\n            v = tf.exp(v) + self.lo\n        elif b == 'ub':\n            v = self.hi - tf.exp(-v)\n        else:\n            v = v + tf.constant(0.)\n        return v\n\n    def update_value(self):\n        self.value = self.surface().numpy()\n</code></pre>"},{"location":"api/#src.geostat.param.Parameter.create_tf_variable","title":"<code>create_tf_variable()</code>","text":"<p>Create TF variable for underlying parameter or update it</p> Source code in <code>src/geostat/param.py</code> <pre><code>def create_tf_variable(self):\n    \"\"\"Create TF variable for underlying parameter or update it\"\"\"\n    # Create underlying parameter.\n    b = self.bounding()\n    if b == 'bb':\n        init = logit((self.value - self.lo) / (self.hi - self.lo))\n    elif b == 'bu':\n        init = np.log(self.value - self.lo)\n    elif b == 'ub':\n        init = -np.log(self.hi - self.value)\n    else:\n        init = self.value\n\n    if self.underlying is None:\n        self.underlying = tf.Variable(init, name=self.name, dtype=tf.float32)\n    else:\n        self.underlying.assign(init)\n</code></pre>"},{"location":"api/#src.geostat.param.Parameter.surface","title":"<code>surface()</code>","text":"<p>Create tensor for surface parameter</p> Source code in <code>src/geostat/param.py</code> <pre><code>def surface(self):\n    \"\"\" Create tensor for surface parameter\"\"\"\n    # Create surface parameter.\n    b = self.bounding()\n    v = self.underlying\n    if b == 'bb':\n        v = tf.math.sigmoid(v) * (self.hi - self.lo) + self.lo\n    elif b == 'bu':\n        v = tf.exp(v) + self.lo\n    elif b == 'ub':\n        v = self.hi - tf.exp(-v)\n    else:\n        v = v + tf.constant(0.)\n    return v\n</code></pre>"},{"location":"api/#src.geostat.param.bpp","title":"<code>bpp(param, lo, hi)</code>","text":"<p>Bounded paper parameter (maybe).</p> Source code in <code>src/geostat/param.py</code> <pre><code>def bpp(param, lo, hi):\n    \"\"\"Bounded paper parameter (maybe).\"\"\"\n    if isinstance(param, Parameter):\n        param.update_bounds(lo, hi)\n        return {param.name: param}\n    else:\n        return {}\n</code></pre>"},{"location":"api/#src.geostat.param.get_parameter_values","title":"<code>get_parameter_values(blob)</code>","text":"<p>For each Parameter encountered in the nested blob, replace it with its surface tensor.</p> Source code in <code>src/geostat/param.py</code> <pre><code>def get_parameter_values(blob: object):\n    \"\"\"\n    For each Parameter encountered in the nested blob,\n    replace it with its surface tensor.\n    \"\"\"\n    if isinstance(blob, dict):\n        return {k: get_parameter_values(a) for k, a in blob.items()}\n    elif isinstance(blob, (list, tuple)):\n        return [get_parameter_values(a) for a in blob]\n    elif isinstance(blob, Parameter):\n        return blob.surface()\n    elif isinstance(blob, str):\n        raise ValueError(f'Bad parameter {blob} is a string')\n    else:\n        return blob\n</code></pre>"},{"location":"api/#src.geostat.param.ppp","title":"<code>ppp(param)</code>","text":"<p>Positive paper parameter (maybe).</p> Source code in <code>src/geostat/param.py</code> <pre><code>def ppp(param):\n    \"\"\"Positive paper parameter (maybe).\"\"\"\n    if isinstance(param, Parameter):\n        param.update_bounds(0., np.inf)\n        return {param.name: param}\n    else:\n        return {}\n</code></pre>"},{"location":"api/#src.geostat.param.upp","title":"<code>upp(param)</code>","text":"<p>Unbounded paper parameter (maybe).</p> Source code in <code>src/geostat/param.py</code> <pre><code>def upp(param):\n    \"\"\"Unbounded paper parameter (maybe).\"\"\"\n    if isinstance(param, Parameter):\n        param.update_bounds(-np.inf, np.inf)\n        return {param.name: param}\n    else:\n        return {}\n</code></pre>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#an-introduction-to-geostat","title":"An introduction to Geostat","text":"<p>In Geostat, we create one model that is used to create synthetic data according to provided parameters, and we create a second model that does the inverse: it takes the data and infers the parameters.</p>"},{"location":"examples/#structured-covariance-functions","title":"Structured covariance functions","text":"<p>Here we show how a progressively more complex covariance function fits data better than simpler ones.</p>"},{"location":"examples/#making-predictions-in-a-shape","title":"Making predictions in a shape","text":"<p>Geostat has utility functions to make it easier to work with shapes.</p>"},{"location":"examples/#gaussian-processes-in-tensorflow","title":"Gaussian processes in Tensorflow","text":"<p>A tutorial on how to implement Gaussian processes in Tensorflow.</p>"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/","title":"3d gaussian processes","text":"<pre><code>from geostat import GP, Model, Mesh, Parameters\nimport geostat\nimport geostat.kernel as krn\nimport matplotlib.pyplot as pp\nimport numpy as np\nimport tensorflow as tf\n</code></pre>"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/#overview","title":"Overview","text":"<p>In this notebook we will:   * Use a Gaussian process with a complex stacked covariance function to generate synthetic data.   * Use Gaussian processes with covariance functions of increasing complexity to infer the geospatial parameters from the synthetic data. Each time, log likelihood improves and the nugget decreases. A smaller nugget means that predictions are more confident.</p>"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/#synthesizing-data","title":"Synthesizing data","text":"<p>We will synthesize data at random locations in 3D space near the origin.</p> <pre><code>np.random.seed(111)\n\nlocs = np.random.normal(size=[500, 3]) * [1., 1., 0.333]\n</code></pre> <p>There will be a cubic depth trend, but no horizontal trends. The decorator converts the function <code>trend_featurizer</code> into a featurizer that Geostat can use. The <code>normalize</code> argument causes each feature to have zero mean and unit variance after being applied to the given locations. It also adds a constant one feature.</p> <pre><code>@geostat.featurizer(normalize=locs.reshape([-1, 3]))\ndef trend_featurizer(x, y, z): return z, z*z, z*z*z\n</code></pre> <p>Model parameters are specified here, along with their values. The return value <code>p</code> is a namespace.</p> <pre><code>p = Parameters(alpha=0.1, zs=10., r=0.33, s1=1., s2=0.5, g1=1., g2=0.5, nugget=0.25)\n</code></pre> <p>The covariance function will include a trend based on the featurizer, and will combine two gamma-exponentials: one that respects depth with z-anisotropy, and one that ignores depth altogether. We will set the <code>range</code> for both to be the same parameter to show that it is possible to tie parameters together. In <code>TrendPrior</code>, <code>alpha</code> parameterizes the normal distribution prior for trend coefficients. </p> <pre><code>kernel = \\\n    krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\\n    krn.GammaExponential(range=p.r, sill=p.s1, gamma=p.g1, scale=[1., 1., p.zs]) + \\\n    krn.GammaExponential(range=p.r, sill=p.s2, gamma=p.g2, scale=[1., 1., 0.]) + \\\n    krn.Noise(p.nugget)\n</code></pre> <p>Define a Gaussian process with zero mean and a covariance function given by <code>kernel</code>.</p> <pre><code>gp = GP(0, kernel)\n</code></pre> <p>Instantiate a <code>Model</code> and immediately call <code>generate</code> to generate synthetic observations.</p> <pre><code>tf.random.set_seed(113)\n\nobs = Model(gp).generate(locs).vals\n</code></pre> <p>When the data is plotted, you can see an overall trend with some localized variations.</p> <pre><code>fig, axs = pp.subplots(3, figsize=(7, 6), dpi=120, sharex=True, sharey=True)\nvmin, vmax = obs.min(), obs.max()\npane = np.round((locs[:, 1] + 2) / 2).astype(int)\nfor i, ymid in enumerate(np.linspace(-2, 2, 3)):\n    ymin, ymax = ymid - 1, ymid + 1\n    c = axs[i].scatter(locs[pane == i, 0], locs[pane == i, 2], c=obs[pane == i], vmin=vmin, vmax=vmax)\n    axs[i].set_title('y = %0.1f' % ymid)\n    axs[i].set_aspect(0.9)\naxs[2].set_xlabel('x-axis')\naxs[1].set_ylabel('z-axis')\n\nfig.subplots_adjust(right=0.9)\ncbar_ax = fig.add_axes([0.88, 0.1, 0.02, 0.8])\nfig.colorbar(c, cax=cbar_ax)\n\nfig.suptitle('Synthetic data, projected to nearest cross section')\npp.show()\n</code></pre> <p></p> <p>Before we continue, let's define a function that takes a model and plots predictions for the three slices shown above.</p> <pre><code>def plot(model):\n    fig, axs = pp.subplots(3, figsize=(7, 6), dpi=120, sharex=True, sharey=True)\n    for i, ymid in enumerate(np.linspace(-2, 2, 3)):\n\n        mesh = Mesh.from_bounds([-3, -1, 3, 1], nx=200)\n        mesh_locs = mesh.locations(proj=[[1, 0, 0], [0, 0, 1], [0, ymid, 0]]) # [x, z, 1] -&gt; [x, y, z].\n        mean, var = model.predict(mesh_locs)\n        meshx, meshy, out = mesh.slice(mean)\n        c = axs[i].pcolormesh(meshx, meshy, out, vmin=vmin, vmax=vmax)\n\n        axs[i].set_title('y = %0.1f' % ymid)\n        axs[i].set_aspect(0.9)\n    axs[2].set_xlabel('x-axis')\n    axs[1].set_ylabel('z-axis')\n\n    fig.subplots_adjust(right=0.9)\n    cbar_ax = fig.add_axes([0.88, 0.1, 0.02, 0.8])\n    fig.colorbar(c, cax=cbar_ax)\n\n    fig.suptitle('Predictions for 3 cross sections')\n    pp.show()\n</code></pre>"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/#model-1-bayesian-regression","title":"Model 1: Bayesian regression","text":"<p>First let's try modeling the data with a Bayesian regression, which is a model with just trends and uncorrelated noise.</p> <pre><code>p = Parameters(alpha=1.0, nugget=0.5)\n\nkernel = krn.TrendPrior(trend_featurizer, alpha=p.alpha) + krn.Noise(nugget=p.nugget)\n\nmodel1 = Model(GP(0, kernel)).fit(locs, obs, iters=500)\n</code></pre> <pre><code>[iter    50 ll -910.60 time  1.73 reg  0.00 alpha  0.61 nugget  0.79]\n[iter   100 ll -853.59 time  0.69 reg  0.00 alpha  0.37 nugget  1.06]\n[iter   150 ll -836.20 time  0.69 reg  0.00 alpha  0.23 nugget  1.28]\n[iter   200 ll -830.86 time  0.68 reg  0.00 alpha  0.15 nugget  1.42]\n[iter   250 ll -829.16 time  0.69 reg  0.00 alpha  0.10 nugget  1.50]\n[iter   300 ll -828.57 time  0.68 reg  0.00 alpha  0.07 nugget  1.54]\n[iter   350 ll -828.35 time  0.69 reg  0.00 alpha  0.06 nugget  1.56]\n[iter   400 ll -828.27 time  0.68 reg  0.00 alpha  0.05 nugget  1.57]\n[iter   450 ll -828.25 time  0.69 reg  0.00 alpha  0.04 nugget  1.58]\n[iter   500 ll -828.24 time  0.68 reg  0.00 alpha  0.04 nugget  1.58]\n</code></pre> <p>And here are predictions for the three slices shown above.</p> <pre><code>plot(model1)\n</code></pre> <p></p>"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/#model-2-gp-with-isotropic-sq-exp-covariance-function","title":"Model 2: GP with isotropic sq-exp covariance function","text":"<p>Now let's layer on an isotropic squared exponential covariance function to the above model.</p> <pre><code>p = Parameters(alpha=1.0, range=1.0, sill=0.5, nugget=0.5)\n\nkernel = \\\n    krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\\n    krn.SquaredExponential(range=p.range, sill=p.sill) + \\\n    krn.Noise(nugget=p.nugget)\n\nmodel2 = Model(GP(0, kernel)).fit(locs, obs, iters=500)\n</code></pre> <pre><code>[iter    50 ll -864.77 time  1.41 reg  0.00 alpha  0.61 sill  0.75 range  0.62 nugget  0.77]\n[iter   100 ll -829.58 time  0.80 reg  0.00 alpha  0.37 sill  0.53 range  0.46 nugget  0.99]\n[iter   150 ll -820.58 time  0.80 reg  0.00 alpha  0.23 sill  0.38 range  0.40 nugget  1.13]\n[iter   200 ll -817.97 time  0.80 reg  0.00 alpha  0.15 sill  0.33 range  0.38 nugget  1.22]\n[iter   250 ll -816.98 time  0.80 reg  0.00 alpha  0.10 sill  0.31 range  0.38 nugget  1.27]\n[iter   300 ll -816.48 time  0.80 reg  0.00 alpha  0.07 sill  0.30 range  0.38 nugget  1.29]\n[iter   350 ll -816.18 time  0.80 reg  0.00 alpha  0.05 sill  0.29 range  0.39 nugget  1.31]\n[iter   400 ll -816.00 time  0.80 reg  0.00 alpha  0.04 sill  0.28 range  0.40 nugget  1.32]\n[iter   450 ll -815.90 time  0.80 reg  0.00 alpha  0.04 sill  0.27 range  0.42 nugget  1.33]\n[iter   500 ll -815.84 time  0.80 reg  0.00 alpha  0.03 sill  0.27 range  0.43 nugget  1.33]\n</code></pre> <p>The log-likelihood is improved as a result of a more complex model. Nugget is much lower. Predictions:</p> <pre><code>plot(model2)\n</code></pre> <p></p>"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/#model-3-gp-with-anisotropic-sq-exp-covariance-function","title":"Model 3: GP with anisotropic sq-exp covariance function","text":"<p>Now we switch from isotropic to anisotropic for the covariance function.</p> <pre><code>p = Parameters(alpha=1.0, range=1.0, sill=0.5, nugget=0.5, zs=5.0)\n\nkernel = \\\n    krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\\n    krn.SquaredExponential(range=p.range, sill=p.sill, scale=[1., 1., p.zs]) + \\\n    krn.Noise(nugget=p.nugget)\n\nmodel3 = Model(GP(0, kernel)).fit(locs, obs, iters=500)\n</code></pre> <pre><code>[iter    50 ll -841.04 time  1.50 reg  0.00 alpha  0.61 zs  7.43 sill  0.75 range  0.65 nugget  0.76]\n[iter   100 ll -821.03 time  0.86 reg  0.00 alpha  0.37 zs  8.75 sill  0.74 range  0.50 nugget  0.90]\n[iter   150 ll -817.17 time  0.86 reg  0.00 alpha  0.23 zs  8.73 sill  0.66 range  0.43 nugget  0.95]\n[iter   200 ll -815.63 time  0.86 reg  0.00 alpha  0.15 zs  8.13 sill  0.63 range  0.39 nugget  0.97]\n[iter   250 ll -814.62 time  0.86 reg  0.00 alpha  0.10 zs  7.37 sill  0.63 range  0.37 nugget  0.98]\n[iter   300 ll -813.89 time  0.87 reg  0.00 alpha  0.07 zs  6.66 sill  0.63 range  0.36 nugget  0.97]\n[iter   350 ll -813.41 time  0.87 reg  0.00 alpha  0.05 zs  6.08 sill  0.64 range  0.35 nugget  0.97]\n[iter   400 ll -813.12 time  0.87 reg  0.00 alpha  0.04 zs  5.64 sill  0.65 range  0.34 nugget  0.96]\n[iter   450 ll -812.97 time  0.87 reg  0.00 alpha  0.04 zs  5.32 sill  0.66 range  0.33 nugget  0.95]\n[iter   500 ll -812.89 time  0.87 reg  0.00 alpha  0.04 zs  5.11 sill  0.67 range  0.32 nugget  0.94]\n</code></pre> <p>Log-likelihood and nugget both improve further. Predictions:</p> <pre><code>plot(model3)\n</code></pre> <p></p>"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/#model-4-gp-with-anisotropic-gamma-exp-covariance-function","title":"Model 4: GP with anisotropic gamma-exp covariance function","text":"<p>Now we switch from a squared-exponential to a gamma-exponential covariance function, which has an extra shape parameter.</p> <pre><code>p = Parameters(alpha=1.0, range=1.0, sill=0.5, nugget=0.5, zs=5.0, gamma=1.0)\n\nkernel = \\\n    krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\\n    krn.GammaExponential(range=p.range, sill=p.sill, gamma=p.gamma, scale=[1., 1., p.zs]) + \\\n    krn.Noise(nugget=p.nugget)\n\nmodel4 = Model(GP(0, kernel)).fit(locs, obs, iters=500)\n</code></pre> <pre><code>[iter    50 ll -819.50 time  1.83 reg  0.00 alpha  0.61 zs  7.16 sill  0.76 range  0.67 gamma  0.84 nugget  0.74]\n[iter   100 ll -815.43 time  0.94 reg  0.00 alpha  0.37 zs  6.94 sill  0.86 range  0.58 gamma  0.81 nugget  0.82]\n[iter   150 ll -814.43 time  0.94 reg  0.00 alpha  0.23 zs  6.19 sill  0.87 range  0.56 gamma  0.81 nugget  0.82]\n[iter   200 ll -813.66 time  0.94 reg  0.00 alpha  0.15 zs  5.77 sill  0.87 range  0.53 gamma  0.80 nugget  0.82]\n[iter   250 ll -813.07 time  0.94 reg  0.00 alpha  0.10 zs  5.56 sill  0.86 range  0.50 gamma  0.80 nugget  0.82]\n[iter   300 ll -812.64 time  0.94 reg  0.00 alpha  0.07 zs  5.45 sill  0.86 range  0.48 gamma  0.81 nugget  0.81]\n[iter   350 ll -812.36 time  0.95 reg  0.00 alpha  0.05 zs  5.39 sill  0.86 range  0.46 gamma  0.82 nugget  0.80]\n[iter   400 ll -812.19 time  0.95 reg  0.00 alpha  0.04 zs  5.36 sill  0.86 range  0.45 gamma  0.83 nugget  0.79]\n[iter   450 ll -812.10 time  0.95 reg  0.00 alpha  0.04 zs  5.33 sill  0.86 range  0.43 gamma  0.84 nugget  0.78]\n[iter   500 ll -812.06 time  0.95 reg  0.00 alpha  0.03 zs  5.31 sill  0.87 range  0.43 gamma  0.85 nugget  0.77]\n</code></pre> <p>The log-likelihood has improved very slightly but nugget improves more significantly, since the pointy peak in the gamma-exponential does some of the work of a nugget. Predictions:</p> <pre><code>plot(model4)\n</code></pre> <p></p>"},{"location":"notebooks/3d-gaussian-processes/3d-gaussian-processes/#model-5-gp-with-stacked-covariance-functions","title":"Model 5: GP with stacked covariance functions","text":"<p>Finally we switch to using the same covariance function used to generate the synthetic data.</p> <pre><code>p = Parameters(alpha=1., zs=5., r=0.5, s1=2., s2=1., g1=1., g2=1., nugget=1.)\n\nkernel = \\\n    krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\\n    krn.GammaExponential(range=p.r, sill=p.s1, gamma=p.g1, scale=[1., 1., p.zs]) + \\\n    krn.GammaExponential(range=p.r, sill=p.s2, gamma=p.g2, scale=[1., 1., 0.]) + \\\n    krn.Noise(nugget=p.nugget)\n\nmodel5 = Model(GP(0, kernel)).fit(locs, obs, iters=500)\n</code></pre> <pre><code>[iter    50 ll -824.02 time  2.05 reg  0.00 alpha  0.61 zs  4.04 s1  1.32 r  0.75 g1  0.85 s2  0.65 g2  1.16 nugget  0.67]\n[iter   100 ll -816.07 time  1.10 reg  0.00 alpha  0.38 zs  6.56 s1  1.11 r  0.81 g1  0.57 s2  0.50 g2  1.00 nugget  0.62]\n[iter   150 ll -813.38 time  1.10 reg  0.00 alpha  0.24 zs  8.73 s1  1.02 r  0.79 g1  0.45 s2  0.43 g2  0.79 nugget  0.60]\n[iter   200 ll -812.33 time  1.10 reg  0.00 alpha  0.16 zs  9.27 s1  0.97 r  0.74 g1  0.41 s2  0.39 g2  0.63 nugget  0.59]\n[iter   250 ll -811.74 time  1.11 reg  0.00 alpha  0.11 zs  9.15 s1  0.94 r  0.67 g1  0.42 s2  0.38 g2  0.57 nugget  0.59]\n[iter   300 ll -811.27 time  1.11 reg  0.00 alpha  0.08 zs  9.08 s1  0.91 r  0.60 g1  0.45 s2  0.38 g2  0.54 nugget  0.58]\n[iter   350 ll -810.74 time  1.11 reg  0.00 alpha  0.06 zs  9.15 s1  0.88 r  0.52 g1  0.52 s2  0.37 g2  0.54 nugget  0.57]\n[iter   400 ll -809.82 time  1.10 reg  0.00 alpha  0.05 zs  9.39 s1  0.82 r  0.43 g1  0.66 s2  0.37 g2  0.54 nugget  0.56]\n[iter   450 ll -808.27 time  1.11 reg  0.00 alpha  0.04 zs  9.57 s1  0.76 r  0.36 g1  0.93 s2  0.36 g2  0.52 nugget  0.54]\n[iter   500 ll -807.77 time  1.11 reg  0.00 alpha  0.04 zs  8.73 s1  0.73 r  0.35 g1  1.11 s2  0.37 g2  0.49 nugget  0.53]\n</code></pre> <p>Not surprisingly, log likelihood and nugget both improve further. You can see faint vertical stripes that correspond the the \"depth-invariant\" component of the covariance function.</p> <pre><code>plot(model5)\n</code></pre> <p></p>"},{"location":"notebooks/gaussian-processes-in-geostat/gaussian-processes-in-geostat/","title":"Gaussian processes in geostat","text":"<pre><code>from geostat import GP, Model, Mesh, Parameters\nimport geostat\nimport geostat.kernel as krn\nimport matplotlib.pyplot as pp\nimport numpy as np\n</code></pre>"},{"location":"notebooks/gaussian-processes-in-geostat/gaussian-processes-in-geostat/#overview","title":"Overview","text":"<p>In this notebook we will:   * Use a Gaussian process to generate synthetic data with known geospatial parameters.   * Use a second Gaussian process to infer the geospatial parameters from the synthetic data.   * Use the fitted Gaussian process to interpolate locations on a mesh.</p>"},{"location":"notebooks/gaussian-processes-in-geostat/gaussian-processes-in-geostat/#synthesizing-data","title":"Synthesizing data","text":"<p>We will synthesize data at mesh locations in a square centered on the origin.</p> <p>First define mesh locations using a <code>Mesh</code> object. The <code>nx</code> argument specifies 80 mesh coordinates in the x direction, and keeps the pitch the same in the y direction (which results in 80 mesh coordinates in that direction as well).</p> <pre><code>mesh = Mesh.from_bounds([-1, -1, 1, 1], nx=80)\n</code></pre> <p>Declare the terms of the spatial trend. The decorator converts the function <code>trend_featurizer</code> into a featurizer that Geostat can use. The <code>normalize</code> argument causes each feature to have zero mean and unit variance after being applied to mesh locations. It also adds a constant one feature. The method <code>mesh.locations()</code> returns an array of shape <code>[N, 2]</code>, where <code>N</code> is the number of locations. </p> <pre><code>@geostat.featurizer(normalize=mesh.locations())\ndef trend_featurizer(x, y): return x, y, x*x, x*y, y*y\n</code></pre> <p>Model parameters are specified here, along with their values. The return value <code>p</code> is a namespace.</p> <pre><code>p = Parameters(alpha=0.25, range=0.33, sill=1., nugget=0.25)\n</code></pre> <p>The covariance function has three terms:</p> <ul> <li><code>TrendPrior</code> specifies a trend based on <code>trend_featurizer</code>. In <code>TrendPrior</code>, <code>alpha</code> parameterizes the normal distribution prior for trend coefficients. </li> <li><code>SquaredExponential</code>, a stationary covariance function.</li> <li><code>Noise</code>, uncorrelated noise.</li> </ul> <pre><code>kernel = krn.TrendPrior(trend_featurizer, alpha=p.alpha) + \\\n         krn.SquaredExponential(range=p.range, sill=p.sill) + \\\n         krn.Noise(nugget=p.nugget)\n</code></pre> <p>Define a Gaussian process with zero mean and a covariance function given by <code>kernel</code>.</p> <pre><code>gp = GP(0, kernel)\n</code></pre> <p>Instantiate a <code>Model</code> and then call <code>generate</code> to generate synthetic observations. The result <code>mesh_obs</code> has shape <code>[N]</code>.</p> <pre><code>model = Model(gp)\nmesh_obs = model.generate(mesh.locations()).vals\n</code></pre> <p>When the data is plotted, you can see an overall trend with some localized variations. The method <code>mesh.slice()</code> forms the observations into a 2d array suitable for use with <code>pcolormesh</code>.</p> <pre><code>vmin, vmax = mesh_obs.min(), mesh_obs.max()\nmeshx, meshy, mesh_obs_2d = mesh.slice(mesh_obs) # Each return value is a 2d array.\nc = pp.pcolormesh(meshx, meshy, mesh_obs_2d, vmin=vmin, vmax=vmax)\npp.colorbar(c)\npp.title('Synthetic data')\npp.show()\n</code></pre> <p></p> <p>Of these synthetic datapoints we'll sample just 200, with which we'll try to reconstruct the rest of the data.</p> <pre><code>sample_indices = np.random.choice(len(mesh_obs), [200], replace=False)\nlocs = mesh.locations()[sample_indices, :]\nobs = mesh_obs[sample_indices]\n\nc = pp.scatter(locs[:, 0], locs[:, 1], c=obs, vmin=vmin, vmax=vmax)\npp.colorbar(c)\npp.title('Synthetic observations')\npp.show()\n</code></pre> <p></p>"},{"location":"notebooks/gaussian-processes-in-geostat/gaussian-processes-in-geostat/#inferring-parameters","title":"Inferring parameters","text":"<p>Now we <code>set</code> the parameters in model to something arbitrary and see if the model can infer the correct parameters from the data, which consists of <code>locs</code> and <code>obs</code>. We don't expect <code>alpha</code> to converge to what it used to be, since <code>TrendPrior</code> generates only a small number of trend coefficients using <code>alpha</code>. However, <code>sill</code>, <code>range</code>, and <code>nugget</code> should all converge to something close.</p> <p>(The <code>None</code> at the end suppresses extraneous output.)</p> <pre><code>model.set(alpha=1.0, range=1.0, sill=0.5, nugget=0.5)\n\nmodel.fit(locs, obs, iters=500)\n\nNone\n</code></pre> <pre><code>[iter    50 ll -237.51 time  1.86 reg  0.00 alpha  0.61 sill  0.84 range  0.60 nugget  0.56]\n[iter   100 ll -200.69 time  0.59 reg  0.00 alpha  0.37 sill  1.24 range  0.42 nugget  0.34]\n[iter   150 ll -194.76 time  0.59 reg  0.00 alpha  0.23 sill  1.44 range  0.37 nugget  0.26]\n[iter   200 ll -194.28 time  0.58 reg  0.00 alpha  0.17 sill  1.48 range  0.36 nugget  0.25]\n[iter   250 ll -194.12 time  0.58 reg  0.00 alpha  0.13 sill  1.47 range  0.36 nugget  0.25]\n[iter   300 ll -194.05 time  0.58 reg  0.00 alpha  0.11 sill  1.46 range  0.35 nugget  0.25]\n[iter   350 ll -194.01 time  0.58 reg  0.00 alpha  0.09 sill  1.46 range  0.35 nugget  0.25]\n[iter   400 ll -194.00 time  0.58 reg  0.00 alpha  0.09 sill  1.46 range  0.35 nugget  0.25]\n[iter   450 ll -193.99 time  0.58 reg  0.00 alpha  0.08 sill  1.47 range  0.35 nugget  0.25]\n[iter   500 ll -193.99 time  0.58 reg  0.00 alpha  0.08 sill  1.48 range  0.36 nugget  0.25]\n</code></pre>"},{"location":"notebooks/gaussian-processes-in-geostat/gaussian-processes-in-geostat/#generating-predictions","title":"Generating predictions","text":"<p>Call <code>model</code> to get predictions at the same mesh locations as before:</p> <pre><code>mean, var = model.predict(mesh.locations())\n</code></pre> <pre><code>meshx, meshy, mean2d = mesh.slice(mean) # Each return value is a 2d array.\nc = pp.pcolormesh(meshx, meshy, mean2d, vmin=vmin, vmax=vmax)\npp.colorbar(c)\npp.title('Prediction mean')\npp.show()\n</code></pre> <p></p> <p>For comparison, here's the original synthetic data:</p> <pre><code>c = pp.pcolormesh(meshx, meshy, mesh_obs_2d, vmin=vmin, vmax=vmax)\npp.colorbar(c)\npp.title('Synthetic data')\npp.show()\n</code></pre> <p></p> <p>And here's a plot of prediction variance, which accounts for, among other things, the noise that the model is unable to reconstruct.</p> <pre><code>meshx, meshy, var2d = mesh.slice(var) # Each return value is a 2d array.\nc = pp.pcolormesh(meshx, meshy, var2d, cmap='gist_heat_r')\npp.colorbar(c)\npp.title('Prediction variance')\npp.show()\n</code></pre> <p></p>"},{"location":"notebooks/gaussian-processes-in-tensorflow/gaussian-processes-in-tensorflow/","title":"Gaussian processes in Tensorflow","text":"<pre><code>import tensorflow as tf\ntf.__version__\n</code></pre> <pre><code>'1.8.0'\n</code></pre> <p>Geostatistical datasets are often a set of measurements with locations. Nearby measurements covary a lot. Distant measurements are nearly independent.</p> <p>Let's simulate this:</p> <ul> <li>Define \\(N\\) locations \\(x\\) by drawing uniformly at random from a square area.</li> <li>Create \\(N\\times N\\) distance matrix \\(D\\) (euclidean distance between locations).</li> <li>Define a covariance function: \\(c(d; r, s, n) = s \\cdot \\exp(-(d/r)^2) + n \\cdot \\delta_d\\), where \\(d\\) is distance, and \\((r, s, n)\\) correspond to range/sill/nugget on a variogram. (I think the convention varies between  calling either \\(s+n\\) or \\(s\\) the sill.) Also, \\(\\delta_d\\) is 1 when \\(d\\) is 0, and 0 otherwise.</li> <li>Use the covariance function to map \\(D\\) elementwise to a covariance matrix \\(C\\).</li> <li>Draw \\(u \\sim \\textrm{Normal}(\\beta_1 \\cdot \\mathbb{1}, C)\\) to obtain values, where \\(\\beta_1\\) is the (scalar) mean value of a single draw, and \\(\\mathbb{1}\\) is a vector of \\(N\\) ones.</li> </ul> <p>This is implemented as <code>simulate_gp</code> below. The range is called <code>vrange</code> to avoid conflicting with Python's <code>range</code>.</p> <p>The data consists of locations <code>x</code> (or equivalently, the distance matrix <code>D</code>) and measurements <code>u</code>.</p> <pre><code>import numpy as np\nfrom scipy.spatial.distance import cdist\nnp.set_printoptions(precision=2, threshold=50)\n\ndef simulate_gp(N, vrange, sill, nugget, offset):\n\n    # Sample N locations from square with corners at [\u00b110, \u00b110].\n    x = np.random.uniform(-10.0, 10.0, [N, 2]) \n\n    # Compute distance matrix for sampled locations.\n    D = cdist(x, x)\n\n    # Compute corresponding covariance matrix.\n    C = sill * np.exp(-np.square(D/vrange)) + nugget * np.eye(N)\n\n    # The mean is just a vector where every entry is the offset.\n    m = np.zeros([N]) + offset\n\n    # Simulate geospatial measurements by sampling using covariance matrix\n    u = np.random.multivariate_normal(m, C)\n\n    return x, D, C, m, u\n</code></pre> <p>Now we call <code>simulate_gp</code> and plot the result. (You may have to run this twice to get the plot to show up.)</p> <pre><code>x, D, C, m, u = simulate_gp(\n    N = 300,\n    vrange = 5.0,\n    sill = 2.0,\n    nugget = 2.0,\n    offset = 1.0)\n\nprint(\"Locations\")\nprint(x)\n\nprint(\"Distance matrix\")\nprint(D)\n\nprint(\"Covariance matrix\")\nprint(C)\n\nprint(\"Simulated measurements\")\nprint(u)\n\nimport matplotlib.pyplot as pp\n\npp.scatter(x[:, 0], x[:, 1], c=u)\npp.show()\n</code></pre> <pre><code>Locations\n[[ 0.83  9.5 ]\n [-6.51  0.99]\n [ 2.84  6.14]\n ...\n [-4.99  0.45]\n [-3.49  2.24]\n [ 5.49 -0.34]]\nDistance matrix\n[[ 0.   11.24  3.92 ... 10.76  8.45 10.89]\n [11.24  0.   10.68 ...  1.61  3.26 12.07]\n [ 3.92 10.68  0.   ...  9.68  7.44  7.  ]\n ...\n [10.76  1.61  9.68 ...  0.    2.33 10.51]\n [ 8.45  3.26  7.44 ...  2.33  0.    9.34]\n [10.89 12.07  7.   ... 10.51  9.34  0.  ]]\nCovariance matrix\n[[4.   0.01 1.08 ... 0.02 0.11 0.02]\n [0.01 4.   0.02 ... 1.8  1.31 0.01]\n [1.08 0.02 4.   ... 0.05 0.22 0.28]\n ...\n [0.02 1.8  0.05 ... 4.   1.61 0.02]\n [0.11 1.31 0.22 ... 1.61 4.   0.06]\n [0.02 0.01 0.28 ... 0.02 0.06 4.  ]]\nSimulated measurements\n[ 2.78 -0.64 -0.56 ... -0.27  2.02  2.  ]\n</code></pre> <p></p> <p>Now we make a function <code>infer_gp</code> to infer the gaussian process parameters (range, sill, nugget, offset) using maximum likelihood. We implement the same graph in Tensorflow as we did in NumPy, and tack on the negative log PDF of a multivariate normal distribution at the end, which we minimize. That is, we minimize: \\(\\(-\\log p(u \\mid m, C) = \\frac{1}{2}\\bigg[\\log\\,\\big|\\, 2\\pi C\\,\\big| + (u-m)^T C^{-1} (u-m) \\bigg],\\)\\) where \\(m = \\beta_1 \\cdot \\mathbb{1}\\).</p> <p>The function has two arguments:   * <code>inputs</code> is a list of <code>numpy</code> arrays:       * Distance matrix <code>D</code> (shape: [N, N])       * Measurements <code>u</code> (shape: [N])   *  <code>parameters</code> is a list of tensors for range, sill, nugget, and offset. Each tensor can be a <code>tf.Variables</code> (if it is to be inferred) or a constant (if it's a given).</p> <pre><code>def infer_gp(inputs, parameters):\n\n    D, u = inputs\n    vrange, sill, nugget, offset = parameters\n\n    # Construct covariance; boost diagonal by 1e-6 for numerical stability.\n    covariance = sill * tf.exp(-tf.square(tf.constant(D)/vrange)) \\\n               + (nugget + 1e-6) * tf.eye(D.shape[0], dtype=tf.float64)\n\n    # Log likelihood is the PDF of a multivariate gaussian.\n    u_adj = tf.constant(u) - offset\n    logdet = tf.linalg.logdet(2 * np.pi * covariance)\n    quad = tf.matmul(tf.expand_dims(u_adj, 0), tf.matrix_solve(covariance, tf.expand_dims(u_adj, -1)))[0, 0]\n    ll = -0.5 * (logdet + quad)\n\n    # Infer using an adaptive gradient descent optimizer.\n    train = tf.train.AdamOptimizer(1e-2).minimize(-ll)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for i in range(20):\n            for j in range(100):\n                sess.run(train)\n            print('[ll %7.2f] [range %4.2f] [sill %4.2f] [nugget %4.2f] [offset %4.2f]' % \n                  tuple(sess.run([ll, vrange, sill, nugget, offset])))\n\n        return sess.run([vrange, sill, nugget, offset])\n</code></pre> <p>Here we make a <code>tf.Variable</code> for each parameter, using a log as the underlying representation if the parameter is positive-only.</p> <pre><code># Define parameters using tf.Variable.\nlog_vrange = tf.Variable(0.0, dtype=tf.float64)\nlog_sill = tf.Variable(0.0, dtype=tf.float64)\nlog_nugget = tf.Variable(0.0, dtype=tf.float64)\n\nvrange = tf.exp(log_vrange)\nsill = tf.exp(log_sill)\nnugget = tf.exp(log_nugget)\noffset = tf.Variable(0.0, dtype=tf.float64)\n\nvrange_val, sill_val, nugget_val, offset_val = infer_gp([D, u], [vrange, sill, nugget, offset])\n</code></pre> <pre><code>[ll -552.48] [range 2.25] [sill 1.48] [nugget 1.66] [offset 0.63]\n[ll -537.88] [range 3.71] [sill 1.36] [nugget 1.68] [offset 0.84]\n[ll -536.83] [range 4.20] [sill 1.38] [nugget 1.70] [offset 0.96]\n[ll -536.62] [range 4.28] [sill 1.42] [nugget 1.71] [offset 1.05]\n[ll -536.51] [range 4.28] [sill 1.43] [nugget 1.71] [offset 1.12]\n[ll -536.45] [range 4.27] [sill 1.42] [nugget 1.71] [offset 1.18]\n[ll -536.42] [range 4.26] [sill 1.41] [nugget 1.71] [offset 1.23]\n[ll -536.41] [range 4.25] [sill 1.40] [nugget 1.71] [offset 1.26]\n[ll -536.40] [range 4.25] [sill 1.40] [nugget 1.71] [offset 1.27]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.29]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.29]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30]\n[ll -536.40] [range 4.24] [sill 1.40] [nugget 1.71] [offset 1.30]\n</code></pre> <p>We can do better inference by integrating over all possibilities for \\(\\beta_1\\).  (Integrating over range, sill, and nugget, which are parameters in \\(C\\), is hard; but integrating over parameters in \\(m\\) is relatively easy.) This corresponds to the following generative model.</p> <p>First, draw \\(\\beta_1\\) from a normal distribution:</p> \\[\\beta_1 \\sim \\mathcal{N}(0, 100).\\] <p>The variance should be large enough that the distribution assigns reasonably large probabilities to any plausible value for \\(\\beta_1\\).</p> <p>Next, draw \\(u\\) from a multivariate normal distribution, as we've been doing all along:</p> \\[u \\mid \\beta_1 \\sim \\mathcal{N}(\\beta_1 \\cdot \\mathbb{1}, C).\\] <p>From this we can derive a distribution for \\(u\\) by marginalizing (integrating) over \\(\\beta_1\\). That is, we can compute:</p> \\[p(u) = \\int_{-\\infty}^{\\infty} p(u\\mid\\beta_1) \\, p(\\beta_1) \\, d\\beta_1.\\] <p>We rely on the abstract fact that if \\(X_1 \\sim \\mathcal{N}(\\mu_1, \\Sigma_1)\\) and \\(X_2 \\mid X_1 \\sim \\mathcal{N}(\\mu_2, \\Sigma_2)\\), then \\(X_2 \\sim \\mathcal{N}(\\mu_1, \\Sigma_1 + \\Sigma_2)\\).  (\\(X_1\\) and \\(X_2\\) are vectors equal in length.) To apply this, we note that since \\(\\beta_1 \\sim \\mathcal{N}(0, 100)\\), then \\(\\beta_1 \\cdot \\mathbb{1} \\sim \\mathcal{N}(0, 100 \\cdot \\mathbb{1}\\mathbb{1}^T)\\), where \\(\\mathbb{1}\\mathbb{1}^T\\) is a matrix of all ones. From this it follows that</p> \\[u \\sim \\mathcal{N}(0, A)\\] <p>where \\(A = 100 \\cdot \\mathbb{1}\\mathbb{1}^T + C.\\) The function below implements inference with this model, where the offset is marginalized out.</p> <pre><code>def infer_gp_marginalize_over_offset(inputs, parameters, offset_prior):\n\n    D, u = inputs\n    vrange, sill, nugget = parameters\n\n    # Construct covariance; boost diagonal by 1e-6 for numerical stability.\n    covariance = sill * tf.exp(-tf.square(tf.constant(D)/vrange)) \\\n               + (nugget + 1e-6) * tf.eye(D.shape[0], dtype=tf.float64) \\\n               + offset_prior\n\n    # Log likelihood is the PDF of a multivariate gaussian.\n    u_adj = tf.constant(u) - offset\n    logdet = tf.linalg.logdet(2 * np.pi * covariance)\n    quad = tf.matmul(tf.expand_dims(u_adj, 0), tf.matrix_solve(covariance, tf.expand_dims(u_adj, -1)))[0, 0]\n    ll = -0.5 * (logdet + quad)\n\n    # Infer using an adaptive gradient descent optimizer.\n    train = tf.train.AdamOptimizer(1e-2).minimize(-ll)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for i in range(20):\n            for j in range(100):\n                sess.run(train)\n            print('[ll %7.2f] [range %4.2f] [sill %4.2f] [nugget %4.2f]' % tuple(sess.run([ll, vrange, sill, nugget])))\n\n        return sess.run([vrange, sill, nugget])\n</code></pre> <p>When we run this, we don't get an estimate for the offset, but the other estimates are often improved. Bear in mind that we can actually solve for the posterior distribution of the offset if we want to.</p> <pre><code># Define parameters using tf.Variable.\nlog_vrange = tf.Variable(0.0, dtype=tf.float64)\nlog_sill = tf.Variable(0.0, dtype=tf.float64)\nlog_nugget = tf.Variable(0.0, dtype=tf.float64)\n\nvrange = tf.exp(log_vrange)\nsill = tf.exp(log_sill)\nnugget = tf.exp(log_nugget)\n\noffset_prior = 100.0\n\nvrange_val, sill_val, nugget_val = infer_gp_marginalize_over_offset([D, u], [vrange, sill, nugget], offset_prior)\n</code></pre> <pre><code>[ll -541.34] [range 3.02] [sill 0.96] [nugget 1.69]\n[ll -537.97] [range 4.51] [sill 1.13] [nugget 1.73]\n[ll -537.93] [range 4.65] [sill 1.25] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n[ll -537.93] [range 4.67] [sill 1.27] [nugget 1.73]\n</code></pre> <p>Interpolation takes values at \\(N_1\\) locations, and gives means and variances for \\(N_2\\) locations. Formally, interpolation takes</p> <ul> <li>input locations \\(x_1\\), an \\(N_1 \\times 2\\) matrix,</li> <li>input values \\(u_1\\), a vector of \\(N_1\\) elements,</li> <li>output locations \\(x_2\\), an \\(N_2 \\times 2\\) matrix.</li> </ul> <p>and gives a distribution for output values \\(u_2\\), a vector of \\(N_2\\) elements. For notational convenience, define</p> \\[x = \\begin{bmatrix}x_1\\\\x_2\\end{bmatrix} \\textrm{ and } u = \\begin{bmatrix}u_1\\\\u_2\\end{bmatrix}.\\] <p>The model remains the same as before, so \\(u \\sim \\mathcal{N}(0, A)\\), where \\(A\\) is constructed from a distance matrix of all locations \\(x\\) as before. For clarity, let's expand \\(u\\) and \\(A\\):</p> \\[\\begin{bmatrix}u_1\\\\u_2\\end{bmatrix} \\sim \\mathcal{N}\\bigg(0, \\begin{bmatrix}A_{11} &amp; A_{12}\\\\A_{21} &amp; A_{22}\\end{bmatrix}\\bigg).\\] <p>Interpolation consists of getting a distribution for \\(u_2\\) given \\(u_1\\). This is a textbook thing to do with a multivariate normal distribution, and the solution is:</p> \\[u_2 \\mid u_1 \\sim \\mathcal{N}(A_{21}A_{11}^{-1}u_1,\\ A_{22} - A_{21}A_{11}^{-1}A_{12}).\\] <p>Bear in mind, if we just want  the marginal variance of each element in \\(u_2\\), we only need to compute the diagonal entries of \\(A_{22} - A_{21}A_{11}^{-1}A_{12}\\).</p> <p>Refer to Wikipedia for more details.</p> <pre><code>def interpolate_gp(x1, u1, x2, parameter_vals, offset_prior):\n\n    vrange, sill, nugget = parameter_vals\n\n    # Compute distance matrices for sampled locations.\n    D11 = cdist(x1, x1)\n    D12 = cdist(x1, x2)\n    D21 = cdist(x2, x1)\n    D22 = cdist(x2, x2)\n\n    # Compute covariance matrices.\n    C11 = sill * np.exp(-np.square(D11/vrange)) + nugget * np.eye(len(x1)) + offset_prior\n    C12 = sill * np.exp(-np.square(D12/vrange)) + offset_prior # No nugget for off-diagonal entries\n    C21 = sill * np.exp(-np.square(D21/vrange)) + offset_prior # No nugget for off-diagonal entries\n    C22 = sill * np.exp(-np.square(D22/vrange)) + nugget * np.eye(len(x2)) + offset_prior\n\n    u2_mean = np.matmul(C21, np.linalg.solve(C11, u))\n    u2_var = np.diag(C22) -  np.sum(C12 * np.linalg.solve(C11, C12), axis=0)\n\n    return u2_mean, u2_var\n\nMX = 61\nMY = 61\nM = MX * MY # Number of points to infer.\n\n# Gross code to get mesh locations.\nxx, yy = np.meshgrid(np.linspace(-12, 12, MX), np.linspace(-12, 12, MY))\nx2 = np.hstack([xx.reshape((M, 1)), yy.reshape((M, 1))])\n\n# Interpolate!\nu2_mean, u2_var = interpolate_gp(x, u, x2, [vrange_val, sill_val, nugget_val], offset_prior)                                                                \n\n# Plot old values, new value means, new value variances\nfor locations, values in [(x, u), (x2, u2_mean), (x2, u2_var)]:\n    pp.scatter(locations[:, 0], locations[:, 1], c=values)\n    pp.xlim(-12, 12)\n    pp.ylim(-12, 12)\n    pp.show()\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"notebooks/predictions-with-mesh/predictions-with-mesh/","title":"Predictions with mesh","text":"<pre><code>from geostat import GP, Model, Mesh, NormalizingFeaturizer, Parameters\nimport geostat.kernel as krn\nimport matplotlib.pyplot as pp\nimport numpy as np\nfrom shapely.geometry import Point, Polygon\nimport shapely.vectorized as shv\nimport geopandas as gpd\nimport contextily as ctx\n</code></pre>"},{"location":"notebooks/predictions-with-mesh/predictions-with-mesh/#overview","title":"Overview","text":"<p>In this notebook we will show how <code>Mesh</code> is used to make prediction locations. We will:   * Generate synthetic data on a map of watersheds in Berkeley, California.   * Fit a Gaussian Process <code>Model</code> to the data.   * Make predictions using <code>Mesh</code>.</p>"},{"location":"notebooks/predictions-with-mesh/predictions-with-mesh/#synthesizing-data","title":"Synthesizing data","text":"<p>We will synthesize data at 200 random locations drawn from inside a polygon for Berkeley watersheds.</p> <pre><code>berkeleydf = gpd.read_file(\"./berkeley-watershed.zip\")\nberkeley = berkeleydf['geometry'].iloc[0]\n\nx0, y0, x1, y1 = berkeley.bounds\nlocs = np.random.uniform(size = [2000, 2]) * [x1-x0, y1-y0] + [x0, y0] # Generate 2000 points.\nmask = [berkeley.contains(Point(p)) for p in locs]\nlocs = locs[mask, :][:200, :] # Filter away points outside of shape and keep just 200.\n</code></pre> <p>Declare the terms of the spatial trend:</p> <pre><code>def trend_terms(x, y): return x, y\n</code></pre> <p>Create a featurizer that the Gaussian process class <code>GP</code> will use to convert locations into trend features:</p> <pre><code>featurizer = NormalizingFeaturizer(trend_terms, locs)\n</code></pre> <p>Make geostatistical parameters for the <code>GP</code>, instiate the <code>GP</code>, instantiate a <code>Model</code>, and call <code>generate</code> to generate synthetic observations.</p> <pre><code>p = Parameters(alpha=0.25, range=2000, sill=1., nugget=0.25)\n\ngp = GP(0, krn.TrendPrior(featurizer, alpha=p.alpha) + \\\n           krn.SquaredExponential(range=p.range, sill=p.sill) + \\\n           krn.Noise(nugget=p.nugget))\n\nmodel = Model(gp)\n\nobs = model.generate(locs).vals\n\nvmin, vmax = obs.min(), obs.max()\n</code></pre> <p>When the data is plotted, you can see an overall trend with some localized variations.</p> <pre><code>fig, ax = pp.subplots(figsize=(7, 7), dpi=120)\n\ncax = pp.scatter(locs[:, 0], locs[:, 1], c=obs, vmin=vmin, vmax=vmax)\n\nberkeleydf.plot(ax=ax, fc='none', ec='black', lw=1)\n\npp.colorbar(cax, shrink=0.7)\n\nctx.add_basemap(ax, attribution=False, crs=berkeleydf.crs.to_string(), \n                source='https://basemap.nationalmap.gov/arcgis/rest/services/'\n                       'USGSImageryOnly/MapServer/tile/{z}/{y}/{x}')\n\npp.title('Synthetic data')\n\npp.tight_layout()\npp.show()\n</code></pre> <p></p>"},{"location":"notebooks/predictions-with-mesh/predictions-with-mesh/#inferring-parameters","title":"Inferring parameters","text":"<p>Now set the parameters to something arbitrary to see if we can successfully infer model parameters. We call <code>fit</code> with the data (<code>locs</code> and <code>obs</code>).</p> <pre><code>model.set(alpha=1.0, range=1000.0, sill=0.5, nugget=0.5)\nmodel.fit(locs, obs, iters=300)\nNone\n</code></pre> <pre><code>[iter    30 ll -194.17 time  1.49 reg  0.00 alpha  1.24 sill  0.42 range 1336.73 nugget  0.37]\n[iter    60 ll -184.73 time  0.36 reg  0.00 alpha  1.12 sill  0.52 range 1687.33 nugget  0.29]\n[iter    90 ll -181.58 time  0.35 reg  0.00 alpha  1.01 sill  0.68 range 1964.81 nugget  0.25]\n[iter   120 ll -180.89 time  0.34 reg  0.00 alpha  0.96 sill  0.85 range 2135.51 nugget  0.24]\n[iter   150 ll -180.72 time  0.35 reg  0.00 alpha  0.93 sill  0.99 range 2234.71 nugget  0.24]\n[iter   180 ll -180.67 time  0.35 reg  0.00 alpha  0.92 sill  1.08 range 2291.47 nugget  0.24]\n[iter   210 ll -180.66 time  0.35 reg  0.00 alpha  0.91 sill  1.13 range 2321.38 nugget  0.24]\n[iter   240 ll -180.66 time  0.35 reg  0.00 alpha  0.91 sill  1.16 range 2336.16 nugget  0.24]\n[iter   270 ll -180.65 time  0.34 reg  0.00 alpha  0.90 sill  1.17 range 2342.93 nugget  0.24]\n[iter   300 ll -180.65 time  0.34 reg  0.00 alpha  0.90 sill  1.18 range 2345.77 nugget  0.24]\n</code></pre>"},{"location":"notebooks/predictions-with-mesh/predictions-with-mesh/#generating-predictions-in-convex-hull","title":"Generating predictions in convex hull","text":"<p>Create a mesh using a convex hull for making predictions.</p> <pre><code>mesh = Mesh.from_convex_hull(locs, nx=200)\n</code></pre> <p>Call <code>Model</code> to get predictions at mesh locations:</p> <pre><code>mean, var = model.predict(mesh.locations())\n</code></pre> <p>Create a slice for prediction mean and plot:</p> <pre><code>meshx, meshy, value = mesh.slice(mean)\n\nfig, ax = pp.subplots(figsize=(7, 7), dpi=120)\n\ncax = pp.pcolormesh(meshx, meshy, value, vmin=vmin, vmax=vmax)\n\nberkeleydf.plot(ax=ax, fc='none', ec='black', lw=1)\n\npp.colorbar(cax, shrink=0.7)\n\nctx.add_basemap(ax, attribution=False, crs=berkeleydf.crs.to_string(), \n                source='https://basemap.nationalmap.gov/arcgis/rest/services/'\n                       'USGSImageryOnly/MapServer/tile/{z}/{y}/{x}')\n\npp.title('Prediction mean')\n\npp.tight_layout()\npp.show()\n</code></pre> <p></p> <p>Do the same for prediction variance:</p> <pre><code>meshx, meshy, value = mesh.slice(var)\n\nfig, ax = pp.subplots(figsize=(7, 7), dpi=120)\n\ncax = pp.pcolormesh(meshx, meshy, value, cmap='gist_heat_r')\n\nberkeleydf.plot(ax=ax, fc='none', ec='black', lw=1)\n\npp.colorbar(cax, shrink=0.7)\n\nctx.add_basemap(ax, attribution=False, crs=berkeleydf.crs.to_string(), \n                source='https://basemap.nationalmap.gov/arcgis/rest/services/'\n                       'USGSImageryOnly/MapServer/tile/{z}/{y}/{x}')\n\npp.title('Prediction variance')\n\npp.tight_layout()\npp.show()\n</code></pre> <p></p>"},{"location":"notebooks/predictions-with-mesh/predictions-with-mesh/#generating-predictions-in-arbitrary-shape","title":"Generating predictions in arbitrary shape","text":"<p>The convex hull produces predictions both inside and outside the shape for Berkeley watersheds, which can be a bit awkward. Now instead, let's create a mesh using the shape, for making predictions.</p> <pre><code>mesh = Mesh.from_polygon(berkeley, nx=200)\n</code></pre> <p>Make predictions:</p> <pre><code>mean, var = model.predict(mesh.locations())\n</code></pre> <p>Create a slice for prediction mean and plot:</p> <pre><code>meshx, meshy, value = mesh.slice(mean)\n\nfig, ax = pp.subplots(figsize=(7, 7), dpi=120)\n\ncax = pp.pcolormesh(meshx, meshy, value, vmin=vmin, vmax=vmax)\n\n# Add contour\nvalue_contains = shv.contains(berkeleydf.geometry.item(), meshx, meshy)\nvalue_mask = np.where(value_contains, value, np.nan)\npp.contour(meshx, meshy, value_mask, colors='k', linewidths=0.5, alpha=0.8)\n\n\nberkeleydf.plot(ax=ax, fc='none', ec='black', lw=1)\n\npp.colorbar(cax, shrink=0.7)\n\nctx.add_basemap(ax, attribution=False, crs=berkeleydf.crs.to_string(), \n                source='https://basemap.nationalmap.gov/arcgis/rest/services/'\n                       'USGSImageryOnly/MapServer/tile/{z}/{y}/{x}')\n\npp.title('Prediction mean')\n\npp.tight_layout()\npp.show()\n</code></pre> <p></p> <p>Do the same for prediction variance:</p> <pre><code>meshx, meshy, value = mesh.slice(var)\n\nfig, ax = pp.subplots(figsize=(7, 7), dpi=120)\n\ncax = pp.pcolormesh(meshx, meshy, value, cmap='gist_heat_r')\n\nberkeleydf.plot(ax=ax, fc='none', ec='black', lw=1)\n\npp.colorbar(cax, shrink=0.7)\n\nctx.add_basemap(ax, attribution=False, crs=berkeleydf.crs.to_string(), \n                source='https://basemap.nationalmap.gov/arcgis/rest/services/'\n                       'USGSImageryOnly/MapServer/tile/{z}/{y}/{x}')\n\npp.title('Prediction variance')\n\npp.tight_layout()\npp.show()\n</code></pre> <p></p>"},{"location":"notebooks/wiener-process/wiener-process/","title":"Wiener process","text":"<pre><code>from geostat import GP, Model, Featurizer, Parameters\nimport geostat.kernel as krn\nimport matplotlib.pyplot as plt\nimport numpy as np\n</code></pre> <pre><code>x = np.linspace(-8, 8, 321)[:, np.newaxis]\n\nlocs = np.array([-4, -3, -2, -1, 0, 1, 2, 3, 4], dtype=float)[:, np.newaxis]\nobs = np.array([-2, -2, -2, -1, 0, 1, 2, 2, 2], dtype=float)\n\nfeaturizer = Featurizer(lambda x: (1., x))\nfeaturizer2 = Featurizer(lambda x: (1.,))\n\np = Parameters(alpha=0.25)\nkernel = krn.TrendPrior(featurizer, alpha=p.alpha) + krn.Noise(0.25)\nmodel = Model(GP(0, kernel)).fit(locs, obs, iters=500)\nmu1, _ = model.predict(x)\n\np = Parameters(range=0.33, sill=1.)\nkernel = krn.SquaredExponential(range=p.range, sill=p.sill) + krn.Noise(0.25)\nmodel = Model(GP(0, kernel)).fit(locs, obs, iters=500)\nmu2, _ = model.predict(x)\n\np = Parameters(alpha=0.25, sill=1.)\nkernel = krn.TrendPrior(featurizer2, alpha=p.alpha) \\\n       + krn.Constant(sill=p.sill) \\\n       * krn.Wiener(axis=0, start=-4) + krn.Noise(nugget=0.25)\nmodel = Model(GP(0, kernel)).fit(locs, obs, iters=500)\nmu3, _ = model.predict(x)\n</code></pre> <pre><code>[iter    50 ll -9.88 time  0.43 reg  0.00 alpha  0.21]\n[iter   100 ll -9.88 time  0.23 reg  0.00 alpha  0.21]\n[iter   150 ll -9.88 time  0.23 reg  0.00 alpha  0.21]\n[iter   200 ll -9.88 time  0.23 reg  0.00 alpha  0.21]\n[iter   250 ll -9.88 time  0.23 reg  0.00 alpha  0.21]\n[iter   300 ll -9.88 time  0.23 reg  0.00 alpha  0.21]\n[iter   350 ll -9.88 time  0.23 reg  0.00 alpha  0.21]\n[iter   400 ll -9.88 time  0.23 reg  0.00 alpha  0.21]\n[iter   450 ll -9.88 time  0.23 reg  0.00 alpha  0.21]\n[iter   500 ll -9.88 time  0.23 reg  0.00 alpha  0.21]\n[iter    50 ll -16.37 time  0.50 reg  0.00 sill  1.54 range  0.59]\n[iter   100 ll -13.03 time  0.21 reg  0.00 sill  1.72 range  1.09]\n[iter   150 ll -10.63 time  0.21 reg  0.00 sill  1.75 range  1.85]\n[iter   200 ll -9.75 time  0.21 reg  0.00 sill  1.93 range  2.56]\n[iter   250 ll -9.57 time  0.21 reg  0.00 sill  2.25 range  2.91]\n[iter   300 ll -9.52 time  0.21 reg  0.00 sill  2.55 range  3.03]\n[iter   350 ll -9.50 time  0.20 reg  0.00 sill  2.76 range  3.10]\n[iter   400 ll -9.50 time  0.21 reg  0.00 sill  2.90 range  3.14]\n[iter   450 ll -9.50 time  0.20 reg  0.00 sill  2.98 range  3.16]\n[iter   500 ll -9.50 time  0.21 reg  0.00 sill  3.02 range  3.18]\n[iter    50 ll -14.39 time  1.96 reg  0.00 alpha  0.41 sill  0.68]\n[iter   100 ll -13.64 time  0.21 reg  0.00 alpha  0.64 sill  0.65]\n[iter   150 ll -13.18 time  0.22 reg  0.00 alpha  0.93 sill  0.64]\n[iter   200 ll -12.91 time  0.22 reg  0.00 alpha  1.25 sill  0.64]\n[iter   250 ll -12.75 time  0.22 reg  0.00 alpha  1.59 sill  0.64]\n[iter   300 ll -12.66 time  0.22 reg  0.00 alpha  1.93 sill  0.64]\n[iter   350 ll -12.61 time  0.22 reg  0.00 alpha  2.24 sill  0.64]\n[iter   400 ll -12.58 time  0.22 reg  0.00 alpha  2.52 sill  0.64]\n[iter   450 ll -12.57 time  0.22 reg  0.00 alpha  2.77 sill  0.64]\n[iter   500 ll -12.56 time  0.22 reg  0.00 alpha  2.98 sill  0.64]\n</code></pre> <pre><code>plt.scatter(locs[:, 0], obs, marker='o', color='black')\nplt.plot(x[:, 0], mu1, label='Trend')\nplt.plot(x[:, 0], mu2, label='Stationary')\nplt.plot(x[:, 0], mu3, label='Wiener process')\nplt.legend(title='GP models')\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x7f9df0363ee0&gt;\n</code></pre>"}]}