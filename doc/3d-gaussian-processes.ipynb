{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geostat import GP, Mesh, NormalizingFeaturizer\n",
    "import geostat.covfunc as cf\n",
    "import matplotlib.pyplot as pp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook we will:\n",
    "  * Use a Gaussian process with a complex stacked covariance function to generate synthetic data.\n",
    "  * Use Gaussian processes with covariance functions of increasing complexity to infer the geospatial parameters from the synthetic data. Each time, log likelihood improves and the nugget decreases. A smaller nugget means that predictions are more confident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesizing data\n",
    "\n",
    "We will synthesize data at random locations in 3D space near the origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = np.random.normal(size=[500, 3]) * [1., 1., 0.333]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be a depth trend, but no horizontal trends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_terms(x, y, z): return z, z*z, z*z*z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a featurizer that the Gaussian process class `GP` will use to convert locations into trend features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = NormalizingFeaturizer(trend_terms, locs.reshape([-1, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance function will be a combination of two gamma-exponentials, one which respects depth with z-anisotropy, and one that ignores depth altogether. We will set the `range` for both to be the same parameter to show that it is possible to tie parameters together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance = \\\n",
    "    cf.GammaExponential(range='r', sill='s1', gamma='g1', scale=[1., 1., 'zs']) + \\\n",
    "    cf.GammaExponential(range='r', sill='s2', gamma='g2', scale=[1., 1., 0.]) + \\\n",
    "    cf.Noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a `GP` and immediately call `generate` to generate synthetic observations.\n",
    "  * `parameter` holds the geostatistical parameters named above.\n",
    "  * `alpha` parameterizes the normal distribution prior for trend coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = GP(featurizer = featurizer,\n",
    "         covariance = covariance,\n",
    "         parameters = dict(zs=10., r=0.33, s1=1., s2=0.5, g1=1., g2=0.5, nugget=0.25),\n",
    "         hyperparameters = dict(alpha=0.1),\n",
    "         verbose=True).generate(locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the data is plotted, you can see an overall trend with some localized variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = pp.subplots(3, figsize=(7, 6), dpi=120, sharex=True, sharey=True)\n",
    "vmin, vmax = obs.min(), obs.max()\n",
    "pane = np.round((locs[:, 1] + 2) / 2).astype(int)\n",
    "for i, ymid in enumerate(np.linspace(-2, 2, 3)):\n",
    "    ymin, ymax = ymid - 1, ymid + 1\n",
    "    c = axs[i].scatter(locs[pane == i, 0], locs[pane == i, 2], c=obs[pane == i], vmin=vmin, vmax=vmax)\n",
    "    axs[i].set_title('y = %0.1f' % ymid)\n",
    "    axs[i].set_aspect(0.9)\n",
    "axs[2].set_xlabel('x-axis')\n",
    "axs[1].set_ylabel('z-axis')\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.88, 0.1, 0.02, 0.8])\n",
    "fig.colorbar(c, cax=cbar_ax)\n",
    "\n",
    "fig.suptitle('Synthetic data, projected to nearest cross section')\n",
    "pp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, let's define a function that takes a model and plots predictions for the three slices shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(gp):\n",
    "    fig, axs = pp.subplots(3, figsize=(7, 6), dpi=120, sharex=True, sharey=True)\n",
    "    for i, ymid in enumerate(np.linspace(-2, 2, 3)):\n",
    "\n",
    "        mesh = Mesh.from_bounds([-3, -1, 3, 1], nx=200)\n",
    "        mesh_locs = mesh.locations(proj=[[1, 0, 0], [0, 0, 1], [0, ymid, 0]]) # [x, z, 1] -> [x, y, z].\n",
    "        mean, var = gp.predict(locs, obs, mesh_locs)\n",
    "        meshx, meshy, out = mesh.slice(mean)\n",
    "        c = axs[i].pcolormesh(meshx, meshy, out, vmin=vmin, vmax=vmax)\n",
    "\n",
    "        axs[i].set_title('y = %0.1f' % ymid)\n",
    "        axs[i].set_aspect(0.9)\n",
    "    axs[2].set_xlabel('x-axis')\n",
    "    axs[1].set_ylabel('z-axis')\n",
    "\n",
    "    fig.subplots_adjust(right=0.9)\n",
    "    cbar_ax = fig.add_axes([0.88, 0.1, 0.02, 0.8])\n",
    "    fig.colorbar(c, cax=cbar_ax)\n",
    "\n",
    "    fig.suptitle('Predictions for 3 cross sections')\n",
    "    pp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Bayesian regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's try modeling the data with a Bayesian regression, which is a model with just trends and uncorrelated noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance = cf.Noise()\n",
    "\n",
    "gp1 = GP(featurizer = featurizer,\n",
    "        covariance = covariance,\n",
    "        parameters = dict(range=1.0, sill=0.5, nugget=0.5),\n",
    "        hyperparameters = dict(alpha=obs.ptp()**2, reg=0, train_iters=500),\n",
    "        verbose=True).fit(locs, obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are predictions for the three slices shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(gp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: GP with isotropic sq-exp covariance function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's layer on an isotropic squared exponential covariance function to the above model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance = \\\n",
    "    cf.SquaredExponential() + \\\n",
    "    cf.Noise()\n",
    "\n",
    "gp2 = GP(featurizer = featurizer,\n",
    "        covariance = covariance,\n",
    "        parameters = dict(range=1.0, sill=0.5, nugget=0.5),\n",
    "        hyperparameters = dict(alpha=obs.ptp()**2, reg=0, train_iters=500),\n",
    "        verbose=True).fit(locs, obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-likelihood is improved as a result of a more complex model. Nugget is much lower. Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(gp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: GP with anisotropic sq-exp covariance function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we switch from isotropic to anisotropic for the covariance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance = \\\n",
    "    cf.SquaredExponential(scale=[1., 1., 'zs']) + \\\n",
    "    cf.Noise()\n",
    "\n",
    "gp3 = GP(featurizer = featurizer,\n",
    "        covariance = covariance,\n",
    "        parameters = dict(range=1.0, sill=0.5, nugget=0.5, zs=5.0),\n",
    "        hyperparameters = dict(alpha=obs.ptp()**2, reg=0, train_iters=500),\n",
    "        verbose=True).fit(locs, obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-likelihood and nugget both improve further. Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(gp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4: GP with anisotropic gamma-exp covariance function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we switch from a squared-exponential to a gamma-exponential covariance function, which has an extra shape parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance = \\\n",
    "    cf.GammaExponential(scale=[1., 1., 'zs']) + \\\n",
    "    cf.Noise()\n",
    "\n",
    "gp4 = GP(featurizer = featurizer,\n",
    "        covariance = covariance,\n",
    "        parameters = dict(range=1.0, sill=0.5, nugget=0.5, zs=5.0, gamma=1.0),\n",
    "        hyperparameters = dict(alpha=obs.ptp()**2, reg=0, train_iters=500),\n",
    "        verbose=True).fit(locs, obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-likelihood has improved very slightly but nugget improves more significantly, since the pointy peak in the gamma-exponential does some of the work of a nugget. Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(gp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5: GP with stacked covariance functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we switch to using the same covariance function used to generate the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance = \\\n",
    "    cf.GammaExponential(range='r', sill='s1', gamma='g1', scale=[1., 1., 'zs']) + \\\n",
    "    cf.GammaExponential(range='r', sill='s2', gamma='g2', scale=[1., 1., 0.]) + \\\n",
    "    cf.Noise() \n",
    "\n",
    "gp5 = GP(featurizer = featurizer,\n",
    "        covariance = covariance,\n",
    "        parameters = dict(zs=5., r=0.5, s1=2., s2=1., g1=1., g2=1., nugget=1.),\n",
    "        hyperparameters = dict(alpha=obs.ptp()**2, reg=0, train_iters=500),\n",
    "        verbose=True).fit(locs, obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, log likelihood and nugget both improve further. You can see faint vertical stripes that correspond the the \"depth-invariant\" component of the covariance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(gp5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geostat",
   "language": "python",
   "name": "geostat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
